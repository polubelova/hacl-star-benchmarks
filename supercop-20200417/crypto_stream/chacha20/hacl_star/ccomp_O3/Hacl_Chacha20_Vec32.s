# File generated by CompCert 3.7
# Command line: -O3 -S -D_BSD_SOURCE -D_DEFAULT_SOURCE Hacl_Chacha20_Vec32.c
	.data
	.align	4
	.globl	Hacl_Impl_Chacha20_Vec_chacha20_constants
Hacl_Impl_Chacha20_Vec_chacha20_constants:
	.long	1634760805
	.long	857760878
	.long	2036477234
	.long	1797285236
	.type	Hacl_Impl_Chacha20_Vec_chacha20_constants, @object
	.size	Hacl_Impl_Chacha20_Vec_chacha20_constants, . - Hacl_Impl_Chacha20_Vec_chacha20_constants
	.text
	.align	16
	.globl Hacl_Chacha20_Vec32_chacha20_encrypt_32
Hacl_Chacha20_Vec32_chacha20_encrypt_32:
	.cfi_startproc
	subq	$472, %rsp
	.cfi_adjust_cfa_offset	472
	leaq	480(%rsp), %rax
	movq	%rax, 0(%rsp)
	movq	%rbx, 8(%rsp)
	movq	%rbp, 16(%rsp)
	movq	%r12, 24(%rsp)
	movq	%r13, 32(%rsp)
	movq	%r14, 40(%rsp)
	movq	%r15, 48(%rsp)
	movq	%rcx, %rbp
	movq	%rdx, 96(%rsp)
	movq	%rsi, 104(%rsp)
	movq	%rdi, %rbx
	xorl	%r11d, %r11d
	movl	%r11d, 136(%rsp)
	xorl	%ecx, %ecx
	movl	%ecx, 140(%rsp)
	xorl	%edi, %edi
	movl	%edi, 144(%rsp)
	xorl	%r10d, %r10d
	movl	%r10d, 148(%rsp)
	xorl	%eax, %eax
	movl	%eax, 152(%rsp)
	xorl	%edi, %edi
	movl	%edi, 156(%rsp)
	xorl	%edx, %edx
	movl	%edx, 160(%rsp)
	xorl	%r10d, %r10d
	movl	%r10d, 164(%rsp)
	xorl	%ecx, %ecx
	movl	%ecx, 168(%rsp)
	xorl	%edi, %edi
	movl	%edi, 172(%rsp)
	xorl	%eax, %eax
	movl	%eax, 176(%rsp)
	xorl	%ecx, %ecx
	movl	%ecx, 180(%rsp)
	xorl	%edi, %edi
	movl	%edi, 184(%rsp)
	xorl	%r10d, %r10d
	movl	%r10d, 188(%rsp)
	xorl	%r11d, %r11d
	movl	%r11d, 192(%rsp)
	xorl	%r10d, %r10d
	movl	%r10d, 196(%rsp)
	movq	%r9, %r13
	movq	%r8, %r15
	xorl	%ecx, %ecx
	movl	%ecx, 392(%rsp)
	xorl	%r11d, %r11d
	movl	%r11d, 396(%rsp)
	xorl	%esi, %esi
	movl	%esi, 400(%rsp)
	xorl	%r9d, %r9d
	movl	%r9d, 404(%rsp)
	xorl	%eax, %eax
	movl	%eax, 408(%rsp)
	xorl	%r9d, %r9d
	movl	%r9d, 412(%rsp)
	xorl	%eax, %eax
	movl	%eax, 416(%rsp)
	xorl	%esi, %esi
	movl	%esi, 420(%rsp)
	xorl	%r9d, %r9d
	movl	%r9d, 424(%rsp)
	xorl	%edx, %edx
	movl	%edx, 428(%rsp)
	xorl	%edx, %edx
	movl	%edx, 432(%rsp)
	xorl	%r9d, %r9d
	movl	%r9d, 436(%rsp)
	xorl	%eax, %eax
	movl	%eax, 440(%rsp)
	xorl	%r8d, %r8d
	movl	%r8d, 444(%rsp)
	xorl	%eax, %eax
	movl	%eax, 448(%rsp)
	xorl	%esi, %esi
	movl	%esi, 452(%rsp)
	xorl	%edx, %edx
.L100:
	leaq	392(%rsp), %rcx
	leaq	Hacl_Impl_Chacha20_Vec_chacha20_constants(%rip), %rsi
	movl	%edx, %eax
	movl	0(%rsi,%rax,4), %esi
	movl	%esi, 0(%rcx,%rax,4)
	leal	1(%edx), %edx
	cmpl	$4, %edx
	jb	.L100
	xorl	%r14d, %r14d
.L101:
	leaq	408(%rsp), %r12
	leal	0(,%r14d,4), %r10d
	movl	%r10d, %edx
	leaq	0(%rbp,%rdx,1), %rsi
	leaq	456(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	456(%rsp), %r11d
	movl	%r14d, %r10d
	movl	%r11d, 0(%r12,%r10,4)
	leal	1(%r14d), %r14d
	cmpl	$8, %r14d
	jb	.L101
	movl	%r13d, 440(%rsp)
	xorl	%r12d, %r12d
.L102:
	leaq	444(%rsp), %rbp
	leal	0(,%r12d,4), %r8d
	movl	%r8d, %esi
	leaq	0(%r15,%rsi,1), %rsi
	leaq	456(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	456(%rsp), %ecx
	movl	%r12d, %r11d
	movl	%ecx, 0(%rbp,%r11,4)
	leal	1(%r12d), %r12d
	cmpl	$3, %r12d
	jb	.L102
	xorl	%r11d, %r11d
.L103:
	leaq	136(%rsp), %rax
	leaq	392(%rsp), %r8
	movl	%r11d, %r10d
	movl	0(%r8,%r10,4), %esi
	movl	%esi, 0(%rax,%r10,4)
	leal	1(%r11d), %r11d
	cmpl	$16, %r11d
	jb	.L103
	movl	184(%rsp), %ecx
	leal	0(%ecx), %r9d
	movl	%r9d, 184(%rsp)
	movq	%rbx, %r11
	andl	$63, %r11d
	movl	%r11d, 116(%rsp)
	movq	%rbx, %rcx
	shrl	$6, %ecx
	movl	%ecx, 72(%rsp)
	movl	%r11d, 112(%rsp)
	xorl	%r11d, %r11d
	movl	%r11d, 68(%rsp)
.L104:
	movl	68(%rsp), %r9d
	movl	72(%rsp), %r10d
	cmpl	%r10d, %r9d
	jae	.L105
	movl	68(%rsp), %ecx
	sall	$6, %ecx
	movl	%ecx, %edi
	movq	104(%rsp), %rsi
	leaq	0(%rsi,%rdi,1), %r10
	movq	%r10, 120(%rsp)
	movq	96(%rsp), %r9
	leaq	0(%r9,%rdi,1), %r8
	movq	%r8, 128(%rsp)
	xorl	%ecx, %ecx
	movl	%ecx, 200(%rsp)
	xorl	%esi, %esi
	movl	%esi, 204(%rsp)
	xorl	%eax, %eax
	movl	%eax, 208(%rsp)
	xorl	%eax, %eax
	movl	%eax, 212(%rsp)
	xorl	%edi, %edi
	movl	%edi, 216(%rsp)
	xorl	%edi, %edi
	movl	%edi, 220(%rsp)
	xorl	%r8d, %r8d
	movl	%r8d, 224(%rsp)
	xorl	%esi, %esi
	movl	%esi, 228(%rsp)
	xorl	%r9d, %r9d
	movl	%r9d, 232(%rsp)
	xorl	%edi, %edi
	movl	%edi, 236(%rsp)
	xorl	%edi, %edi
	movl	%edi, 240(%rsp)
	xorl	%r8d, %r8d
	movl	%r8d, 244(%rsp)
	xorl	%r9d, %r9d
	movl	%r9d, 248(%rsp)
	xorl	%edi, %edi
	movl	%edi, 252(%rsp)
	xorl	%eax, %eax
	movl	%eax, 256(%rsp)
	xorl	%edx, %edx
	movl	%edx, 260(%rsp)
	movl	68(%rsp), %ecx
	movq	%rcx, %rbx
	leaq	136(%rsp), %rax
	movq	%rax, 88(%rsp)
	leaq	200(%rsp), %r10
	movq	%r10, 80(%rsp)
	movq	$64, %rdx
	movq	88(%rsp), %rsi
	movq	80(%rsp), %rdi
	call	memcpy
	movq	%rbx, %rdx
	movl	%edx, 76(%rsp)
	movl	248(%rsp), %r10d
	leal	0(%r10d,%edx,1), %r10d
	movl	200(%rsp), %esi
	movl	216(%rsp), %r15d
	leal	0(%esi,%r15d,1), %r11d
	xorl	%r11d, %r10d
	rorl	$16, %r10d
	movl	232(%rsp), %edi
	leal	0(%edi,%r10d,1), %r9d
	xorl	%r9d, %r15d
	rorl	$20, %r15d
	leal	0(%r11d,%r15d,1), %ecx
	xorl	%ecx, %r10d
	rorl	$24, %r10d
	leal	0(%r9d,%r10d,1), %r8d
	xorl	%r8d, %r15d
	rorl	$25, %r15d
	movl	204(%rsp), %r11d
	movl	220(%rsp), %r12d
	leal	0(%r11d,%r12d,1), %edi
	movl	252(%rsp), %ebx
	xorl	%edi, %ebx
	rorl	$16, %ebx
	movl	236(%rsp), %r9d
	leal	0(%r9d,%ebx,1), %esi
	xorl	%esi, %r12d
	rorl	$20, %r12d
	leal	0(%edi,%r12d,1), %edi
	xorl	%edi, %ebx
	rorl	$24, %ebx
	leal	0(%esi,%ebx,1), %r11d
	movl	%r11d, 56(%rsp)
	xorl	%r11d, %r12d
	rorl	$25, %r12d
	movl	208(%rsp), %edx
	movl	224(%rsp), %ebp
	leal	0(%edx,%ebp,1), %r11d
	movl	256(%rsp), %r13d
	xorl	%r11d, %r13d
	rorl	$16, %r13d
	movl	240(%rsp), %eax
	leal	0(%eax,%r13d,1), %r9d
	xorl	%r9d, %ebp
	rorl	$20, %ebp
	leal	0(%r11d,%ebp,1), %r11d
	xorl	%r11d, %r13d
	rorl	$24, %r13d
	leal	0(%r9d,%r13d,1), %r9d
	xorl	%r9d, %ebp
	rorl	$25, %ebp
	movl	212(%rsp), %edx
	movl	228(%rsp), %eax
	leal	0(%edx,%eax,1), %edx
	movl	260(%rsp), %r14d
	xorl	%edx, %r14d
	rorl	$16, %r14d
	movl	244(%rsp), %esi
	leal	0(%esi,%r14d,1), %esi
	xorl	%esi, %eax
	rorl	$20, %eax
	leal	0(%edx,%eax,1), %edx
	xorl	%edx, %r14d
	rorl	$24, %r14d
	leal	0(%esi,%r14d,1), %esi
	xorl	%esi, %eax
	rorl	$25, %eax
	leal	0(%ecx,%r12d,1), %ecx
	xorl	%ecx, %r14d
	rorl	$16, %r14d
	leal	0(%r9d,%r14d,1), %r9d
	xorl	%r9d, %r12d
	rorl	$20, %r12d
	leal	0(%ecx,%r12d,1), %ecx
	xorl	%ecx, %r14d
	rorl	$24, %r14d
	movl	%r14d, 60(%rsp)
	movl	60(%rsp), %r14d
	leal	0(%r9d,%r14d,1), %r9d
	xorl	%r9d, %r12d
	rorl	$25, %r12d
	leal	0(%edi,%ebp,1), %edi
	xorl	%edi, %r10d
	rorl	$16, %r10d
	leal	0(%esi,%r10d,1), %esi
	xorl	%esi, %ebp
	rorl	$20, %ebp
	leal	0(%edi,%ebp,1), %r14d
	xorl	%r14d, %r10d
	rorl	$24, %r10d
	leal	0(%esi,%r10d,1), %edi
	movq	%rdi, %rsi
	xorl	%esi, %ebp
	rorl	$25, %ebp
	leal	0(%r11d,%eax,1), %esi
	xorl	%esi, %ebx
	rorl	$16, %ebx
	leal	0(%r8d,%ebx,1), %r8d
	xorl	%r8d, %eax
	rorl	$20, %eax
	leal	0(%esi,%eax,1), %esi
	xorl	%esi, %ebx
	rorl	$24, %ebx
	leal	0(%r8d,%ebx,1), %r8d
	xorl	%r8d, %eax
	rorl	$25, %eax
	leal	0(%edx,%r15d,1), %r11d
	xorl	%r11d, %r13d
	rorl	$16, %r13d
	movl	56(%rsp), %edx
	leal	0(%edx,%r13d,1), %edx
	xorl	%edx, %r15d
	rorl	$20, %r15d
	leal	0(%r11d,%r15d,1), %r11d
	xorl	%r11d, %r13d
	rorl	$24, %r13d
	leal	0(%edx,%r13d,1), %edx
	xorl	%edx, %r15d
	rorl	$25, %r15d
	leal	0(%ecx,%r15d,1), %ecx
	xorl	%ecx, %r10d
	rorl	$16, %r10d
	leal	0(%r8d,%r10d,1), %r8d
	xorl	%r8d, %r15d
	rorl	$20, %r15d
	leal	0(%ecx,%r15d,1), %ecx
	xorl	%ecx, %r10d
	rorl	$24, %r10d
	leal	0(%r8d,%r10d,1), %r8d
	xorl	%r8d, %r15d
	rorl	$25, %r15d
	movl	%r15d, 56(%rsp)
	leal	0(%r14d,%r12d,1), %r14d
	xorl	%r14d, %ebx
	rorl	$16, %ebx
	leal	0(%edx,%ebx,1), %edx
	movq	%rdx, %r15
	xorl	%r15d, %r12d
	rorl	$20, %r12d
	movq	%r12, %rdx
	leal	0(%r14d,%edx,1), %r12d
	xorl	%r12d, %ebx
	rorl	$24, %ebx
	movq	%r15, %r14
	leal	0(%r14d,%ebx,1), %r15d
	xorl	%r15d, %edx
	rorl	$25, %edx
	leal	0(%esi,%ebp,1), %r14d
	movq	%r13, %rsi
	movq	%r14, %r13
	xorl	%r13d, %esi
	rorl	$16, %esi
	leal	0(%r9d,%esi,1), %r9d
	movq	%r9, %r14
	xorl	%r14d, %ebp
	rorl	$20, %ebp
	movq	%r13, %r9
	leal	0(%r9d,%ebp,1), %r9d
	xorl	%r9d, %esi
	rorl	$24, %esi
	movq	%r14, %r13
	leal	0(%r13d,%esi,1), %r13d
	xorl	%r13d, %ebp
	rorl	$25, %ebp
	leal	0(%r11d,%eax,1), %r11d
	movl	60(%rsp), %r14d
	xorl	%r11d, %r14d
	rorl	$16, %r14d
	leal	0(%edi,%r14d,1), %edi
	xorl	%edi, %eax
	rorl	$20, %eax
	leal	0(%r11d,%eax,1), %r11d
	xorl	%r11d, %r14d
	rorl	$24, %r14d
	leal	0(%edi,%r14d,1), %edi
	xorl	%edi, %eax
	rorl	$25, %eax
	leal	0(%ecx,%edx,1), %ecx
	xorl	%ecx, %r14d
	rorl	$16, %r14d
	leal	0(%r13d,%r14d,1), %r13d
	xorl	%r13d, %edx
	rorl	$20, %edx
	leal	0(%ecx,%edx,1), %ecx
	xorl	%ecx, %r14d
	rorl	$24, %r14d
	movl	%r14d, 60(%rsp)
	movl	60(%rsp), %r14d
	leal	0(%r13d,%r14d,1), %r13d
	xorl	%r13d, %edx
	rorl	$25, %edx
	leal	0(%r12d,%ebp,1), %r14d
	movq	%r10, %r12
	movq	%r14, %r10
	xorl	%r10d, %r12d
	rorl	$16, %r12d
	movq	%r12, %r14
	leal	0(%edi,%r14d,1), %edi
	xorl	%edi, %ebp
	rorl	$20, %ebp
	movq	%rbp, %r12
	leal	0(%r10d,%r12d,1), %r10d
	xorl	%r10d, %r14d
	rorl	$24, %r14d
	movq	%r14, %rbp
	leal	0(%edi,%ebp,1), %edi
	xorl	%edi, %r12d
	rorl	$25, %r12d
	leal	0(%r9d,%eax,1), %r14d
	movq	%rbx, %r9
	movq	%r14, %rbx
	xorl	%ebx, %r9d
	rorl	$16, %r9d
	leal	0(%r8d,%r9d,1), %r8d
	xorl	%r8d, %eax
	rorl	$20, %eax
	leal	0(%ebx,%eax,1), %ebx
	movq	%rbx, %r14
	xorl	%r14d, %r9d
	rorl	$24, %r9d
	leal	0(%r8d,%r9d,1), %ebx
	xorl	%ebx, %eax
	rorl	$25, %eax
	movq	%r11, %r8
	movl	56(%rsp), %r11d
	leal	0(%r8d,%r11d,1), %r8d
	xorl	%r8d, %esi
	rorl	$16, %esi
	leal	0(%r15d,%esi,1), %r15d
	xorl	%r15d, %r11d
	rorl	$20, %r11d
	leal	0(%r8d,%r11d,1), %r8d
	xorl	%r8d, %esi
	rorl	$24, %esi
	leal	0(%r15d,%esi,1), %r15d
	xorl	%r15d, %r11d
	rorl	$25, %r11d
	leal	0(%ecx,%r11d,1), %ecx
	xorl	%ecx, %ebp
	rorl	$16, %ebp
	leal	0(%ebx,%ebp,1), %ebx
	xorl	%ebx, %r11d
	rorl	$20, %r11d
	leal	0(%ecx,%r11d,1), %ecx
	xorl	%ecx, %ebp
	rorl	$24, %ebp
	leal	0(%ebx,%ebp,1), %ebx
	xorl	%ebx, %r11d
	rorl	$25, %r11d
	leal	0(%r10d,%edx,1), %r10d
	xorl	%r10d, %r9d
	rorl	$16, %r9d
	leal	0(%r15d,%r9d,1), %r15d
	xorl	%r15d, %edx
	rorl	$20, %edx
	leal	0(%r10d,%edx,1), %r10d
	xorl	%r10d, %r9d
	rorl	$24, %r9d
	leal	0(%r15d,%r9d,1), %r15d
	movl	%r15d, 56(%rsp)
	movl	56(%rsp), %r15d
	xorl	%r15d, %edx
	rorl	$25, %edx
	leal	0(%r14d,%r12d,1), %r14d
	movq	%r14, %r15
	xorl	%r15d, %esi
	rorl	$16, %esi
	leal	0(%r13d,%esi,1), %r13d
	movq	%r13, %r14
	xorl	%r14d, %r12d
	rorl	$20, %r12d
	movq	%r15, %r13
	leal	0(%r13d,%r12d,1), %r13d
	xorl	%r13d, %esi
	rorl	$24, %esi
	movq	%r14, %r15
	movq	%rsi, %r14
	leal	0(%r15d,%r14d,1), %esi
	xorl	%esi, %r12d
	rorl	$25, %r12d
	leal	0(%r8d,%eax,1), %r8d
	movl	60(%rsp), %r15d
	xorl	%r8d, %r15d
	rorl	$16, %r15d
	leal	0(%edi,%r15d,1), %edi
	xorl	%edi, %eax
	rorl	$20, %eax
	leal	0(%r8d,%eax,1), %r8d
	xorl	%r8d, %r15d
	rorl	$24, %r15d
	leal	0(%edi,%r15d,1), %edi
	xorl	%edi, %eax
	rorl	$25, %eax
	leal	0(%ecx,%edx,1), %ecx
	xorl	%ecx, %r15d
	rorl	$16, %r15d
	leal	0(%esi,%r15d,1), %esi
	xorl	%esi, %edx
	rorl	$20, %edx
	leal	0(%ecx,%edx,1), %ecx
	xorl	%ecx, %r15d
	rorl	$24, %r15d
	movl	%r15d, 60(%rsp)
	movl	60(%rsp), %r15d
	leal	0(%esi,%r15d,1), %esi
	xorl	%esi, %edx
	rorl	$25, %edx
	leal	0(%r10d,%r12d,1), %r15d
	movq	%rbp, %r10
	movq	%r15, %rbp
	xorl	%ebp, %r10d
	rorl	$16, %r10d
	leal	0(%edi,%r10d,1), %edi
	movq	%r12, %r15
	xorl	%edi, %r15d
	rorl	$20, %r15d
	movq	%rbp, %r12
	movq	%r15, %rbp
	leal	0(%r12d,%ebp,1), %r12d
	xorl	%r12d, %r10d
	rorl	$24, %r10d
	leal	0(%edi,%r10d,1), %edi
	xorl	%edi, %ebp
	rorl	$25, %ebp
	leal	0(%r13d,%eax,1), %r13d
	xorl	%r13d, %r9d
	rorl	$16, %r9d
	leal	0(%ebx,%r9d,1), %ebx
	xorl	%ebx, %eax
	rorl	$20, %eax
	leal	0(%r13d,%eax,1), %r13d
	xorl	%r13d, %r9d
	rorl	$24, %r9d
	movq	%r9, %r15
	leal	0(%ebx,%r15d,1), %r9d
	xorl	%r9d, %eax
	rorl	$25, %eax
	leal	0(%r8d,%r11d,1), %r8d
	xorl	%r8d, %r14d
	rorl	$16, %r14d
	movl	56(%rsp), %ebx
	leal	0(%ebx,%r14d,1), %ebx
	xorl	%ebx, %r11d
	rorl	$20, %r11d
	leal	0(%r8d,%r11d,1), %r8d
	xorl	%r8d, %r14d
	rorl	$24, %r14d
	leal	0(%ebx,%r14d,1), %ebx
	xorl	%ebx, %r11d
	rorl	$25, %r11d
	leal	0(%ecx,%r11d,1), %ecx
	xorl	%ecx, %r10d
	rorl	$16, %r10d
	leal	0(%r9d,%r10d,1), %r9d
	xorl	%r9d, %r11d
	rorl	$20, %r11d
	leal	0(%ecx,%r11d,1), %ecx
	xorl	%ecx, %r10d
	rorl	$24, %r10d
	leal	0(%r9d,%r10d,1), %r9d
	xorl	%r9d, %r11d
	rorl	$25, %r11d
	movl	%r11d, 56(%rsp)
	movq	%rdx, %r11
	leal	0(%r12d,%r11d,1), %edx
	xorl	%edx, %r15d
	rorl	$16, %r15d
	leal	0(%ebx,%r15d,1), %ebx
	xorl	%ebx, %r11d
	rorl	$20, %r11d
	leal	0(%edx,%r11d,1), %r12d
	movq	%r15, %rdx
	xorl	%r12d, %edx
	rorl	$24, %edx
	leal	0(%ebx,%edx,1), %r15d
	movq	%r11, %rbx
	xorl	%r15d, %ebx
	rorl	$25, %ebx
	movq	%r13, %r11
	leal	0(%r11d,%ebp,1), %r13d
	movq	%r14, %r11
	xorl	%r13d, %r11d
	rorl	$16, %r11d
	leal	0(%esi,%r11d,1), %esi
	xorl	%esi, %ebp
	rorl	$20, %ebp
	leal	0(%r13d,%ebp,1), %r13d
	movq	%r11, %r14
	movq	%r13, %r11
	xorl	%r11d, %r14d
	rorl	$24, %r14d
	movq	%rsi, %r13
	movq	%r14, %rsi
	leal	0(%r13d,%esi,1), %r13d
	xorl	%r13d, %ebp
	rorl	$25, %ebp
	leal	0(%r8d,%eax,1), %r8d
	movl	60(%rsp), %r14d
	xorl	%r8d, %r14d
	rorl	$16, %r14d
	leal	0(%edi,%r14d,1), %edi
	xorl	%edi, %eax
	rorl	$20, %eax
	leal	0(%r8d,%eax,1), %r8d
	xorl	%r8d, %r14d
	rorl	$24, %r14d
	leal	0(%edi,%r14d,1), %edi
	xorl	%edi, %eax
	rorl	$25, %eax
	leal	0(%ecx,%ebx,1), %ecx
	xorl	%ecx, %r14d
	rorl	$16, %r14d
	leal	0(%r13d,%r14d,1), %r13d
	xorl	%r13d, %ebx
	rorl	$20, %ebx
	leal	0(%ecx,%ebx,1), %ecx
	xorl	%ecx, %r14d
	rorl	$24, %r14d
	movl	%r14d, 60(%rsp)
	movl	60(%rsp), %r14d
	leal	0(%r13d,%r14d,1), %r13d
	movq	%r13, %r14
	movq	%r14, %r13
	xorl	%r13d, %ebx
	rorl	$25, %ebx
	leal	0(%r12d,%ebp,1), %r13d
	movq	%r10, %r12
	movq	%r13, %r10
	xorl	%r10d, %r12d
	rorl	$16, %r12d
	leal	0(%edi,%r12d,1), %edi
	movq	%rdi, %r13
	xorl	%r13d, %ebp
	rorl	$20, %ebp
	movq	%r10, %rdi
	leal	0(%edi,%ebp,1), %edi
	xorl	%edi, %r12d
	rorl	$24, %r12d
	movq	%r13, %r10
	leal	0(%r10d,%r12d,1), %r10d
	xorl	%r10d, %ebp
	rorl	$25, %ebp
	leal	0(%r11d,%eax,1), %r11d
	xorl	%r11d, %edx
	rorl	$16, %edx
	leal	0(%r9d,%edx,1), %r9d
	xorl	%r9d, %eax
	rorl	$20, %eax
	leal	0(%r11d,%eax,1), %r11d
	movq	%r11, %r13
	xorl	%r13d, %edx
	rorl	$24, %edx
	movq	%r9, %r11
	movq	%rdx, %r9
	leal	0(%r11d,%r9d,1), %r11d
	xorl	%r11d, %eax
	rorl	$25, %eax
	movl	56(%rsp), %edx
	leal	0(%r8d,%edx,1), %r8d
	xorl	%r8d, %esi
	rorl	$16, %esi
	leal	0(%r15d,%esi,1), %r15d
	xorl	%r15d, %edx
	rorl	$20, %edx
	leal	0(%r8d,%edx,1), %r8d
	xorl	%r8d, %esi
	rorl	$24, %esi
	leal	0(%r15d,%esi,1), %r15d
	xorl	%r15d, %edx
	rorl	$25, %edx
	leal	0(%ecx,%edx,1), %ecx
	xorl	%ecx, %r12d
	rorl	$16, %r12d
	leal	0(%r11d,%r12d,1), %r11d
	xorl	%r11d, %edx
	rorl	$20, %edx
	leal	0(%ecx,%edx,1), %ecx
	xorl	%ecx, %r12d
	rorl	$24, %r12d
	leal	0(%r11d,%r12d,1), %r11d
	xorl	%r11d, %edx
	rorl	$25, %edx
	leal	0(%edi,%ebx,1), %edi
	xorl	%edi, %r9d
	rorl	$16, %r9d
	leal	0(%r15d,%r9d,1), %r15d
	xorl	%r15d, %ebx
	rorl	$20, %ebx
	leal	0(%edi,%ebx,1), %edi
	xorl	%edi, %r9d
	rorl	$24, %r9d
	leal	0(%r15d,%r9d,1), %r15d
	movl	%r15d, 56(%rsp)
	movl	56(%rsp), %r15d
	xorl	%r15d, %ebx
	rorl	$25, %ebx
	movq	%rbp, %r15
	leal	0(%r13d,%r15d,1), %ebp
	movq	%rbp, %r13
	xorl	%r13d, %esi
	rorl	$16, %esi
	movq	%rsi, %rbp
	leal	0(%r14d,%ebp,1), %esi
	xorl	%esi, %r15d
	rorl	$20, %r15d
	leal	0(%r13d,%r15d,1), %r13d
	xorl	%r13d, %ebp
	rorl	$24, %ebp
	leal	0(%esi,%ebp,1), %r14d
	movq	%r15, %rsi
	xorl	%r14d, %esi
	rorl	$25, %esi
	leal	0(%r8d,%eax,1), %r8d
	movl	60(%rsp), %r15d
	xorl	%r8d, %r15d
	rorl	$16, %r15d
	leal	0(%r10d,%r15d,1), %r10d
	xorl	%r10d, %eax
	rorl	$20, %eax
	leal	0(%r8d,%eax,1), %r8d
	xorl	%r8d, %r15d
	rorl	$24, %r15d
	leal	0(%r10d,%r15d,1), %r10d
	xorl	%r10d, %eax
	rorl	$25, %eax
	leal	0(%ecx,%ebx,1), %ecx
	xorl	%ecx, %r15d
	rorl	$16, %r15d
	leal	0(%r14d,%r15d,1), %r14d
	xorl	%r14d, %ebx
	rorl	$20, %ebx
	leal	0(%ecx,%ebx,1), %ecx
	xorl	%ecx, %r15d
	rorl	$24, %r15d
	movl	%r15d, 60(%rsp)
	movl	60(%rsp), %r15d
	leal	0(%r14d,%r15d,1), %r14d
	xorl	%r14d, %ebx
	rorl	$25, %ebx
	leal	0(%edi,%esi,1), %edi
	movq	%r12, %r15
	movq	%rdi, %r12
	xorl	%r12d, %r15d
	rorl	$16, %r15d
	movq	%r10, %rdi
	movq	%r15, %r10
	leal	0(%edi,%r10d,1), %edi
	xorl	%edi, %esi
	rorl	$20, %esi
	leal	0(%r12d,%esi,1), %r12d
	xorl	%r12d, %r10d
	rorl	$24, %r10d
	leal	0(%edi,%r10d,1), %edi
	xorl	%edi, %esi
	rorl	$25, %esi
	leal	0(%r13d,%eax,1), %r13d
	xorl	%r13d, %r9d
	rorl	$16, %r9d
	movq	%r11, %r15
	movq	%r9, %r11
	leal	0(%r15d,%r11d,1), %r9d
	xorl	%r9d, %eax
	rorl	$20, %eax
	leal	0(%r13d,%eax,1), %r13d
	xorl	%r13d, %r11d
	rorl	$24, %r11d
	movq	%r11, %r15
	leal	0(%r9d,%r15d,1), %r9d
	xorl	%r9d, %eax
	rorl	$25, %eax
	movq	%r8, %r11
	leal	0(%r11d,%edx,1), %r8d
	xorl	%r8d, %ebp
	rorl	$16, %ebp
	movl	56(%rsp), %r11d
	leal	0(%r11d,%ebp,1), %r11d
	xorl	%r11d, %edx
	rorl	$20, %edx
	leal	0(%r8d,%edx,1), %r8d
	xorl	%r8d, %ebp
	rorl	$24, %ebp
	leal	0(%r11d,%ebp,1), %r11d
	xorl	%r11d, %edx
	rorl	$25, %edx
	leal	0(%ecx,%edx,1), %ecx
	xorl	%ecx, %r10d
	rorl	$16, %r10d
	leal	0(%r9d,%r10d,1), %r9d
	xorl	%r9d, %edx
	rorl	$20, %edx
	leal	0(%ecx,%edx,1), %ecx
	xorl	%ecx, %r10d
	rorl	$24, %r10d
	leal	0(%r9d,%r10d,1), %r9d
	xorl	%r9d, %edx
	rorl	$25, %edx
	movl	%edx, 56(%rsp)
	movq	%r12, %rdx
	leal	0(%edx,%ebx,1), %edx
	movq	%r15, %r12
	xorl	%edx, %r12d
	rorl	$16, %r12d
	movq	%r12, %r15
	leal	0(%r11d,%r15d,1), %r11d
	xorl	%r11d, %ebx
	rorl	$20, %ebx
	movq	%rdx, %r12
	movq	%rbx, %rdx
	leal	0(%r12d,%edx,1), %ebx
	movq	%rbx, %r12
	xorl	%r12d, %r15d
	rorl	$24, %r15d
	movq	%r15, %rbx
	leal	0(%r11d,%ebx,1), %r11d
	xorl	%r11d, %edx
	rorl	$25, %edx
	leal	0(%r13d,%esi,1), %r15d
	movq	%rbp, %r13
	movq	%r15, %rbp
	xorl	%ebp, %r13d
	rorl	$16, %r13d
	movq	%r13, %r15
	leal	0(%r14d,%r15d,1), %r13d
	movq	%rsi, %r14
	movq	%r13, %rsi
	xorl	%esi, %r14d
	rorl	$20, %r14d
	movq	%rbp, %r13
	movq	%r14, %rbp
	leal	0(%r13d,%ebp,1), %r13d
	xorl	%r13d, %r15d
	rorl	$24, %r15d
	leal	0(%esi,%r15d,1), %esi
	xorl	%esi, %ebp
	rorl	$25, %ebp
	leal	0(%r8d,%eax,1), %r8d
	movl	60(%rsp), %r14d
	xorl	%r8d, %r14d
	rorl	$16, %r14d
	leal	0(%edi,%r14d,1), %edi
	xorl	%edi, %eax
	rorl	$20, %eax
	leal	0(%r8d,%eax,1), %r8d
	xorl	%r8d, %r14d
	rorl	$24, %r14d
	leal	0(%edi,%r14d,1), %edi
	xorl	%edi, %eax
	rorl	$25, %eax
	leal	0(%ecx,%edx,1), %ecx
	xorl	%ecx, %r14d
	rorl	$16, %r14d
	leal	0(%esi,%r14d,1), %esi
	xorl	%esi, %edx
	rorl	$20, %edx
	leal	0(%ecx,%edx,1), %ecx
	xorl	%ecx, %r14d
	rorl	$24, %r14d
	movl	%r14d, 60(%rsp)
	movl	60(%rsp), %r14d
	leal	0(%esi,%r14d,1), %esi
	xorl	%esi, %edx
	rorl	$25, %edx
	leal	0(%r12d,%ebp,1), %r12d
	xorl	%r12d, %r10d
	rorl	$16, %r10d
	leal	0(%edi,%r10d,1), %edi
	xorl	%edi, %ebp
	rorl	$20, %ebp
	movq	%r12, %r14
	movq	%rbp, %r12
	leal	0(%r14d,%r12d,1), %ebp
	xorl	%ebp, %r10d
	rorl	$24, %r10d
	movq	%rdi, %r14
	movq	%r10, %rdi
	leal	0(%r14d,%edi,1), %r10d
	xorl	%r10d, %r12d
	rorl	$25, %r12d
	leal	0(%r13d,%eax,1), %r13d
	xorl	%r13d, %ebx
	rorl	$16, %ebx
	leal	0(%r9d,%ebx,1), %r9d
	movq	%r9, %r14
	xorl	%r14d, %eax
	rorl	$20, %eax
	movq	%r13, %r9
	leal	0(%r9d,%eax,1), %r9d
	xorl	%r9d, %ebx
	rorl	$24, %ebx
	movq	%rbx, %r13
	leal	0(%r14d,%r13d,1), %r14d
	xorl	%r14d, %eax
	rorl	$25, %eax
	movl	56(%rsp), %ebx
	leal	0(%r8d,%ebx,1), %r8d
	xorl	%r8d, %r15d
	rorl	$16, %r15d
	leal	0(%r11d,%r15d,1), %r11d
	xorl	%r11d, %ebx
	rorl	$20, %ebx
	leal	0(%r8d,%ebx,1), %r8d
	xorl	%r8d, %r15d
	rorl	$24, %r15d
	leal	0(%r11d,%r15d,1), %r11d
	xorl	%r11d, %ebx
	rorl	$25, %ebx
	leal	0(%ecx,%ebx,1), %ecx
	xorl	%ecx, %edi
	rorl	$16, %edi
	leal	0(%r14d,%edi,1), %r14d
	xorl	%r14d, %ebx
	rorl	$20, %ebx
	leal	0(%ecx,%ebx,1), %ecx
	xorl	%ecx, %edi
	rorl	$24, %edi
	leal	0(%r14d,%edi,1), %r14d
	movl	%r14d, 56(%rsp)
	movl	56(%rsp), %r14d
	xorl	%r14d, %ebx
	rorl	$25, %ebx
	leal	0(%ebp,%edx,1), %ebp
	xorl	%ebp, %r13d
	rorl	$16, %r13d
	leal	0(%r11d,%r13d,1), %r11d
	xorl	%r11d, %edx
	rorl	$20, %edx
	leal	0(%ebp,%edx,1), %ebp
	xorl	%ebp, %r13d
	rorl	$24, %r13d
	movq	%r13, %r14
	leal	0(%r11d,%r14d,1), %r11d
	movq	%r11, %r13
	movq	%r13, %r11
	xorl	%r11d, %edx
	rorl	$25, %edx
	leal	0(%r9d,%r12d,1), %r11d
	movq	%r15, %r9
	movq	%r11, %r15
	xorl	%r15d, %r9d
	rorl	$16, %r9d
	leal	0(%esi,%r9d,1), %r11d
	xorl	%r11d, %r12d
	rorl	$20, %r12d
	movq	%r15, %rsi
	leal	0(%esi,%r12d,1), %esi
	xorl	%esi, %r9d
	rorl	$24, %r9d
	movq	%r11, %r15
	movq	%r9, %r11
	leal	0(%r15d,%r11d,1), %r9d
	xorl	%r9d, %r12d
	rorl	$25, %r12d
	leal	0(%r8d,%eax,1), %r8d
	movl	60(%rsp), %r15d
	xorl	%r8d, %r15d
	rorl	$16, %r15d
	leal	0(%r10d,%r15d,1), %r10d
	xorl	%r10d, %eax
	rorl	$20, %eax
	leal	0(%r8d,%eax,1), %r8d
	xorl	%r8d, %r15d
	rorl	$24, %r15d
	leal	0(%r10d,%r15d,1), %r10d
	xorl	%r10d, %eax
	rorl	$25, %eax
	leal	0(%ecx,%edx,1), %ecx
	xorl	%ecx, %r15d
	rorl	$16, %r15d
	leal	0(%r9d,%r15d,1), %r9d
	xorl	%r9d, %edx
	rorl	$20, %edx
	leal	0(%ecx,%edx,1), %ecx
	xorl	%ecx, %r15d
	rorl	$24, %r15d
	movl	%r15d, 60(%rsp)
	movl	60(%rsp), %r15d
	leal	0(%r9d,%r15d,1), %r9d
	xorl	%r9d, %edx
	rorl	$25, %edx
	leal	0(%ebp,%r12d,1), %ebp
	movq	%rbp, %r15
	xorl	%r15d, %edi
	rorl	$16, %edi
	leal	0(%r10d,%edi,1), %ebp
	movq	%r12, %r10
	xorl	%ebp, %r10d
	rorl	$20, %r10d
	movq	%r15, %r12
	leal	0(%r12d,%r10d,1), %r15d
	xorl	%r15d, %edi
	rorl	$24, %edi
	movq	%rdi, %r12
	leal	0(%ebp,%r12d,1), %edi
	xorl	%edi, %r10d
	rorl	$25, %r10d
	leal	0(%esi,%eax,1), %esi
	movq	%rsi, %rbp
	xorl	%ebp, %r14d
	rorl	$16, %r14d
	movl	56(%rsp), %esi
	leal	0(%esi,%r14d,1), %esi
	xorl	%esi, %eax
	rorl	$20, %eax
	leal	0(%ebp,%eax,1), %ebp
	xorl	%ebp, %r14d
	rorl	$24, %r14d
	leal	0(%esi,%r14d,1), %esi
	xorl	%esi, %eax
	rorl	$25, %eax
	leal	0(%r8d,%ebx,1), %r8d
	xorl	%r8d, %r11d
	rorl	$16, %r11d
	leal	0(%r13d,%r11d,1), %r13d
	xorl	%r13d, %ebx
	rorl	$20, %ebx
	leal	0(%r8d,%ebx,1), %r8d
	xorl	%r8d, %r11d
	rorl	$24, %r11d
	leal	0(%r13d,%r11d,1), %r13d
	xorl	%r13d, %ebx
	rorl	$25, %ebx
	leal	0(%ecx,%ebx,1), %ecx
	xorl	%ecx, %r12d
	rorl	$16, %r12d
	leal	0(%esi,%r12d,1), %esi
	xorl	%esi, %ebx
	rorl	$20, %ebx
	leal	0(%ecx,%ebx,1), %ecx
	xorl	%ecx, %r12d
	rorl	$24, %r12d
	leal	0(%esi,%r12d,1), %esi
	xorl	%esi, %ebx
	rorl	$25, %ebx
	movl	%ebx, 64(%rsp)
	movq	%r15, %rbx
	leal	0(%ebx,%edx,1), %r15d
	movq	%r14, %rbx
	movq	%r15, %r14
	xorl	%r14d, %ebx
	rorl	$16, %ebx
	leal	0(%r13d,%ebx,1), %r13d
	movq	%r13, %r15
	xorl	%r15d, %edx
	rorl	$20, %edx
	movq	%r14, %r13
	leal	0(%r13d,%edx,1), %r13d
	xorl	%r13d, %ebx
	rorl	$24, %ebx
	movq	%r15, %r14
	leal	0(%r14d,%ebx,1), %r15d
	xorl	%r15d, %edx
	rorl	$25, %edx
	leal	0(%ebp,%r10d,1), %r14d
	movq	%r11, %rbp
	movq	%r14, %r11
	xorl	%r11d, %ebp
	rorl	$16, %ebp
	leal	0(%r9d,%ebp,1), %r9d
	xorl	%r9d, %r10d
	rorl	$20, %r10d
	leal	0(%r11d,%r10d,1), %r14d
	movq	%rbp, %r11
	movq	%r14, %rbp
	xorl	%ebp, %r11d
	rorl	$24, %r11d
	leal	0(%r9d,%r11d,1), %r9d
	xorl	%r9d, %r10d
	rorl	$25, %r10d
	leal	0(%r8d,%eax,1), %r8d
	movl	60(%rsp), %r14d
	xorl	%r8d, %r14d
	rorl	$16, %r14d
	leal	0(%edi,%r14d,1), %edi
	xorl	%edi, %eax
	rorl	$20, %eax
	leal	0(%r8d,%eax,1), %r8d
	xorl	%r8d, %r14d
	rorl	$24, %r14d
	leal	0(%edi,%r14d,1), %edi
	xorl	%edi, %eax
	rorl	$25, %eax
	leal	0(%ecx,%edx,1), %ecx
	xorl	%ecx, %r14d
	rorl	$16, %r14d
	leal	0(%r9d,%r14d,1), %r9d
	xorl	%r9d, %edx
	rorl	$20, %edx
	leal	0(%ecx,%edx,1), %ecx
	xorl	%ecx, %r14d
	rorl	$24, %r14d
	movl	%r14d, 56(%rsp)
	movl	56(%rsp), %r14d
	leal	0(%r9d,%r14d,1), %r9d
	xorl	%r9d, %edx
	rorl	$25, %edx
	leal	0(%r13d,%r10d,1), %r14d
	movq	%r12, %r13
	movq	%r14, %r12
	xorl	%r12d, %r13d
	rorl	$16, %r13d
	leal	0(%edi,%r13d,1), %edi
	xorl	%edi, %r10d
	rorl	$20, %r10d
	leal	0(%r12d,%r10d,1), %r12d
	xorl	%r12d, %r13d
	rorl	$24, %r13d
	movq	%r13, %r14
	leal	0(%edi,%r14d,1), %edi
	xorl	%edi, %r10d
	rorl	$25, %r10d
	movq	%rax, %r13
	leal	0(%ebp,%r13d,1), %eax
	xorl	%eax, %ebx
	rorl	$16, %ebx
	leal	0(%esi,%ebx,1), %esi
	xorl	%esi, %r13d
	rorl	$20, %r13d
	leal	0(%eax,%r13d,1), %ebp
	movq	%rbx, %rax
	xorl	%ebp, %eax
	rorl	$24, %eax
	leal	0(%esi,%eax,1), %ebx
	movq	%r13, %rsi
	xorl	%ebx, %esi
	rorl	$25, %esi
	movl	64(%rsp), %r13d
	leal	0(%r8d,%r13d,1), %r8d
	xorl	%r8d, %r11d
	rorl	$16, %r11d
	leal	0(%r15d,%r11d,1), %r15d
	xorl	%r15d, %r13d
	rorl	$20, %r13d
	leal	0(%r8d,%r13d,1), %r8d
	xorl	%r8d, %r11d
	rorl	$24, %r11d
	leal	0(%r15d,%r11d,1), %r15d
	xorl	%r15d, %r13d
	rorl	$25, %r13d
	leal	0(%ecx,%r13d,1), %ecx
	xorl	%ecx, %r14d
	rorl	$16, %r14d
	leal	0(%ebx,%r14d,1), %ebx
	xorl	%ebx, %r13d
	rorl	$20, %r13d
	leal	0(%ecx,%r13d,1), %ecx
	xorl	%ecx, %r14d
	rorl	$24, %r14d
	leal	0(%ebx,%r14d,1), %ebx
	xorl	%ebx, %r13d
	rorl	$25, %r13d
	leal	0(%r12d,%edx,1), %r12d
	xorl	%r12d, %eax
	rorl	$16, %eax
	leal	0(%r15d,%eax,1), %r15d
	xorl	%r15d, %edx
	rorl	$20, %edx
	leal	0(%r12d,%edx,1), %r12d
	xorl	%r12d, %eax
	rorl	$24, %eax
	leal	0(%r15d,%eax,1), %r15d
	movl	%r15d, 64(%rsp)
	xorl	%r15d, %edx
	rorl	$25, %edx
	leal	0(%ebp,%r10d,1), %ebp
	xorl	%ebp, %r11d
	rorl	$16, %r11d
	leal	0(%r9d,%r11d,1), %r9d
	xorl	%r9d, %r10d
	rorl	$20, %r10d
	leal	0(%ebp,%r10d,1), %ebp
	xorl	%ebp, %r11d
	rorl	$24, %r11d
	leal	0(%r9d,%r11d,1), %r9d
	xorl	%r9d, %r10d
	rorl	$25, %r10d
	leal	0(%r8d,%esi,1), %r8d
	movl	56(%rsp), %r15d
	xorl	%r8d, %r15d
	rorl	$16, %r15d
	leal	0(%edi,%r15d,1), %edi
	xorl	%edi, %esi
	rorl	$20, %esi
	leal	0(%r8d,%esi,1), %r8d
	xorl	%r8d, %r15d
	rorl	$24, %r15d
	leal	0(%edi,%r15d,1), %edi
	xorl	%edi, %esi
	rorl	$25, %esi
	leal	0(%ecx,%edx,1), %ecx
	xorl	%ecx, %r15d
	rorl	$16, %r15d
	leal	0(%r9d,%r15d,1), %r9d
	xorl	%r9d, %edx
	rorl	$20, %edx
	leal	0(%ecx,%edx,1), %ecx
	xorl	%ecx, %r15d
	rorl	$24, %r15d
	movl	%r15d, 60(%rsp)
	movl	60(%rsp), %r15d
	leal	0(%r9d,%r15d,1), %r9d
	xorl	%r9d, %edx
	rorl	$25, %edx
	leal	0(%r12d,%r10d,1), %r12d
	movq	%r14, %r15
	movq	%r12, %r14
	xorl	%r14d, %r15d
	rorl	$16, %r15d
	movq	%r15, %r12
	leal	0(%edi,%r12d,1), %edi
	xorl	%edi, %r10d
	rorl	$20, %r10d
	leal	0(%r14d,%r10d,1), %r15d
	xorl	%r15d, %r12d
	rorl	$24, %r12d
	movq	%r12, %r14
	leal	0(%edi,%r14d,1), %edi
	xorl	%edi, %r10d
	rorl	$25, %r10d
	leal	0(%ebp,%esi,1), %ebp
	movq	%rax, %r12
	movq	%rbp, %rax
	xorl	%eax, %r12d
	rorl	$16, %r12d
	movq	%r12, %rbp
	leal	0(%ebx,%ebp,1), %ebx
	xorl	%ebx, %esi
	rorl	$20, %esi
	leal	0(%eax,%esi,1), %eax
	xorl	%eax, %ebp
	rorl	$24, %ebp
	leal	0(%ebx,%ebp,1), %ebx
	movq	%rbx, %r12
	xorl	%r12d, %esi
	rorl	$25, %esi
	leal	0(%r8d,%r13d,1), %r8d
	xorl	%r8d, %r11d
	rorl	$16, %r11d
	movl	64(%rsp), %ebx
	leal	0(%ebx,%r11d,1), %ebx
	xorl	%ebx, %r13d
	rorl	$20, %r13d
	leal	0(%r8d,%r13d,1), %r8d
	xorl	%r8d, %r11d
	rorl	$24, %r11d
	leal	0(%ebx,%r11d,1), %ebx
	xorl	%ebx, %r13d
	rorl	$25, %r13d
	leal	0(%ecx,%r13d,1), %ecx
	xorl	%ecx, %r14d
	rorl	$16, %r14d
	leal	0(%r12d,%r14d,1), %r12d
	xorl	%r12d, %r13d
	rorl	$20, %r13d
	leal	0(%ecx,%r13d,1), %ecx
	xorl	%ecx, %r14d
	rorl	$24, %r14d
	leal	0(%r12d,%r14d,1), %r12d
	xorl	%r12d, %r13d
	rorl	$25, %r13d
	leal	0(%r15d,%edx,1), %r15d
	xorl	%r15d, %ebp
	rorl	$16, %ebp
	leal	0(%ebx,%ebp,1), %ebx
	xorl	%ebx, %edx
	rorl	$20, %edx
	leal	0(%r15d,%edx,1), %r15d
	movl	%r15d, 56(%rsp)
	xorl	%r15d, %ebp
	rorl	$24, %ebp
	leal	0(%ebx,%ebp,1), %ebx
	xorl	%ebx, %edx
	rorl	$25, %edx
	leal	0(%eax,%r10d,1), %eax
	xorl	%eax, %r11d
	rorl	$16, %r11d
	leal	0(%r9d,%r11d,1), %r9d
	xorl	%r9d, %r10d
	rorl	$20, %r10d
	leal	0(%eax,%r10d,1), %eax
	xorl	%eax, %r11d
	rorl	$24, %r11d
	leal	0(%r9d,%r11d,1), %r9d
	xorl	%r9d, %r10d
	rorl	$25, %r10d
	leal	0(%r8d,%esi,1), %r8d
	movl	60(%rsp), %r15d
	xorl	%r8d, %r15d
	rorl	$16, %r15d
	leal	0(%edi,%r15d,1), %edi
	xorl	%edi, %esi
	rorl	$20, %esi
	leal	0(%r8d,%esi,1), %r8d
	xorl	%r8d, %r15d
	rorl	$24, %r15d
	leal	0(%edi,%r15d,1), %edi
	xorl	%edi, %esi
	rorl	$25, %esi
	leal	0(%ecx,%edx,1), %ecx
	xorl	%ecx, %r15d
	rorl	$16, %r15d
	leal	0(%r9d,%r15d,1), %r9d
	xorl	%r9d, %edx
	rorl	$20, %edx
	leal	0(%ecx,%edx,1), %ecx
	movl	%ecx, 200(%rsp)
	xorl	%ecx, %r15d
	rorl	$24, %r15d
	movl	%r15d, 260(%rsp)
	leal	0(%r9d,%r15d,1), %r9d
	movl	%r9d, 240(%rsp)
	xorl	%r9d, %edx
	rorl	$25, %edx
	movl	%edx, 220(%rsp)
	movl	56(%rsp), %edx
	leal	0(%edx,%r10d,1), %r9d
	xorl	%r9d, %r14d
	rorl	$16, %r14d
	leal	0(%edi,%r14d,1), %edi
	xorl	%edi, %r10d
	rorl	$20, %r10d
	leal	0(%r9d,%r10d,1), %r9d
	movl	%r9d, 204(%rsp)
	xorl	%r9d, %r14d
	rorl	$24, %r14d
	movl	%r14d, 248(%rsp)
	leal	0(%edi,%r14d,1), %ecx
	movl	%ecx, 244(%rsp)
	xorl	%ecx, %r10d
	rorl	$25, %r10d
	movl	%r10d, 224(%rsp)
	leal	0(%eax,%esi,1), %eax
	xorl	%eax, %ebp
	rorl	$16, %ebp
	leal	0(%r12d,%ebp,1), %ecx
	xorl	%ecx, %esi
	rorl	$20, %esi
	leal	0(%eax,%esi,1), %edx
	movl	%edx, 208(%rsp)
	xorl	%edx, %ebp
	rorl	$24, %ebp
	movl	%ebp, 252(%rsp)
	leal	0(%ecx,%ebp,1), %r9d
	movl	%r9d, 232(%rsp)
	xorl	%r9d, %esi
	rorl	$25, %esi
	movl	%esi, 228(%rsp)
	leal	0(%r8d,%r13d,1), %edx
	xorl	%edx, %r11d
	rorl	$16, %r11d
	leal	0(%ebx,%r11d,1), %esi
	xorl	%esi, %r13d
	rorl	$20, %r13d
	leal	0(%edx,%r13d,1), %edi
	movl	%edi, 212(%rsp)
	xorl	%edi, %r11d
	rorl	$24, %r11d
	movl	%r11d, 256(%rsp)
	leal	0(%esi,%r11d,1), %r11d
	movl	%r11d, 236(%rsp)
	xorl	%r11d, %r13d
	rorl	$25, %r13d
	movl	%r13d, 216(%rsp)
	xorl	%edx, %edx
.L106:
	leaq	200(%rsp), %rsi
	movl	%edx, %r11d
	movq	80(%rsp), %rdi
	movl	0(%rdi,%r11,4), %eax
	movq	88(%rsp), %rcx
	movl	0(%rcx,%r11,4), %ecx
	leal	0(%eax,%ecx,1), %r8d
	movl	%r8d, 0(%rsi,%r11,4)
	leal	1(%edx), %edx
	cmpl	$16, %edx
	jb	.L106
	movl	248(%rsp), %edx
	movl	76(%rsp), %r11d
	leal	0(%edx,%r11d,1), %eax
	movl	%eax, 248(%rsp)
	xorl	%ebx, %ebx
.L107:
	leal	0(,%ebx,4), %ecx
	movl	%ecx, %r8d
	movq	128(%rsp), %rax
	leaq	0(%rax,%r8,1), %rsi
	leaq	392(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	392(%rsp), %r8d
	leaq	200(%rsp), %rcx
	movl	%ebx, %esi
	movl	0(%rcx,%rsi,4), %r10d
	xorl	%r10d, %r8d
	leal	0(,%ebx,4), %edx
	movl	%edx, %r10d
	movq	120(%rsp), %r9
	leaq	0(%r9,%r10,1), %rdi
	movl	%r8d, 392(%rsp)
	leaq	392(%rsp), %rsi
	movq	$4, %rdx
	call	memcpy
	leal	1(%ebx), %ebx
	cmpl	$16, %ebx
	jb	.L107
	movl	68(%rsp), %ecx
	leal	1(%ecx), %r8d
	movl	%r8d, 68(%rsp)
	jmp	.L104
.L105:
	movl	112(%rsp), %esi
	cmpl	$0, %esi
	jbe	.L108
	movl	72(%rsp), %r9d
	sall	$6, %r9d
	movl	%r9d, %r8d
	movq	104(%rsp), %rdi
	leaq	0(%rdi,%r8,1), %rdx
	movq	%rdx, 88(%rsp)
	movq	96(%rsp), %r9
	leaq	0(%r9,%r8,1), %rsi
	xorl	%r8d, %r8d
	movb	%r8b, 264(%rsp)
	xorl	%r10d, %r10d
	movb	%r10b, 265(%rsp)
	xorl	%r10d, %r10d
	movb	%r10b, 266(%rsp)
	xorl	%r9d, %r9d
	movb	%r9b, 267(%rsp)
	xorl	%r11d, %r11d
	movb	%r11b, 268(%rsp)
	xorl	%r8d, %r8d
	movb	%r8b, 269(%rsp)
	xorl	%edi, %edi
	movb	%dil, 270(%rsp)
	xorl	%r10d, %r10d
	movb	%r10b, 271(%rsp)
	xorl	%ecx, %ecx
	movb	%cl, 272(%rsp)
	xorl	%edx, %edx
	movb	%dl, 273(%rsp)
	xorl	%ecx, %ecx
	movb	%cl, 274(%rsp)
	xorl	%r10d, %r10d
	movb	%r10b, 275(%rsp)
	xorl	%r9d, %r9d
	movb	%r9b, 276(%rsp)
	xorl	%eax, %eax
	movb	%al, 277(%rsp)
	xorl	%edi, %edi
	movb	%dil, 278(%rsp)
	xorl	%r10d, %r10d
	movb	%r10b, 279(%rsp)
	xorl	%ecx, %ecx
	movb	%cl, 280(%rsp)
	xorl	%r10d, %r10d
	movb	%r10b, 281(%rsp)
	xorl	%ecx, %ecx
	movb	%cl, 282(%rsp)
	xorl	%ecx, %ecx
	movb	%cl, 283(%rsp)
	xorl	%edi, %edi
	movb	%dil, 284(%rsp)
	xorl	%r11d, %r11d
	movb	%r11b, 285(%rsp)
	xorl	%edi, %edi
	movb	%dil, 286(%rsp)
	xorl	%edi, %edi
	movb	%dil, 287(%rsp)
	xorl	%edx, %edx
	movb	%dl, 288(%rsp)
	xorl	%edi, %edi
	movb	%dil, 289(%rsp)
	xorl	%r8d, %r8d
	movb	%r8b, 290(%rsp)
	xorl	%r9d, %r9d
	movb	%r9b, 291(%rsp)
	xorl	%edx, %edx
	movb	%dl, 292(%rsp)
	xorl	%edi, %edi
	movb	%dil, 293(%rsp)
	xorl	%r11d, %r11d
	movb	%r11b, 294(%rsp)
	xorl	%r11d, %r11d
	movb	%r11b, 295(%rsp)
	xorl	%eax, %eax
	movb	%al, 296(%rsp)
	xorl	%eax, %eax
	movb	%al, 297(%rsp)
	xorl	%r8d, %r8d
	movb	%r8b, 298(%rsp)
	xorl	%ecx, %ecx
	movb	%cl, 299(%rsp)
	xorl	%edx, %edx
	movb	%dl, 300(%rsp)
	xorl	%r11d, %r11d
	movb	%r11b, 301(%rsp)
	xorl	%edi, %edi
	movb	%dil, 302(%rsp)
	xorl	%r8d, %r8d
	movb	%r8b, 303(%rsp)
	xorl	%edi, %edi
	movb	%dil, 304(%rsp)
	xorl	%r8d, %r8d
	movb	%r8b, 305(%rsp)
	xorl	%r11d, %r11d
	movb	%r11b, 306(%rsp)
	xorl	%edi, %edi
	movb	%dil, 307(%rsp)
	xorl	%r8d, %r8d
	movb	%r8b, 308(%rsp)
	xorl	%r10d, %r10d
	movb	%r10b, 309(%rsp)
	xorl	%r10d, %r10d
	movb	%r10b, 310(%rsp)
	xorl	%edi, %edi
	movb	%dil, 311(%rsp)
	xorl	%r8d, %r8d
	movb	%r8b, 312(%rsp)
	xorl	%r11d, %r11d
	movb	%r11b, 313(%rsp)
	xorl	%ecx, %ecx
	movb	%cl, 314(%rsp)
	xorl	%r10d, %r10d
	movb	%r10b, 315(%rsp)
	xorl	%edx, %edx
	movb	%dl, 316(%rsp)
	xorl	%r9d, %r9d
	movb	%r9b, 317(%rsp)
	xorl	%edx, %edx
	movb	%dl, 318(%rsp)
	xorl	%edi, %edi
	movb	%dil, 319(%rsp)
	xorl	%r8d, %r8d
	movb	%r8b, 320(%rsp)
	xorl	%ecx, %ecx
	movb	%cl, 321(%rsp)
	xorl	%edi, %edi
	movb	%dil, 322(%rsp)
	xorl	%r10d, %r10d
	movb	%r10b, 323(%rsp)
	xorl	%r10d, %r10d
	movb	%r10b, 324(%rsp)
	xorl	%r8d, %r8d
	movb	%r8b, 325(%rsp)
	xorl	%edi, %edi
	movb	%dil, 326(%rsp)
	xorl	%r11d, %r11d
	movb	%r11b, 327(%rsp)
	leaq	264(%rsp), %rdi
	movl	116(%rsp), %ecx
	movl	%ecx, %edx
	call	memcpy
	xorl	%edi, %edi
	movl	%edi, 328(%rsp)
	xorl	%r9d, %r9d
	movl	%r9d, 332(%rsp)
	xorl	%esi, %esi
	movl	%esi, 336(%rsp)
	xorl	%r8d, %r8d
	movl	%r8d, 340(%rsp)
	xorl	%eax, %eax
	movl	%eax, 344(%rsp)
	xorl	%edx, %edx
	movl	%edx, 348(%rsp)
	xorl	%ecx, %ecx
	movl	%ecx, 352(%rsp)
	xorl	%r11d, %r11d
	movl	%r11d, 356(%rsp)
	xorl	%r10d, %r10d
	movl	%r10d, 360(%rsp)
	xorl	%edi, %edi
	movl	%edi, 364(%rsp)
	xorl	%edx, %edx
	movl	%edx, 368(%rsp)
	xorl	%r8d, %r8d
	movl	%r8d, 372(%rsp)
	xorl	%r11d, %r11d
	movl	%r11d, 376(%rsp)
	xorl	%r10d, %r10d
	movl	%r10d, 380(%rsp)
	xorl	%edx, %edx
	movl	%edx, 384(%rsp)
	xorl	%esi, %esi
	movl	%esi, 388(%rsp)
	movl	72(%rsp), %ebx
	leaq	136(%rsp), %r10
	movq	%r10, 72(%rsp)
	leaq	328(%rsp), %r8
	movq	%r8, 80(%rsp)
	movq	$64, %rdx
	movq	72(%rsp), %rsi
	movq	80(%rsp), %rdi
	call	memcpy
	movl	%ebx, 68(%rsp)
	movl	376(%rsp), %eax
	movl	68(%rsp), %edi
	leal	0(%eax,%edi,1), %r12d
	movl	328(%rsp), %r9d
	movl	344(%rsp), %r10d
	leal	0(%r9d,%r10d,1), %eax
	xorl	%eax, %r12d
	rorl	$16, %r12d
	movl	360(%rsp), %esi
	leal	0(%esi,%r12d,1), %edx
	xorl	%edx, %r10d
	rorl	$20, %r10d
	leal	0(%eax,%r10d,1), %r14d
	xorl	%r14d, %r12d
	rorl	$24, %r12d
	leal	0(%edx,%r12d,1), %edi
	movl	%edi, 56(%rsp)
	xorl	%edi, %r10d
	rorl	$25, %r10d
	movl	332(%rsp), %edi
	movl	348(%rsp), %ebx
	leal	0(%edi,%ebx,1), %ecx
	movl	380(%rsp), %r11d
	xorl	%ecx, %r11d
	rorl	$16, %r11d
	movl	364(%rsp), %r9d
	leal	0(%r9d,%r11d,1), %eax
	xorl	%eax, %ebx
	rorl	$20, %ebx
	leal	0(%ecx,%ebx,1), %r13d
	xorl	%r13d, %r11d
	rorl	$24, %r11d
	leal	0(%eax,%r11d,1), %ebp
	xorl	%ebp, %ebx
	rorl	$25, %ebx
	movl	336(%rsp), %eax
	movl	352(%rsp), %esi
	leal	0(%eax,%esi,1), %r9d
	movl	384(%rsp), %r8d
	xorl	%r9d, %r8d
	rorl	$16, %r8d
	movl	368(%rsp), %eax
	leal	0(%eax,%r8d,1), %eax
	xorl	%eax, %esi
	rorl	$20, %esi
	leal	0(%r9d,%esi,1), %r9d
	xorl	%r9d, %r8d
	rorl	$24, %r8d
	leal	0(%eax,%r8d,1), %edi
	xorl	%edi, %esi
	rorl	$25, %esi
	movl	340(%rsp), %ecx
	movl	356(%rsp), %eax
	leal	0(%ecx,%eax,1), %ecx
	movl	388(%rsp), %edx
	movq	%rcx, %r15
	xorl	%ecx, %edx
	rorl	$16, %edx
	movl	372(%rsp), %ecx
	leal	0(%ecx,%edx,1), %ecx
	xorl	%ecx, %eax
	rorl	$20, %eax
	leal	0(%r15d,%eax,1), %r15d
	movl	%r15d, 60(%rsp)
	xorl	%r15d, %edx
	rorl	$24, %edx
	leal	0(%ecx,%edx,1), %ecx
	xorl	%ecx, %eax
	rorl	$25, %eax
	leal	0(%r14d,%ebx,1), %r14d
	xorl	%r14d, %edx
	rorl	$16, %edx
	leal	0(%edi,%edx,1), %edi
	xorl	%edi, %ebx
	rorl	$20, %ebx
	leal	0(%r14d,%ebx,1), %r14d
	xorl	%r14d, %edx
	rorl	$24, %edx
	movl	%edx, 64(%rsp)
	movl	64(%rsp), %edx
	leal	0(%edi,%edx,1), %edi
	movq	%rdi, %rdx
	xorl	%edx, %ebx
	rorl	$25, %ebx
	leal	0(%r13d,%esi,1), %edi
	xorl	%edi, %r12d
	rorl	$16, %r12d
	movq	%rcx, %r13
	movq	%r12, %rcx
	leal	0(%r13d,%ecx,1), %r12d
	xorl	%r12d, %esi
	rorl	$20, %esi
	leal	0(%edi,%esi,1), %edi
	xorl	%edi, %ecx
	rorl	$24, %ecx
	leal	0(%r12d,%ecx,1), %r12d
	xorl	%r12d, %esi
	rorl	$25, %esi
	leal	0(%r9d,%eax,1), %r9d
	movq	%r9, %r13
	xorl	%r13d, %r11d
	rorl	$16, %r11d
	movl	56(%rsp), %r15d
	movq	%r11, %r9
	leal	0(%r15d,%r9d,1), %r11d
	xorl	%r11d, %eax
	rorl	$20, %eax
	leal	0(%r13d,%eax,1), %r13d
	xorl	%r13d, %r9d
	rorl	$24, %r9d
	movq	%r9, %r15
	leal	0(%r11d,%r15d,1), %r9d
	xorl	%r9d, %eax
	rorl	$25, %eax
	movl	60(%rsp), %r11d
	leal	0(%r11d,%r10d,1), %r11d
	xorl	%r11d, %r8d
	rorl	$16, %r8d
	leal	0(%ebp,%r8d,1), %ebp
	xorl	%ebp, %r10d
	rorl	$20, %r10d
	leal	0(%r11d,%r10d,1), %r11d
	xorl	%r11d, %r8d
	rorl	$24, %r8d
	leal	0(%ebp,%r8d,1), %ebp
	xorl	%ebp, %r10d
	rorl	$25, %r10d
	leal	0(%r14d,%r10d,1), %r14d
	xorl	%r14d, %ecx
	rorl	$16, %ecx
	leal	0(%r9d,%ecx,1), %r9d
	xorl	%r9d, %r10d
	rorl	$20, %r10d
	leal	0(%r14d,%r10d,1), %r14d
	xorl	%r14d, %ecx
	rorl	$24, %ecx
	leal	0(%r9d,%ecx,1), %r9d
	xorl	%r9d, %r10d
	rorl	$25, %r10d
	movl	%r10d, 60(%rsp)
	leal	0(%edi,%ebx,1), %edi
	movq	%r15, %r10
	xorl	%edi, %r10d
	rorl	$16, %r10d
	leal	0(%ebp,%r10d,1), %ebp
	xorl	%ebp, %ebx
	rorl	$20, %ebx
	movq	%rdi, %r15
	movq	%rbx, %rdi
	leal	0(%r15d,%edi,1), %ebx
	xorl	%ebx, %r10d
	rorl	$24, %r10d
	movq	%rbp, %r15
	movq	%r10, %rbp
	leal	0(%r15d,%ebp,1), %r10d
	xorl	%r10d, %edi
	rorl	$25, %edi
	leal	0(%r13d,%esi,1), %r13d
	xorl	%r13d, %r8d
	rorl	$16, %r8d
	leal	0(%edx,%r8d,1), %edx
	movq	%rdx, %r15
	xorl	%r15d, %esi
	rorl	$20, %esi
	movq	%r13, %rdx
	leal	0(%edx,%esi,1), %edx
	xorl	%edx, %r8d
	rorl	$24, %r8d
	movq	%r8, %r13
	leal	0(%r15d,%r13d,1), %r8d
	xorl	%r8d, %esi
	rorl	$25, %esi
	leal	0(%r11d,%eax,1), %r11d
	movl	64(%rsp), %r15d
	xorl	%r11d, %r15d
	rorl	$16, %r15d
	leal	0(%r12d,%r15d,1), %r12d
	xorl	%r12d, %eax
	rorl	$20, %eax
	leal	0(%r11d,%eax,1), %r11d
	xorl	%r11d, %r15d
	rorl	$24, %r15d
	leal	0(%r12d,%r15d,1), %r12d
	xorl	%r12d, %eax
	rorl	$25, %eax
	leal	0(%r14d,%edi,1), %r14d
	xorl	%r14d, %r15d
	rorl	$16, %r15d
	leal	0(%r8d,%r15d,1), %r8d
	xorl	%r8d, %edi
	rorl	$20, %edi
	leal	0(%r14d,%edi,1), %r14d
	xorl	%r14d, %r15d
	rorl	$24, %r15d
	movl	%r15d, 64(%rsp)
	movl	64(%rsp), %r15d
	leal	0(%r8d,%r15d,1), %r8d
	movl	%r8d, 56(%rsp)
	movl	56(%rsp), %r8d
	xorl	%r8d, %edi
	rorl	$25, %edi
	movq	%rbx, %r8
	leal	0(%r8d,%esi,1), %r8d
	xorl	%r8d, %ecx
	rorl	$16, %ecx
	leal	0(%r12d,%ecx,1), %ebx
	movq	%rbx, %r12
	xorl	%r12d, %esi
	rorl	$20, %esi
	leal	0(%r8d,%esi,1), %r8d
	movq	%r8, %rbx
	xorl	%ebx, %ecx
	rorl	$24, %ecx
	movq	%r12, %r8
	leal	0(%r8d,%ecx,1), %r8d
	xorl	%r8d, %esi
	rorl	$25, %esi
	leal	0(%edx,%eax,1), %r12d
	movq	%rbp, %rdx
	movq	%r12, %rbp
	xorl	%ebp, %edx
	rorl	$16, %edx
	leal	0(%r9d,%edx,1), %r9d
	xorl	%r9d, %eax
	rorl	$20, %eax
	leal	0(%ebp,%eax,1), %ebp
	movq	%rbp, %r12
	xorl	%r12d, %edx
	rorl	$24, %edx
	movq	%rdx, %rbp
	leal	0(%r9d,%ebp,1), %edx
	xorl	%edx, %eax
	rorl	$25, %eax
	movq	%r11, %r9
	movl	60(%rsp), %r11d
	leal	0(%r9d,%r11d,1), %r9d
	movq	%r13, %r15
	movq	%r9, %r13
	xorl	%r13d, %r15d
	rorl	$16, %r15d
	movq	%r10, %r9
	movq	%r15, %r10
	leal	0(%r9d,%r10d,1), %r9d
	xorl	%r9d, %r11d
	rorl	$20, %r11d
	movq	%r13, %r15
	movq	%r11, %r13
	leal	0(%r15d,%r13d,1), %r11d
	xorl	%r11d, %r10d
	rorl	$24, %r10d
	leal	0(%r9d,%r10d,1), %r9d
	xorl	%r9d, %r13d
	rorl	$25, %r13d
	leal	0(%r14d,%r13d,1), %r15d
	movq	%rcx, %r14
	movq	%r15, %rcx
	xorl	%ecx, %r14d
	rorl	$16, %r14d
	leal	0(%edx,%r14d,1), %r15d
	movq	%r13, %rdx
	movq	%r15, %r13
	xorl	%r13d, %edx
	rorl	$20, %edx
	leal	0(%ecx,%edx,1), %ecx
	xorl	%ecx, %r14d
	rorl	$24, %r14d
	leal	0(%r13d,%r14d,1), %r13d
	xorl	%r13d, %edx
	rorl	$25, %edx
	leal	0(%ebx,%edi,1), %r15d
	movq	%rbp, %rbx
	xorl	%r15d, %ebx
	rorl	$16, %ebx
	leal	0(%r9d,%ebx,1), %ebp
	movq	%rdi, %r9
	xorl	%ebp, %r9d
	rorl	$20, %r9d
	movq	%r15, %rdi
	leal	0(%edi,%r9d,1), %edi
	xorl	%edi, %ebx
	rorl	$24, %ebx
	leal	0(%ebp,%ebx,1), %ebp
	movl	%ebp, 60(%rsp)
	movl	60(%rsp), %ebp
	xorl	%ebp, %r9d
	rorl	$25, %r9d
	leal	0(%r12d,%esi,1), %r12d
	movq	%r10, %rbp
	movq	%r12, %r10
	xorl	%r10d, %ebp
	rorl	$16, %ebp
	movl	56(%rsp), %r12d
	leal	0(%r12d,%ebp,1), %r12d
	movq	%rsi, %r15
	xorl	%r12d, %r15d
	rorl	$20, %r15d
	movq	%r10, %rsi
	movq	%r15, %r10
	leal	0(%esi,%r10d,1), %esi
	xorl	%esi, %ebp
	rorl	$24, %ebp
	leal	0(%r12d,%ebp,1), %r12d
	xorl	%r12d, %r10d
	rorl	$25, %r10d
	leal	0(%r11d,%eax,1), %r11d
	movl	64(%rsp), %r15d
	xorl	%r11d, %r15d
	rorl	$16, %r15d
	leal	0(%r8d,%r15d,1), %r8d
	xorl	%r8d, %eax
	rorl	$20, %eax
	leal	0(%r11d,%eax,1), %r11d
	xorl	%r11d, %r15d
	rorl	$24, %r15d
	leal	0(%r8d,%r15d,1), %r8d
	xorl	%r8d, %eax
	rorl	$25, %eax
	leal	0(%ecx,%r9d,1), %ecx
	xorl	%ecx, %r15d
	rorl	$16, %r15d
	leal	0(%r12d,%r15d,1), %r12d
	xorl	%r12d, %r9d
	rorl	$20, %r9d
	leal	0(%ecx,%r9d,1), %ecx
	xorl	%ecx, %r15d
	rorl	$24, %r15d
	movl	%r15d, 56(%rsp)
	movl	56(%rsp), %r15d
	leal	0(%r12d,%r15d,1), %r12d
	xorl	%r12d, %r9d
	rorl	$25, %r9d
	leal	0(%edi,%r10d,1), %r15d
	movq	%r14, %rdi
	xorl	%r15d, %edi
	rorl	$16, %edi
	leal	0(%r8d,%edi,1), %r8d
	movq	%r8, %r14
	xorl	%r14d, %r10d
	rorl	$20, %r10d
	movq	%r15, %r8
	leal	0(%r8d,%r10d,1), %r8d
	xorl	%r8d, %edi
	rorl	$24, %edi
	movq	%r14, %r15
	movq	%rdi, %r14
	leal	0(%r15d,%r14d,1), %edi
	xorl	%edi, %r10d
	rorl	$25, %r10d
	leal	0(%esi,%eax,1), %esi
	xorl	%esi, %ebx
	rorl	$16, %ebx
	leal	0(%r13d,%ebx,1), %r13d
	movq	%rax, %r15
	xorl	%r13d, %r15d
	rorl	$20, %r15d
	movq	%rsi, %rax
	movq	%r15, %rsi
	leal	0(%eax,%esi,1), %eax
	xorl	%eax, %ebx
	rorl	$24, %ebx
	leal	0(%r13d,%ebx,1), %r13d
	xorl	%r13d, %esi
	rorl	$25, %esi
	leal	0(%r11d,%edx,1), %r11d
	xorl	%r11d, %ebp
	rorl	$16, %ebp
	movl	60(%rsp), %r15d
	leal	0(%r15d,%ebp,1), %r15d
	xorl	%r15d, %edx
	rorl	$20, %edx
	leal	0(%r11d,%edx,1), %r11d
	xorl	%r11d, %ebp
	rorl	$24, %ebp
	leal	0(%r15d,%ebp,1), %r15d
	xorl	%r15d, %edx
	rorl	$25, %edx
	leal	0(%ecx,%edx,1), %ecx
	xorl	%ecx, %r14d
	rorl	$16, %r14d
	leal	0(%r13d,%r14d,1), %r13d
	xorl	%r13d, %edx
	rorl	$20, %edx
	leal	0(%ecx,%edx,1), %ecx
	xorl	%ecx, %r14d
	rorl	$24, %r14d
	leal	0(%r13d,%r14d,1), %r13d
	xorl	%r13d, %edx
	rorl	$25, %edx
	movl	%edx, 60(%rsp)
	movq	%r9, %rdx
	leal	0(%r8d,%edx,1), %r8d
	xorl	%r8d, %ebx
	rorl	$16, %ebx
	leal	0(%r15d,%ebx,1), %r9d
	xorl	%r9d, %edx
	rorl	$20, %edx
	leal	0(%r8d,%edx,1), %r15d
	movq	%rbx, %r8
	movq	%r15, %rbx
	xorl	%ebx, %r8d
	rorl	$24, %r8d
	leal	0(%r9d,%r8d,1), %r9d
	xorl	%r9d, %edx
	rorl	$25, %edx
	leal	0(%eax,%r10d,1), %r15d
	movq	%rbp, %rax
	movq	%r15, %rbp
	xorl	%ebp, %eax
	rorl	$16, %eax
	leal	0(%r12d,%eax,1), %r12d
	movq	%r12, %r15
	xorl	%r15d, %r10d
	rorl	$20, %r10d
	leal	0(%ebp,%r10d,1), %r12d
	movq	%rax, %rbp
	xorl	%r12d, %ebp
	rorl	$24, %ebp
	movq	%r15, %rax
	leal	0(%eax,%ebp,1), %eax
	xorl	%eax, %r10d
	rorl	$25, %r10d
	leal	0(%r11d,%esi,1), %r11d
	movl	56(%rsp), %r15d
	xorl	%r11d, %r15d
	rorl	$16, %r15d
	leal	0(%edi,%r15d,1), %edi
	xorl	%edi, %esi
	rorl	$20, %esi
	leal	0(%r11d,%esi,1), %r11d
	xorl	%r11d, %r15d
	rorl	$24, %r15d
	leal	0(%edi,%r15d,1), %edi
	xorl	%edi, %esi
	rorl	$25, %esi
	leal	0(%ecx,%edx,1), %ecx
	xorl	%ecx, %r15d
	rorl	$16, %r15d
	leal	0(%eax,%r15d,1), %eax
	xorl	%eax, %edx
	rorl	$20, %edx
	leal	0(%ecx,%edx,1), %ecx
	xorl	%ecx, %r15d
	rorl	$24, %r15d
	movl	%r15d, 56(%rsp)
	movl	56(%rsp), %r15d
	leal	0(%eax,%r15d,1), %eax
	xorl	%eax, %edx
	rorl	$25, %edx
	leal	0(%ebx,%r10d,1), %ebx
	xorl	%ebx, %r14d
	rorl	$16, %r14d
	leal	0(%edi,%r14d,1), %edi
	xorl	%edi, %r10d
	rorl	$20, %r10d
	leal	0(%ebx,%r10d,1), %r15d
	movq	%r14, %rbx
	movq	%r15, %r14
	xorl	%r14d, %ebx
	rorl	$24, %ebx
	leal	0(%edi,%ebx,1), %edi
	xorl	%edi, %r10d
	rorl	$25, %r10d
	leal	0(%r12d,%esi,1), %r15d
	movq	%r8, %r12
	movq	%r15, %r8
	xorl	%r8d, %r12d
	rorl	$16, %r12d
	leal	0(%r13d,%r12d,1), %r13d
	movq	%r13, %r15
	xorl	%r15d, %esi
	rorl	$20, %esi
	leal	0(%r8d,%esi,1), %r8d
	movq	%r8, %r13
	xorl	%r13d, %r12d
	rorl	$24, %r12d
	movq	%r15, %r8
	leal	0(%r8d,%r12d,1), %r8d
	xorl	%r8d, %esi
	rorl	$25, %esi
	movq	%r11, %r15
	movl	60(%rsp), %r11d
	leal	0(%r15d,%r11d,1), %r15d
	xorl	%r15d, %ebp
	rorl	$16, %ebp
	leal	0(%r9d,%ebp,1), %r9d
	xorl	%r9d, %r11d
	rorl	$20, %r11d
	leal	0(%r15d,%r11d,1), %r15d
	xorl	%r15d, %ebp
	rorl	$24, %ebp
	leal	0(%r9d,%ebp,1), %r9d
	xorl	%r9d, %r11d
	rorl	$25, %r11d
	leal	0(%ecx,%r11d,1), %ecx
	xorl	%ecx, %ebx
	rorl	$16, %ebx
	leal	0(%r8d,%ebx,1), %r8d
	xorl	%r8d, %r11d
	rorl	$20, %r11d
	leal	0(%ecx,%r11d,1), %ecx
	xorl	%ecx, %ebx
	rorl	$24, %ebx
	leal	0(%r8d,%ebx,1), %r8d
	movl	%r8d, 60(%rsp)
	movl	60(%rsp), %r8d
	xorl	%r8d, %r11d
	rorl	$25, %r11d
	leal	0(%r14d,%edx,1), %r8d
	xorl	%r8d, %r12d
	rorl	$16, %r12d
	leal	0(%r9d,%r12d,1), %r9d
	xorl	%r9d, %edx
	rorl	$20, %edx
	leal	0(%r8d,%edx,1), %r8d
	xorl	%r8d, %r12d
	rorl	$24, %r12d
	movq	%r9, %r14
	movq	%r12, %r9
	leal	0(%r14d,%r9d,1), %r12d
	xorl	%r12d, %edx
	rorl	$25, %edx
	leal	0(%r13d,%r10d,1), %r13d
	movq	%r13, %r14
	xorl	%r14d, %ebp
	rorl	$16, %ebp
	leal	0(%eax,%ebp,1), %eax
	movq	%rax, %r13
	xorl	%r13d, %r10d
	rorl	$20, %r10d
	movq	%r14, %rax
	leal	0(%eax,%r10d,1), %eax
	xorl	%eax, %ebp
	rorl	$24, %ebp
	movq	%r13, %r14
	movq	%rbp, %r13
	leal	0(%r14d,%r13d,1), %ebp
	xorl	%ebp, %r10d
	rorl	$25, %r10d
	movq	%r15, %r14
	leal	0(%r14d,%esi,1), %r14d
	movl	56(%rsp), %r15d
	xorl	%r14d, %r15d
	rorl	$16, %r15d
	leal	0(%edi,%r15d,1), %edi
	xorl	%edi, %esi
	rorl	$20, %esi
	leal	0(%r14d,%esi,1), %r14d
	xorl	%r14d, %r15d
	rorl	$24, %r15d
	leal	0(%edi,%r15d,1), %edi
	xorl	%edi, %esi
	rorl	$25, %esi
	leal	0(%ecx,%edx,1), %ecx
	xorl	%ecx, %r15d
	rorl	$16, %r15d
	leal	0(%ebp,%r15d,1), %ebp
	xorl	%ebp, %edx
	rorl	$20, %edx
	leal	0(%ecx,%edx,1), %ecx
	xorl	%ecx, %r15d
	rorl	$24, %r15d
	movl	%r15d, 64(%rsp)
	movl	64(%rsp), %r15d
	leal	0(%ebp,%r15d,1), %ebp
	xorl	%ebp, %edx
	rorl	$25, %edx
	leal	0(%r8d,%r10d,1), %r8d
	movq	%r8, %r15
	xorl	%r15d, %ebx
	rorl	$16, %ebx
	movq	%rbx, %r8
	leal	0(%edi,%r8d,1), %edi
	movq	%rdi, %rbx
	xorl	%ebx, %r10d
	rorl	$20, %r10d
	movq	%r15, %rdi
	leal	0(%edi,%r10d,1), %edi
	xorl	%edi, %r8d
	rorl	$24, %r8d
	leal	0(%ebx,%r8d,1), %ebx
	movl	%ebx, 56(%rsp)
	movl	56(%rsp), %ebx
	xorl	%ebx, %r10d
	rorl	$25, %r10d
	leal	0(%eax,%esi,1), %eax
	xorl	%eax, %r9d
	rorl	$16, %r9d
	movl	60(%rsp), %ebx
	leal	0(%ebx,%r9d,1), %ebx
	xorl	%ebx, %esi
	rorl	$20, %esi
	leal	0(%eax,%esi,1), %eax
	xorl	%eax, %r9d
	rorl	$24, %r9d
	leal	0(%ebx,%r9d,1), %r15d
	xorl	%r15d, %esi
	rorl	$25, %esi
	leal	0(%r14d,%r11d,1), %ebx
	xorl	%ebx, %r13d
	rorl	$16, %r13d
	leal	0(%r12d,%r13d,1), %r12d
	xorl	%r12d, %r11d
	rorl	$20, %r11d
	leal	0(%ebx,%r11d,1), %ebx
	xorl	%ebx, %r13d
	rorl	$24, %r13d
	leal	0(%r12d,%r13d,1), %r12d
	xorl	%r12d, %r11d
	rorl	$25, %r11d
	leal	0(%ecx,%r11d,1), %ecx
	xorl	%ecx, %r8d
	rorl	$16, %r8d
	leal	0(%r15d,%r8d,1), %r14d
	movq	%r14, %r15
	xorl	%r15d, %r11d
	rorl	$20, %r11d
	movq	%r11, %r14
	leal	0(%ecx,%r14d,1), %r11d
	xorl	%r11d, %r8d
	rorl	$24, %r8d
	movq	%r8, %rcx
	leal	0(%r15d,%ecx,1), %r8d
	xorl	%r8d, %r14d
	rorl	$25, %r14d
	movl	%r14d, 60(%rsp)
	leal	0(%edi,%edx,1), %r14d
	movq	%r9, %rdi
	movq	%r14, %r9
	xorl	%r9d, %edi
	rorl	$16, %edi
	leal	0(%r12d,%edi,1), %r12d
	xorl	%r12d, %edx
	rorl	$20, %edx
	leal	0(%r9d,%edx,1), %r9d
	movq	%r9, %r14
	xorl	%r14d, %edi
	rorl	$24, %edi
	movq	%r12, %r9
	movq	%rdi, %r12
	leal	0(%r9d,%r12d,1), %r9d
	xorl	%r9d, %edx
	rorl	$25, %edx
	leal	0(%eax,%r10d,1), %edi
	movq	%r13, %rax
	xorl	%edi, %eax
	rorl	$16, %eax
	leal	0(%ebp,%eax,1), %ebp
	movq	%rbp, %r13
	xorl	%r13d, %r10d
	rorl	$20, %r10d
	leal	0(%edi,%r10d,1), %edi
	movq	%rdi, %rbp
	xorl	%ebp, %eax
	rorl	$24, %eax
	movq	%r13, %rdi
	movq	%rax, %r13
	leal	0(%edi,%r13d,1), %eax
	xorl	%eax, %r10d
	rorl	$25, %r10d
	movq	%rbx, %rdi
	leal	0(%edi,%esi,1), %r15d
	movl	64(%rsp), %ebx
	xorl	%r15d, %ebx
	rorl	$16, %ebx
	movl	56(%rsp), %edi
	leal	0(%edi,%ebx,1), %edi
	xorl	%edi, %esi
	rorl	$20, %esi
	leal	0(%r15d,%esi,1), %r15d
	xorl	%r15d, %ebx
	rorl	$24, %ebx
	leal	0(%edi,%ebx,1), %edi
	xorl	%edi, %esi
	rorl	$25, %esi
	leal	0(%r11d,%edx,1), %r11d
	xorl	%r11d, %ebx
	rorl	$16, %ebx
	leal	0(%eax,%ebx,1), %eax
	xorl	%eax, %edx
	rorl	$20, %edx
	leal	0(%r11d,%edx,1), %r11d
	xorl	%r11d, %ebx
	rorl	$24, %ebx
	movl	%ebx, 56(%rsp)
	movl	56(%rsp), %ebx
	leal	0(%eax,%ebx,1), %eax
	movq	%rax, %rbx
	movq	%rbx, %rax
	xorl	%eax, %edx
	rorl	$25, %edx
	movq	%r14, %rax
	leal	0(%eax,%r10d,1), %r14d
	movq	%rcx, %rax
	movq	%r14, %rcx
	xorl	%ecx, %eax
	rorl	$16, %eax
	leal	0(%edi,%eax,1), %edi
	xorl	%edi, %r10d
	rorl	$20, %r10d
	movq	%rcx, %r14
	movq	%r10, %rcx
	leal	0(%r14d,%ecx,1), %r10d
	movq	%r10, %r14
	xorl	%r14d, %eax
	rorl	$24, %eax
	movq	%rax, %r10
	leal	0(%edi,%r10d,1), %edi
	xorl	%edi, %ecx
	rorl	$25, %ecx
	movq	%rbp, %rax
	leal	0(%eax,%esi,1), %ebp
	movq	%r12, %rax
	xorl	%ebp, %eax
	rorl	$16, %eax
	leal	0(%r8d,%eax,1), %r8d
	movq	%rsi, %r12
	movq	%r8, %rsi
	xorl	%esi, %r12d
	rorl	$20, %r12d
	movq	%r12, %r8
	leal	0(%ebp,%r8d,1), %ebp
	movq	%rbp, %r12
	xorl	%r12d, %eax
	rorl	$24, %eax
	movq	%rax, %rbp
	leal	0(%esi,%ebp,1), %esi
	xorl	%esi, %r8d
	rorl	$25, %r8d
	movl	60(%rsp), %eax
	leal	0(%r15d,%eax,1), %r15d
	xorl	%r15d, %r13d
	rorl	$16, %r13d
	leal	0(%r9d,%r13d,1), %r9d
	xorl	%r9d, %eax
	rorl	$20, %eax
	leal	0(%r15d,%eax,1), %r15d
	xorl	%r15d, %r13d
	rorl	$24, %r13d
	leal	0(%r9d,%r13d,1), %r9d
	xorl	%r9d, %eax
	rorl	$25, %eax
	leal	0(%r11d,%eax,1), %r11d
	xorl	%r11d, %r10d
	rorl	$16, %r10d
	leal	0(%esi,%r10d,1), %esi
	xorl	%esi, %eax
	rorl	$20, %eax
	leal	0(%r11d,%eax,1), %r11d
	xorl	%r11d, %r10d
	rorl	$24, %r10d
	leal	0(%esi,%r10d,1), %esi
	movl	%esi, 60(%rsp)
	movl	60(%rsp), %esi
	xorl	%esi, %eax
	rorl	$25, %eax
	leal	0(%r14d,%edx,1), %esi
	xorl	%esi, %ebp
	rorl	$16, %ebp
	leal	0(%r9d,%ebp,1), %r9d
	movq	%rdx, %r14
	xorl	%r9d, %r14d
	rorl	$20, %r14d
	movq	%rsi, %rdx
	movq	%r14, %rsi
	leal	0(%edx,%esi,1), %edx
	xorl	%edx, %ebp
	rorl	$24, %ebp
	movq	%r9, %r14
	movq	%rbp, %r9
	leal	0(%r14d,%r9d,1), %ebp
	xorl	%ebp, %esi
	rorl	$25, %esi
	leal	0(%r12d,%ecx,1), %r14d
	movq	%r13, %r12
	xorl	%r14d, %r12d
	rorl	$16, %r12d
	leal	0(%ebx,%r12d,1), %r13d
	movq	%rcx, %rbx
	xorl	%r13d, %ebx
	rorl	$20, %ebx
	movq	%r14, %rcx
	leal	0(%ecx,%ebx,1), %ecx
	xorl	%ecx, %r12d
	rorl	$24, %r12d
	leal	0(%r13d,%r12d,1), %r13d
	xorl	%r13d, %ebx
	rorl	$25, %ebx
	movq	%r15, %r14
	leal	0(%r14d,%r8d,1), %r14d
	movl	56(%rsp), %r15d
	xorl	%r14d, %r15d
	rorl	$16, %r15d
	leal	0(%edi,%r15d,1), %edi
	xorl	%edi, %r8d
	rorl	$20, %r8d
	leal	0(%r14d,%r8d,1), %r14d
	xorl	%r14d, %r15d
	rorl	$24, %r15d
	leal	0(%edi,%r15d,1), %edi
	xorl	%edi, %r8d
	rorl	$25, %r8d
	leal	0(%r11d,%esi,1), %r11d
	xorl	%r11d, %r15d
	rorl	$16, %r15d
	leal	0(%r13d,%r15d,1), %r13d
	xorl	%r13d, %esi
	rorl	$20, %esi
	leal	0(%r11d,%esi,1), %r11d
	xorl	%r11d, %r15d
	rorl	$24, %r15d
	movl	%r15d, 64(%rsp)
	movl	64(%rsp), %r15d
	leal	0(%r13d,%r15d,1), %r13d
	xorl	%r13d, %esi
	rorl	$25, %esi
	leal	0(%edx,%ebx,1), %edx
	movq	%r10, %r15
	xorl	%edx, %r15d
	rorl	$16, %r15d
	movq	%rdi, %r10
	leal	0(%r10d,%r15d,1), %edi
	xorl	%edi, %ebx
	rorl	$20, %ebx
	movq	%rdx, %r10
	movq	%rbx, %rdx
	leal	0(%r10d,%edx,1), %r10d
	movq	%r15, %rbx
	xorl	%r10d, %ebx
	rorl	$24, %ebx
	movq	%rdi, %r15
	movq	%rbx, %rdi
	leal	0(%r15d,%edi,1), %ebx
	movl	%ebx, 56(%rsp)
	movl	56(%rsp), %ebx
	xorl	%ebx, %edx
	rorl	$25, %edx
	movq	%rcx, %rbx
	movq	%r8, %rcx
	leal	0(%ebx,%ecx,1), %r8d
	movq	%r8, %rbx
	xorl	%ebx, %r9d
	rorl	$16, %r9d
	movl	60(%rsp), %r8d
	leal	0(%r8d,%r9d,1), %r8d
	xorl	%r8d, %ecx
	rorl	$20, %ecx
	leal	0(%ebx,%ecx,1), %ebx
	xorl	%ebx, %r9d
	rorl	$24, %r9d
	leal	0(%r8d,%r9d,1), %r8d
	xorl	%r8d, %ecx
	rorl	$25, %ecx
	leal	0(%r14d,%eax,1), %r14d
	movq	%r14, %r15
	xorl	%r15d, %r12d
	rorl	$16, %r12d
	leal	0(%ebp,%r12d,1), %ebp
	movq	%rbp, %r14
	xorl	%r14d, %eax
	rorl	$20, %eax
	movq	%r15, %rbp
	leal	0(%ebp,%eax,1), %ebp
	xorl	%ebp, %r12d
	rorl	$24, %r12d
	leal	0(%r14d,%r12d,1), %r14d
	movq	%rax, %r15
	movq	%r14, %rax
	xorl	%eax, %r15d
	rorl	$25, %r15d
	movq	%r11, %r14
	movq	%r15, %r11
	leal	0(%r14d,%r11d,1), %r14d
	movq	%r14, %r15
	xorl	%r15d, %edi
	rorl	$16, %edi
	leal	0(%r8d,%edi,1), %r8d
	movq	%r11, %r14
	movq	%r8, %r11
	xorl	%r11d, %r14d
	rorl	$20, %r14d
	movq	%r15, %r8
	leal	0(%r8d,%r14d,1), %r8d
	movq	%rdi, %r15
	xorl	%r8d, %r15d
	rorl	$24, %r15d
	movq	%r11, %rdi
	movq	%r15, %r11
	leal	0(%edi,%r11d,1), %edi
	xorl	%edi, %r14d
	rorl	$25, %r14d
	movl	%r14d, 60(%rsp)
	leal	0(%r10d,%esi,1), %r10d
	xorl	%r10d, %r9d
	rorl	$16, %r9d
	leal	0(%eax,%r9d,1), %eax
	movq	%rax, %r14
	xorl	%r14d, %esi
	rorl	$20, %esi
	leal	0(%r10d,%esi,1), %eax
	movq	%r9, %r10
	xorl	%eax, %r10d
	rorl	$24, %r10d
	movq	%r14, %r9
	leal	0(%r9d,%r10d,1), %r9d
	xorl	%r9d, %esi
	rorl	$25, %esi
	leal	0(%ebx,%edx,1), %ebx
	xorl	%ebx, %r12d
	rorl	$16, %r12d
	leal	0(%r13d,%r12d,1), %r13d
	movq	%r13, %r14
	xorl	%r14d, %edx
	rorl	$20, %edx
	leal	0(%ebx,%edx,1), %ebx
	movq	%rbx, %r13
	xorl	%r13d, %r12d
	rorl	$24, %r12d
	movq	%r14, %rbx
	movq	%r12, %r14
	leal	0(%ebx,%r14d,1), %r12d
	xorl	%r12d, %edx
	rorl	$25, %edx
	movq	%rbp, %rbx
	leal	0(%ebx,%ecx,1), %r15d
	movl	64(%rsp), %ebp
	xorl	%r15d, %ebp
	rorl	$16, %ebp
	movl	56(%rsp), %ebx
	leal	0(%ebx,%ebp,1), %ebx
	xorl	%ebx, %ecx
	rorl	$20, %ecx
	leal	0(%r15d,%ecx,1), %r15d
	xorl	%r15d, %ebp
	rorl	$24, %ebp
	leal	0(%ebx,%ebp,1), %ebx
	xorl	%ebx, %ecx
	rorl	$25, %ecx
	leal	0(%r8d,%esi,1), %r8d
	xorl	%r8d, %ebp
	rorl	$16, %ebp
	leal	0(%r12d,%ebp,1), %r12d
	xorl	%r12d, %esi
	rorl	$20, %esi
	leal	0(%r8d,%esi,1), %r8d
	xorl	%r8d, %ebp
	rorl	$24, %ebp
	movl	%ebp, 64(%rsp)
	movl	64(%rsp), %ebp
	leal	0(%r12d,%ebp,1), %ebp
	movl	%ebp, 56(%rsp)
	movl	56(%rsp), %ebp
	xorl	%ebp, %esi
	rorl	$25, %esi
	leal	0(%eax,%edx,1), %ebp
	movq	%r11, %rax
	movq	%rbp, %r11
	xorl	%r11d, %eax
	rorl	$16, %eax
	leal	0(%ebx,%eax,1), %ebx
	xorl	%ebx, %edx
	rorl	$20, %edx
	leal	0(%r11d,%edx,1), %ebp
	movq	%rax, %r11
	xorl	%ebp, %r11d
	rorl	$24, %r11d
	movq	%rbx, %rax
	leal	0(%eax,%r11d,1), %ebx
	xorl	%ebx, %edx
	rorl	$25, %edx
	movq	%r13, %r12
	movq	%rcx, %rax
	leal	0(%r12d,%eax,1), %ecx
	xorl	%ecx, %r10d
	rorl	$16, %r10d
	leal	0(%edi,%r10d,1), %edi
	xorl	%edi, %eax
	rorl	$20, %eax
	leal	0(%ecx,%eax,1), %ecx
	movq	%r10, %r12
	movq	%rcx, %r10
	xorl	%r10d, %r12d
	rorl	$24, %r12d
	movq	%rdi, %rcx
	movq	%r12, %rdi
	leal	0(%ecx,%edi,1), %r12d
	movq	%rax, %rcx
	xorl	%r12d, %ecx
	rorl	$25, %ecx
	movq	%r15, %r13
	movl	60(%rsp), %eax
	leal	0(%r13d,%eax,1), %r13d
	xorl	%r13d, %r14d
	rorl	$16, %r14d
	leal	0(%r9d,%r14d,1), %r15d
	movq	%rax, %r9
	movq	%r15, %rax
	xorl	%eax, %r9d
	rorl	$20, %r9d
	leal	0(%r13d,%r9d,1), %r13d
	xorl	%r13d, %r14d
	rorl	$24, %r14d
	leal	0(%eax,%r14d,1), %eax
	xorl	%eax, %r9d
	rorl	$25, %r9d
	leal	0(%r8d,%r9d,1), %r8d
	movq	%r11, %r15
	movq	%r8, %r11
	xorl	%r11d, %r15d
	rorl	$16, %r15d
	movq	%r15, %r8
	leal	0(%r12d,%r8d,1), %r12d
	xorl	%r12d, %r9d
	rorl	$20, %r9d
	leal	0(%r11d,%r9d,1), %r11d
	xorl	%r11d, %r8d
	rorl	$24, %r8d
	movq	%r12, %r15
	movq	%r8, %r12
	leal	0(%r15d,%r12d,1), %r8d
	xorl	%r8d, %r9d
	rorl	$25, %r9d
	movq	%rbp, %r15
	movq	%rsi, %rbp
	leal	0(%r15d,%ebp,1), %esi
	xorl	%esi, %edi
	rorl	$16, %edi
	movq	%rax, %r15
	movq	%rdi, %rax
	leal	0(%r15d,%eax,1), %edi
	xorl	%edi, %ebp
	rorl	$20, %ebp
	leal	0(%esi,%ebp,1), %esi
	xorl	%esi, %eax
	rorl	$24, %eax
	leal	0(%edi,%eax,1), %edi
	movl	%edi, 60(%rsp)
	movl	60(%rsp), %edi
	xorl	%edi, %ebp
	movq	%rbp, %rdi
	rorl	$25, %edi
	leal	0(%r10d,%edx,1), %r10d
	movq	%r10, %rbp
	xorl	%ebp, %r14d
	rorl	$16, %r14d
	movl	56(%rsp), %r15d
	movq	%r14, %r10
	leal	0(%r15d,%r10d,1), %r14d
	xorl	%r14d, %edx
	rorl	$20, %edx
	leal	0(%ebp,%edx,1), %ebp
	xorl	%ebp, %r10d
	rorl	$24, %r10d
	leal	0(%r14d,%r10d,1), %r14d
	xorl	%r14d, %edx
	rorl	$25, %edx
	leal	0(%r13d,%ecx,1), %r13d
	movl	64(%rsp), %r15d
	xorl	%r13d, %r15d
	rorl	$16, %r15d
	leal	0(%ebx,%r15d,1), %ebx
	xorl	%ebx, %ecx
	rorl	$20, %ecx
	leal	0(%r13d,%ecx,1), %r13d
	xorl	%r13d, %r15d
	rorl	$24, %r15d
	leal	0(%ebx,%r15d,1), %ebx
	xorl	%ebx, %ecx
	rorl	$25, %ecx
	leal	0(%r11d,%edi,1), %r11d
	xorl	%r11d, %r15d
	rorl	$16, %r15d
	leal	0(%r14d,%r15d,1), %r14d
	xorl	%r14d, %edi
	rorl	$20, %edi
	leal	0(%r11d,%edi,1), %r11d
	xorl	%r11d, %r15d
	rorl	$24, %r15d
	movl	%r15d, 64(%rsp)
	movl	64(%rsp), %r15d
	leal	0(%r14d,%r15d,1), %r14d
	movl	%r14d, 56(%rsp)
	movl	56(%rsp), %r14d
	xorl	%r14d, %edi
	rorl	$25, %edi
	leal	0(%esi,%edx,1), %esi
	movq	%r12, %r14
	movq	%rsi, %r12
	xorl	%r12d, %r14d
	rorl	$16, %r14d
	movq	%rbx, %rsi
	movq	%r14, %rbx
	leal	0(%esi,%ebx,1), %esi
	xorl	%esi, %edx
	rorl	$20, %edx
	leal	0(%r12d,%edx,1), %r12d
	xorl	%r12d, %ebx
	rorl	$24, %ebx
	movq	%rsi, %r14
	movq	%rbx, %rsi
	leal	0(%r14d,%esi,1), %ebx
	xorl	%ebx, %edx
	rorl	$25, %edx
	leal	0(%ebp,%ecx,1), %ebp
	movq	%rax, %r14
	movq	%rbp, %rax
	xorl	%eax, %r14d
	rorl	$16, %r14d
	movq	%r14, %rbp
	leal	0(%r8d,%ebp,1), %r8d
	xorl	%r8d, %ecx
	rorl	$20, %ecx
	leal	0(%eax,%ecx,1), %eax
	xorl	%eax, %ebp
	rorl	$24, %ebp
	leal	0(%r8d,%ebp,1), %r8d
	xorl	%r8d, %ecx
	rorl	$25, %ecx
	movq	%r9, %r14
	leal	0(%r13d,%r14d,1), %r9d
	xorl	%r9d, %r10d
	rorl	$16, %r10d
	movl	60(%rsp), %r13d
	leal	0(%r13d,%r10d,1), %r13d
	xorl	%r13d, %r14d
	rorl	$20, %r14d
	leal	0(%r9d,%r14d,1), %r9d
	xorl	%r9d, %r10d
	rorl	$24, %r10d
	leal	0(%r13d,%r10d,1), %r13d
	xorl	%r13d, %r14d
	rorl	$25, %r14d
	leal	0(%r11d,%r14d,1), %r15d
	movq	%rsi, %r11
	movq	%r15, %rsi
	xorl	%esi, %r11d
	rorl	$16, %r11d
	leal	0(%r8d,%r11d,1), %r8d
	xorl	%r8d, %r14d
	rorl	$20, %r14d
	movq	%rsi, %r15
	movq	%r14, %rsi
	leal	0(%r15d,%esi,1), %r14d
	xorl	%r14d, %r11d
	rorl	$24, %r11d
	leal	0(%r8d,%r11d,1), %r8d
	xorl	%r8d, %esi
	rorl	$25, %esi
	leal	0(%r12d,%edi,1), %r12d
	xorl	%r12d, %ebp
	rorl	$16, %ebp
	leal	0(%r13d,%ebp,1), %r13d
	movq	%r13, %r15
	xorl	%r15d, %edi
	rorl	$20, %edi
	leal	0(%r12d,%edi,1), %r12d
	movq	%r12, %r13
	xorl	%r13d, %ebp
	rorl	$24, %ebp
	movq	%rbp, %r12
	leal	0(%r15d,%r12d,1), %ebp
	xorl	%ebp, %edi
	rorl	$25, %edi
	leal	0(%eax,%edx,1), %eax
	movq	%rax, %r15
	xorl	%r15d, %r10d
	rorl	$16, %r10d
	movl	56(%rsp), %eax
	leal	0(%eax,%r10d,1), %eax
	xorl	%eax, %edx
	rorl	$20, %edx
	leal	0(%r15d,%edx,1), %r15d
	movl	%r15d, 56(%rsp)
	xorl	%r15d, %r10d
	rorl	$24, %r10d
	leal	0(%eax,%r10d,1), %eax
	xorl	%eax, %edx
	rorl	$25, %edx
	leal	0(%r9d,%ecx,1), %r9d
	movl	64(%rsp), %r15d
	xorl	%r9d, %r15d
	rorl	$16, %r15d
	leal	0(%ebx,%r15d,1), %ebx
	xorl	%ebx, %ecx
	rorl	$20, %ecx
	leal	0(%r9d,%ecx,1), %r9d
	xorl	%r9d, %r15d
	rorl	$24, %r15d
	leal	0(%ebx,%r15d,1), %ebx
	xorl	%ebx, %ecx
	rorl	$25, %ecx
	leal	0(%r14d,%edi,1), %r14d
	xorl	%r14d, %r15d
	rorl	$16, %r15d
	leal	0(%eax,%r15d,1), %eax
	xorl	%eax, %edi
	rorl	$20, %edi
	leal	0(%r14d,%edi,1), %r14d
	movl	%r14d, 328(%rsp)
	xorl	%r14d, %r15d
	rorl	$24, %r15d
	movl	%r15d, 388(%rsp)
	leal	0(%eax,%r15d,1), %eax
	movl	%eax, 368(%rsp)
	xorl	%eax, %edi
	rorl	$25, %edi
	movl	%edi, 348(%rsp)
	leal	0(%r13d,%edx,1), %edi
	xorl	%edi, %r11d
	rorl	$16, %r11d
	leal	0(%ebx,%r11d,1), %eax
	xorl	%eax, %edx
	rorl	$20, %edx
	leal	0(%edi,%edx,1), %edi
	movl	%edi, 332(%rsp)
	xorl	%edi, %r11d
	rorl	$24, %r11d
	movl	%r11d, 376(%rsp)
	leal	0(%eax,%r11d,1), %eax
	movl	%eax, 372(%rsp)
	xorl	%eax, %edx
	rorl	$25, %edx
	movl	%edx, 352(%rsp)
	movl	56(%rsp), %r11d
	leal	0(%r11d,%ecx,1), %edx
	xorl	%edx, %r12d
	rorl	$16, %r12d
	leal	0(%r8d,%r12d,1), %eax
	xorl	%eax, %ecx
	rorl	$20, %ecx
	leal	0(%edx,%ecx,1), %r8d
	movl	%r8d, 336(%rsp)
	xorl	%r8d, %r12d
	rorl	$24, %r12d
	movl	%r12d, 380(%rsp)
	leal	0(%eax,%r12d,1), %r11d
	movl	%r11d, 360(%rsp)
	xorl	%r11d, %ecx
	rorl	$25, %ecx
	movl	%ecx, 356(%rsp)
	leal	0(%r9d,%esi,1), %r8d
	xorl	%r8d, %r10d
	rorl	$16, %r10d
	leal	0(%ebp,%r10d,1), %ecx
	xorl	%ecx, %esi
	rorl	$20, %esi
	leal	0(%r8d,%esi,1), %r11d
	movl	%r11d, 340(%rsp)
	xorl	%r11d, %r10d
	rorl	$24, %r10d
	movl	%r10d, 384(%rsp)
	leal	0(%ecx,%r10d,1), %r10d
	movl	%r10d, 364(%rsp)
	xorl	%r10d, %esi
	rorl	$25, %esi
	movl	%esi, 344(%rsp)
	xorl	%r11d, %r11d
.L109:
	leaq	328(%rsp), %r9
	movl	%r11d, %r8d
	movq	80(%rsp), %rdi
	movl	0(%rdi,%r8,4), %ecx
	movq	72(%rsp), %rax
	movl	0(%rax,%r8,4), %edx
	leal	0(%ecx,%edx,1), %esi
	movl	%esi, 0(%r9,%r8,4)
	leal	1(%r11d), %r11d
	cmpl	$16, %r11d
	jb	.L109
	movl	376(%rsp), %edx
	movl	68(%rsp), %esi
	leal	0(%edx,%esi,1), %edx
	movl	%edx, 376(%rsp)
	xorl	%ebx, %ebx
.L110:
	leaq	264(%rsp), %rdi
	leal	0(,%ebx,4), %r11d
	movl	%r11d, %r8d
	leaq	0(%rdi,%r8,1), %rsi
	leaq	392(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	392(%rsp), %esi
	leaq	328(%rsp), %rdx
	movl	%ebx, %eax
	movl	0(%rdx,%rax,4), %r8d
	xorl	%r8d, %esi
	leaq	264(%rsp), %rdi
	leal	0(,%ebx,4), %r9d
	movl	%r9d, %r10d
	leaq	0(%rdi,%r10,1), %rdi
	movl	%esi, 392(%rsp)
	leaq	392(%rsp), %rsi
	movq	$4, %rdx
	call	memcpy
	leal	1(%ebx), %ebx
	cmpl	$16, %ebx
	jb	.L110
	leaq	264(%rsp), %rsi
	movl	116(%rsp), %eax
	movl	%eax, %edx
	movq	88(%rsp), %rdi
	call	memcpy
.L108:
	movq	8(%rsp), %rbx
	movq	16(%rsp), %rbp
	movq	24(%rsp), %r12
	movq	32(%rsp), %r13
	movq	40(%rsp), %r14
	movq	48(%rsp), %r15
	addq	$472, %rsp
	ret
	.cfi_endproc
	.type	Hacl_Chacha20_Vec32_chacha20_encrypt_32, @function
	.size	Hacl_Chacha20_Vec32_chacha20_encrypt_32, . - Hacl_Chacha20_Vec32_chacha20_encrypt_32
	.text
	.align	16
	.globl Hacl_Chacha20_Vec32_chacha20_decrypt_32
Hacl_Chacha20_Vec32_chacha20_decrypt_32:
	.cfi_startproc
	subq	$472, %rsp
	.cfi_adjust_cfa_offset	472
	leaq	480(%rsp), %rax
	movq	%rax, 0(%rsp)
	movq	%rbx, 8(%rsp)
	movq	%rbp, 16(%rsp)
	movq	%r12, 24(%rsp)
	movq	%r13, 32(%rsp)
	movq	%r14, 40(%rsp)
	movq	%r15, 48(%rsp)
	movq	%rcx, %rbp
	movq	%rdx, 96(%rsp)
	movq	%rsi, 104(%rsp)
	movq	%rdi, %rbx
	xorl	%r11d, %r11d
	movl	%r11d, 136(%rsp)
	xorl	%ecx, %ecx
	movl	%ecx, 140(%rsp)
	xorl	%edi, %edi
	movl	%edi, 144(%rsp)
	xorl	%r10d, %r10d
	movl	%r10d, 148(%rsp)
	xorl	%eax, %eax
	movl	%eax, 152(%rsp)
	xorl	%edi, %edi
	movl	%edi, 156(%rsp)
	xorl	%edx, %edx
	movl	%edx, 160(%rsp)
	xorl	%r10d, %r10d
	movl	%r10d, 164(%rsp)
	xorl	%ecx, %ecx
	movl	%ecx, 168(%rsp)
	xorl	%edi, %edi
	movl	%edi, 172(%rsp)
	xorl	%eax, %eax
	movl	%eax, 176(%rsp)
	xorl	%ecx, %ecx
	movl	%ecx, 180(%rsp)
	xorl	%edi, %edi
	movl	%edi, 184(%rsp)
	xorl	%r10d, %r10d
	movl	%r10d, 188(%rsp)
	xorl	%r11d, %r11d
	movl	%r11d, 192(%rsp)
	xorl	%r10d, %r10d
	movl	%r10d, 196(%rsp)
	movq	%r9, %r13
	movq	%r8, %r15
	xorl	%ecx, %ecx
	movl	%ecx, 392(%rsp)
	xorl	%r11d, %r11d
	movl	%r11d, 396(%rsp)
	xorl	%esi, %esi
	movl	%esi, 400(%rsp)
	xorl	%r9d, %r9d
	movl	%r9d, 404(%rsp)
	xorl	%eax, %eax
	movl	%eax, 408(%rsp)
	xorl	%r9d, %r9d
	movl	%r9d, 412(%rsp)
	xorl	%eax, %eax
	movl	%eax, 416(%rsp)
	xorl	%esi, %esi
	movl	%esi, 420(%rsp)
	xorl	%r9d, %r9d
	movl	%r9d, 424(%rsp)
	xorl	%edx, %edx
	movl	%edx, 428(%rsp)
	xorl	%edx, %edx
	movl	%edx, 432(%rsp)
	xorl	%r9d, %r9d
	movl	%r9d, 436(%rsp)
	xorl	%eax, %eax
	movl	%eax, 440(%rsp)
	xorl	%r8d, %r8d
	movl	%r8d, 444(%rsp)
	xorl	%eax, %eax
	movl	%eax, 448(%rsp)
	xorl	%esi, %esi
	movl	%esi, 452(%rsp)
	xorl	%edx, %edx
.L111:
	leaq	392(%rsp), %rcx
	leaq	Hacl_Impl_Chacha20_Vec_chacha20_constants(%rip), %rsi
	movl	%edx, %eax
	movl	0(%rsi,%rax,4), %esi
	movl	%esi, 0(%rcx,%rax,4)
	leal	1(%edx), %edx
	cmpl	$4, %edx
	jb	.L111
	xorl	%r14d, %r14d
.L112:
	leaq	408(%rsp), %r12
	leal	0(,%r14d,4), %r10d
	movl	%r10d, %edx
	leaq	0(%rbp,%rdx,1), %rsi
	leaq	456(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	456(%rsp), %r11d
	movl	%r14d, %r10d
	movl	%r11d, 0(%r12,%r10,4)
	leal	1(%r14d), %r14d
	cmpl	$8, %r14d
	jb	.L112
	movl	%r13d, 440(%rsp)
	xorl	%r12d, %r12d
.L113:
	leaq	444(%rsp), %rbp
	leal	0(,%r12d,4), %r8d
	movl	%r8d, %esi
	leaq	0(%r15,%rsi,1), %rsi
	leaq	456(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	456(%rsp), %ecx
	movl	%r12d, %r11d
	movl	%ecx, 0(%rbp,%r11,4)
	leal	1(%r12d), %r12d
	cmpl	$3, %r12d
	jb	.L113
	xorl	%r11d, %r11d
.L114:
	leaq	136(%rsp), %rax
	leaq	392(%rsp), %r8
	movl	%r11d, %r10d
	movl	0(%r8,%r10,4), %esi
	movl	%esi, 0(%rax,%r10,4)
	leal	1(%r11d), %r11d
	cmpl	$16, %r11d
	jb	.L114
	movl	184(%rsp), %ecx
	leal	0(%ecx), %r9d
	movl	%r9d, 184(%rsp)
	movq	%rbx, %r11
	andl	$63, %r11d
	movl	%r11d, 116(%rsp)
	movq	%rbx, %rcx
	shrl	$6, %ecx
	movl	%ecx, 72(%rsp)
	movl	%r11d, 112(%rsp)
	xorl	%r11d, %r11d
	movl	%r11d, 68(%rsp)
.L115:
	movl	68(%rsp), %r9d
	movl	72(%rsp), %r10d
	cmpl	%r10d, %r9d
	jae	.L116
	movl	68(%rsp), %ecx
	sall	$6, %ecx
	movl	%ecx, %edi
	movq	104(%rsp), %rsi
	leaq	0(%rsi,%rdi,1), %r10
	movq	%r10, 120(%rsp)
	movq	96(%rsp), %r9
	leaq	0(%r9,%rdi,1), %r8
	movq	%r8, 128(%rsp)
	xorl	%ecx, %ecx
	movl	%ecx, 200(%rsp)
	xorl	%esi, %esi
	movl	%esi, 204(%rsp)
	xorl	%eax, %eax
	movl	%eax, 208(%rsp)
	xorl	%eax, %eax
	movl	%eax, 212(%rsp)
	xorl	%edi, %edi
	movl	%edi, 216(%rsp)
	xorl	%edi, %edi
	movl	%edi, 220(%rsp)
	xorl	%r8d, %r8d
	movl	%r8d, 224(%rsp)
	xorl	%esi, %esi
	movl	%esi, 228(%rsp)
	xorl	%r9d, %r9d
	movl	%r9d, 232(%rsp)
	xorl	%edi, %edi
	movl	%edi, 236(%rsp)
	xorl	%edi, %edi
	movl	%edi, 240(%rsp)
	xorl	%r8d, %r8d
	movl	%r8d, 244(%rsp)
	xorl	%r9d, %r9d
	movl	%r9d, 248(%rsp)
	xorl	%edi, %edi
	movl	%edi, 252(%rsp)
	xorl	%eax, %eax
	movl	%eax, 256(%rsp)
	xorl	%edx, %edx
	movl	%edx, 260(%rsp)
	movl	68(%rsp), %ecx
	movq	%rcx, %rbx
	leaq	136(%rsp), %rax
	movq	%rax, 88(%rsp)
	leaq	200(%rsp), %r10
	movq	%r10, 80(%rsp)
	movq	$64, %rdx
	movq	88(%rsp), %rsi
	movq	80(%rsp), %rdi
	call	memcpy
	movq	%rbx, %rdx
	movl	%edx, 76(%rsp)
	movl	248(%rsp), %r10d
	leal	0(%r10d,%edx,1), %r10d
	movl	200(%rsp), %esi
	movl	216(%rsp), %r15d
	leal	0(%esi,%r15d,1), %r11d
	xorl	%r11d, %r10d
	rorl	$16, %r10d
	movl	232(%rsp), %edi
	leal	0(%edi,%r10d,1), %r9d
	xorl	%r9d, %r15d
	rorl	$20, %r15d
	leal	0(%r11d,%r15d,1), %ecx
	xorl	%ecx, %r10d
	rorl	$24, %r10d
	leal	0(%r9d,%r10d,1), %r8d
	xorl	%r8d, %r15d
	rorl	$25, %r15d
	movl	204(%rsp), %r11d
	movl	220(%rsp), %r12d
	leal	0(%r11d,%r12d,1), %edi
	movl	252(%rsp), %ebx
	xorl	%edi, %ebx
	rorl	$16, %ebx
	movl	236(%rsp), %r9d
	leal	0(%r9d,%ebx,1), %esi
	xorl	%esi, %r12d
	rorl	$20, %r12d
	leal	0(%edi,%r12d,1), %edi
	xorl	%edi, %ebx
	rorl	$24, %ebx
	leal	0(%esi,%ebx,1), %r11d
	movl	%r11d, 56(%rsp)
	xorl	%r11d, %r12d
	rorl	$25, %r12d
	movl	208(%rsp), %edx
	movl	224(%rsp), %ebp
	leal	0(%edx,%ebp,1), %r11d
	movl	256(%rsp), %r13d
	xorl	%r11d, %r13d
	rorl	$16, %r13d
	movl	240(%rsp), %eax
	leal	0(%eax,%r13d,1), %r9d
	xorl	%r9d, %ebp
	rorl	$20, %ebp
	leal	0(%r11d,%ebp,1), %r11d
	xorl	%r11d, %r13d
	rorl	$24, %r13d
	leal	0(%r9d,%r13d,1), %r9d
	xorl	%r9d, %ebp
	rorl	$25, %ebp
	movl	212(%rsp), %edx
	movl	228(%rsp), %eax
	leal	0(%edx,%eax,1), %edx
	movl	260(%rsp), %r14d
	xorl	%edx, %r14d
	rorl	$16, %r14d
	movl	244(%rsp), %esi
	leal	0(%esi,%r14d,1), %esi
	xorl	%esi, %eax
	rorl	$20, %eax
	leal	0(%edx,%eax,1), %edx
	xorl	%edx, %r14d
	rorl	$24, %r14d
	leal	0(%esi,%r14d,1), %esi
	xorl	%esi, %eax
	rorl	$25, %eax
	leal	0(%ecx,%r12d,1), %ecx
	xorl	%ecx, %r14d
	rorl	$16, %r14d
	leal	0(%r9d,%r14d,1), %r9d
	xorl	%r9d, %r12d
	rorl	$20, %r12d
	leal	0(%ecx,%r12d,1), %ecx
	xorl	%ecx, %r14d
	rorl	$24, %r14d
	movl	%r14d, 60(%rsp)
	movl	60(%rsp), %r14d
	leal	0(%r9d,%r14d,1), %r9d
	xorl	%r9d, %r12d
	rorl	$25, %r12d
	leal	0(%edi,%ebp,1), %edi
	xorl	%edi, %r10d
	rorl	$16, %r10d
	leal	0(%esi,%r10d,1), %esi
	xorl	%esi, %ebp
	rorl	$20, %ebp
	leal	0(%edi,%ebp,1), %r14d
	xorl	%r14d, %r10d
	rorl	$24, %r10d
	leal	0(%esi,%r10d,1), %edi
	movq	%rdi, %rsi
	xorl	%esi, %ebp
	rorl	$25, %ebp
	leal	0(%r11d,%eax,1), %esi
	xorl	%esi, %ebx
	rorl	$16, %ebx
	leal	0(%r8d,%ebx,1), %r8d
	xorl	%r8d, %eax
	rorl	$20, %eax
	leal	0(%esi,%eax,1), %esi
	xorl	%esi, %ebx
	rorl	$24, %ebx
	leal	0(%r8d,%ebx,1), %r8d
	xorl	%r8d, %eax
	rorl	$25, %eax
	leal	0(%edx,%r15d,1), %r11d
	xorl	%r11d, %r13d
	rorl	$16, %r13d
	movl	56(%rsp), %edx
	leal	0(%edx,%r13d,1), %edx
	xorl	%edx, %r15d
	rorl	$20, %r15d
	leal	0(%r11d,%r15d,1), %r11d
	xorl	%r11d, %r13d
	rorl	$24, %r13d
	leal	0(%edx,%r13d,1), %edx
	xorl	%edx, %r15d
	rorl	$25, %r15d
	leal	0(%ecx,%r15d,1), %ecx
	xorl	%ecx, %r10d
	rorl	$16, %r10d
	leal	0(%r8d,%r10d,1), %r8d
	xorl	%r8d, %r15d
	rorl	$20, %r15d
	leal	0(%ecx,%r15d,1), %ecx
	xorl	%ecx, %r10d
	rorl	$24, %r10d
	leal	0(%r8d,%r10d,1), %r8d
	xorl	%r8d, %r15d
	rorl	$25, %r15d
	movl	%r15d, 56(%rsp)
	leal	0(%r14d,%r12d,1), %r14d
	xorl	%r14d, %ebx
	rorl	$16, %ebx
	leal	0(%edx,%ebx,1), %edx
	movq	%rdx, %r15
	xorl	%r15d, %r12d
	rorl	$20, %r12d
	movq	%r12, %rdx
	leal	0(%r14d,%edx,1), %r12d
	xorl	%r12d, %ebx
	rorl	$24, %ebx
	movq	%r15, %r14
	leal	0(%r14d,%ebx,1), %r15d
	xorl	%r15d, %edx
	rorl	$25, %edx
	leal	0(%esi,%ebp,1), %r14d
	movq	%r13, %rsi
	movq	%r14, %r13
	xorl	%r13d, %esi
	rorl	$16, %esi
	leal	0(%r9d,%esi,1), %r9d
	movq	%r9, %r14
	xorl	%r14d, %ebp
	rorl	$20, %ebp
	movq	%r13, %r9
	leal	0(%r9d,%ebp,1), %r9d
	xorl	%r9d, %esi
	rorl	$24, %esi
	movq	%r14, %r13
	leal	0(%r13d,%esi,1), %r13d
	xorl	%r13d, %ebp
	rorl	$25, %ebp
	leal	0(%r11d,%eax,1), %r11d
	movl	60(%rsp), %r14d
	xorl	%r11d, %r14d
	rorl	$16, %r14d
	leal	0(%edi,%r14d,1), %edi
	xorl	%edi, %eax
	rorl	$20, %eax
	leal	0(%r11d,%eax,1), %r11d
	xorl	%r11d, %r14d
	rorl	$24, %r14d
	leal	0(%edi,%r14d,1), %edi
	xorl	%edi, %eax
	rorl	$25, %eax
	leal	0(%ecx,%edx,1), %ecx
	xorl	%ecx, %r14d
	rorl	$16, %r14d
	leal	0(%r13d,%r14d,1), %r13d
	xorl	%r13d, %edx
	rorl	$20, %edx
	leal	0(%ecx,%edx,1), %ecx
	xorl	%ecx, %r14d
	rorl	$24, %r14d
	movl	%r14d, 60(%rsp)
	movl	60(%rsp), %r14d
	leal	0(%r13d,%r14d,1), %r13d
	xorl	%r13d, %edx
	rorl	$25, %edx
	leal	0(%r12d,%ebp,1), %r14d
	movq	%r10, %r12
	movq	%r14, %r10
	xorl	%r10d, %r12d
	rorl	$16, %r12d
	movq	%r12, %r14
	leal	0(%edi,%r14d,1), %edi
	xorl	%edi, %ebp
	rorl	$20, %ebp
	movq	%rbp, %r12
	leal	0(%r10d,%r12d,1), %r10d
	xorl	%r10d, %r14d
	rorl	$24, %r14d
	movq	%r14, %rbp
	leal	0(%edi,%ebp,1), %edi
	xorl	%edi, %r12d
	rorl	$25, %r12d
	leal	0(%r9d,%eax,1), %r14d
	movq	%rbx, %r9
	movq	%r14, %rbx
	xorl	%ebx, %r9d
	rorl	$16, %r9d
	leal	0(%r8d,%r9d,1), %r8d
	xorl	%r8d, %eax
	rorl	$20, %eax
	leal	0(%ebx,%eax,1), %ebx
	movq	%rbx, %r14
	xorl	%r14d, %r9d
	rorl	$24, %r9d
	leal	0(%r8d,%r9d,1), %ebx
	xorl	%ebx, %eax
	rorl	$25, %eax
	movq	%r11, %r8
	movl	56(%rsp), %r11d
	leal	0(%r8d,%r11d,1), %r8d
	xorl	%r8d, %esi
	rorl	$16, %esi
	leal	0(%r15d,%esi,1), %r15d
	xorl	%r15d, %r11d
	rorl	$20, %r11d
	leal	0(%r8d,%r11d,1), %r8d
	xorl	%r8d, %esi
	rorl	$24, %esi
	leal	0(%r15d,%esi,1), %r15d
	xorl	%r15d, %r11d
	rorl	$25, %r11d
	leal	0(%ecx,%r11d,1), %ecx
	xorl	%ecx, %ebp
	rorl	$16, %ebp
	leal	0(%ebx,%ebp,1), %ebx
	xorl	%ebx, %r11d
	rorl	$20, %r11d
	leal	0(%ecx,%r11d,1), %ecx
	xorl	%ecx, %ebp
	rorl	$24, %ebp
	leal	0(%ebx,%ebp,1), %ebx
	xorl	%ebx, %r11d
	rorl	$25, %r11d
	leal	0(%r10d,%edx,1), %r10d
	xorl	%r10d, %r9d
	rorl	$16, %r9d
	leal	0(%r15d,%r9d,1), %r15d
	xorl	%r15d, %edx
	rorl	$20, %edx
	leal	0(%r10d,%edx,1), %r10d
	xorl	%r10d, %r9d
	rorl	$24, %r9d
	leal	0(%r15d,%r9d,1), %r15d
	movl	%r15d, 56(%rsp)
	movl	56(%rsp), %r15d
	xorl	%r15d, %edx
	rorl	$25, %edx
	leal	0(%r14d,%r12d,1), %r14d
	movq	%r14, %r15
	xorl	%r15d, %esi
	rorl	$16, %esi
	leal	0(%r13d,%esi,1), %r13d
	movq	%r13, %r14
	xorl	%r14d, %r12d
	rorl	$20, %r12d
	movq	%r15, %r13
	leal	0(%r13d,%r12d,1), %r13d
	xorl	%r13d, %esi
	rorl	$24, %esi
	movq	%r14, %r15
	movq	%rsi, %r14
	leal	0(%r15d,%r14d,1), %esi
	xorl	%esi, %r12d
	rorl	$25, %r12d
	leal	0(%r8d,%eax,1), %r8d
	movl	60(%rsp), %r15d
	xorl	%r8d, %r15d
	rorl	$16, %r15d
	leal	0(%edi,%r15d,1), %edi
	xorl	%edi, %eax
	rorl	$20, %eax
	leal	0(%r8d,%eax,1), %r8d
	xorl	%r8d, %r15d
	rorl	$24, %r15d
	leal	0(%edi,%r15d,1), %edi
	xorl	%edi, %eax
	rorl	$25, %eax
	leal	0(%ecx,%edx,1), %ecx
	xorl	%ecx, %r15d
	rorl	$16, %r15d
	leal	0(%esi,%r15d,1), %esi
	xorl	%esi, %edx
	rorl	$20, %edx
	leal	0(%ecx,%edx,1), %ecx
	xorl	%ecx, %r15d
	rorl	$24, %r15d
	movl	%r15d, 60(%rsp)
	movl	60(%rsp), %r15d
	leal	0(%esi,%r15d,1), %esi
	xorl	%esi, %edx
	rorl	$25, %edx
	leal	0(%r10d,%r12d,1), %r15d
	movq	%rbp, %r10
	movq	%r15, %rbp
	xorl	%ebp, %r10d
	rorl	$16, %r10d
	leal	0(%edi,%r10d,1), %edi
	movq	%r12, %r15
	xorl	%edi, %r15d
	rorl	$20, %r15d
	movq	%rbp, %r12
	movq	%r15, %rbp
	leal	0(%r12d,%ebp,1), %r12d
	xorl	%r12d, %r10d
	rorl	$24, %r10d
	leal	0(%edi,%r10d,1), %edi
	xorl	%edi, %ebp
	rorl	$25, %ebp
	leal	0(%r13d,%eax,1), %r13d
	xorl	%r13d, %r9d
	rorl	$16, %r9d
	leal	0(%ebx,%r9d,1), %ebx
	xorl	%ebx, %eax
	rorl	$20, %eax
	leal	0(%r13d,%eax,1), %r13d
	xorl	%r13d, %r9d
	rorl	$24, %r9d
	movq	%r9, %r15
	leal	0(%ebx,%r15d,1), %r9d
	xorl	%r9d, %eax
	rorl	$25, %eax
	leal	0(%r8d,%r11d,1), %r8d
	xorl	%r8d, %r14d
	rorl	$16, %r14d
	movl	56(%rsp), %ebx
	leal	0(%ebx,%r14d,1), %ebx
	xorl	%ebx, %r11d
	rorl	$20, %r11d
	leal	0(%r8d,%r11d,1), %r8d
	xorl	%r8d, %r14d
	rorl	$24, %r14d
	leal	0(%ebx,%r14d,1), %ebx
	xorl	%ebx, %r11d
	rorl	$25, %r11d
	leal	0(%ecx,%r11d,1), %ecx
	xorl	%ecx, %r10d
	rorl	$16, %r10d
	leal	0(%r9d,%r10d,1), %r9d
	xorl	%r9d, %r11d
	rorl	$20, %r11d
	leal	0(%ecx,%r11d,1), %ecx
	xorl	%ecx, %r10d
	rorl	$24, %r10d
	leal	0(%r9d,%r10d,1), %r9d
	xorl	%r9d, %r11d
	rorl	$25, %r11d
	movl	%r11d, 56(%rsp)
	movq	%rdx, %r11
	leal	0(%r12d,%r11d,1), %edx
	xorl	%edx, %r15d
	rorl	$16, %r15d
	leal	0(%ebx,%r15d,1), %ebx
	xorl	%ebx, %r11d
	rorl	$20, %r11d
	leal	0(%edx,%r11d,1), %r12d
	movq	%r15, %rdx
	xorl	%r12d, %edx
	rorl	$24, %edx
	leal	0(%ebx,%edx,1), %r15d
	movq	%r11, %rbx
	xorl	%r15d, %ebx
	rorl	$25, %ebx
	movq	%r13, %r11
	leal	0(%r11d,%ebp,1), %r13d
	movq	%r14, %r11
	xorl	%r13d, %r11d
	rorl	$16, %r11d
	leal	0(%esi,%r11d,1), %esi
	xorl	%esi, %ebp
	rorl	$20, %ebp
	leal	0(%r13d,%ebp,1), %r13d
	movq	%r11, %r14
	movq	%r13, %r11
	xorl	%r11d, %r14d
	rorl	$24, %r14d
	movq	%rsi, %r13
	movq	%r14, %rsi
	leal	0(%r13d,%esi,1), %r13d
	xorl	%r13d, %ebp
	rorl	$25, %ebp
	leal	0(%r8d,%eax,1), %r8d
	movl	60(%rsp), %r14d
	xorl	%r8d, %r14d
	rorl	$16, %r14d
	leal	0(%edi,%r14d,1), %edi
	xorl	%edi, %eax
	rorl	$20, %eax
	leal	0(%r8d,%eax,1), %r8d
	xorl	%r8d, %r14d
	rorl	$24, %r14d
	leal	0(%edi,%r14d,1), %edi
	xorl	%edi, %eax
	rorl	$25, %eax
	leal	0(%ecx,%ebx,1), %ecx
	xorl	%ecx, %r14d
	rorl	$16, %r14d
	leal	0(%r13d,%r14d,1), %r13d
	xorl	%r13d, %ebx
	rorl	$20, %ebx
	leal	0(%ecx,%ebx,1), %ecx
	xorl	%ecx, %r14d
	rorl	$24, %r14d
	movl	%r14d, 60(%rsp)
	movl	60(%rsp), %r14d
	leal	0(%r13d,%r14d,1), %r13d
	movq	%r13, %r14
	movq	%r14, %r13
	xorl	%r13d, %ebx
	rorl	$25, %ebx
	leal	0(%r12d,%ebp,1), %r13d
	movq	%r10, %r12
	movq	%r13, %r10
	xorl	%r10d, %r12d
	rorl	$16, %r12d
	leal	0(%edi,%r12d,1), %edi
	movq	%rdi, %r13
	xorl	%r13d, %ebp
	rorl	$20, %ebp
	movq	%r10, %rdi
	leal	0(%edi,%ebp,1), %edi
	xorl	%edi, %r12d
	rorl	$24, %r12d
	movq	%r13, %r10
	leal	0(%r10d,%r12d,1), %r10d
	xorl	%r10d, %ebp
	rorl	$25, %ebp
	leal	0(%r11d,%eax,1), %r11d
	xorl	%r11d, %edx
	rorl	$16, %edx
	leal	0(%r9d,%edx,1), %r9d
	xorl	%r9d, %eax
	rorl	$20, %eax
	leal	0(%r11d,%eax,1), %r11d
	movq	%r11, %r13
	xorl	%r13d, %edx
	rorl	$24, %edx
	movq	%r9, %r11
	movq	%rdx, %r9
	leal	0(%r11d,%r9d,1), %r11d
	xorl	%r11d, %eax
	rorl	$25, %eax
	movl	56(%rsp), %edx
	leal	0(%r8d,%edx,1), %r8d
	xorl	%r8d, %esi
	rorl	$16, %esi
	leal	0(%r15d,%esi,1), %r15d
	xorl	%r15d, %edx
	rorl	$20, %edx
	leal	0(%r8d,%edx,1), %r8d
	xorl	%r8d, %esi
	rorl	$24, %esi
	leal	0(%r15d,%esi,1), %r15d
	xorl	%r15d, %edx
	rorl	$25, %edx
	leal	0(%ecx,%edx,1), %ecx
	xorl	%ecx, %r12d
	rorl	$16, %r12d
	leal	0(%r11d,%r12d,1), %r11d
	xorl	%r11d, %edx
	rorl	$20, %edx
	leal	0(%ecx,%edx,1), %ecx
	xorl	%ecx, %r12d
	rorl	$24, %r12d
	leal	0(%r11d,%r12d,1), %r11d
	xorl	%r11d, %edx
	rorl	$25, %edx
	leal	0(%edi,%ebx,1), %edi
	xorl	%edi, %r9d
	rorl	$16, %r9d
	leal	0(%r15d,%r9d,1), %r15d
	xorl	%r15d, %ebx
	rorl	$20, %ebx
	leal	0(%edi,%ebx,1), %edi
	xorl	%edi, %r9d
	rorl	$24, %r9d
	leal	0(%r15d,%r9d,1), %r15d
	movl	%r15d, 56(%rsp)
	movl	56(%rsp), %r15d
	xorl	%r15d, %ebx
	rorl	$25, %ebx
	movq	%rbp, %r15
	leal	0(%r13d,%r15d,1), %ebp
	movq	%rbp, %r13
	xorl	%r13d, %esi
	rorl	$16, %esi
	movq	%rsi, %rbp
	leal	0(%r14d,%ebp,1), %esi
	xorl	%esi, %r15d
	rorl	$20, %r15d
	leal	0(%r13d,%r15d,1), %r13d
	xorl	%r13d, %ebp
	rorl	$24, %ebp
	leal	0(%esi,%ebp,1), %r14d
	movq	%r15, %rsi
	xorl	%r14d, %esi
	rorl	$25, %esi
	leal	0(%r8d,%eax,1), %r8d
	movl	60(%rsp), %r15d
	xorl	%r8d, %r15d
	rorl	$16, %r15d
	leal	0(%r10d,%r15d,1), %r10d
	xorl	%r10d, %eax
	rorl	$20, %eax
	leal	0(%r8d,%eax,1), %r8d
	xorl	%r8d, %r15d
	rorl	$24, %r15d
	leal	0(%r10d,%r15d,1), %r10d
	xorl	%r10d, %eax
	rorl	$25, %eax
	leal	0(%ecx,%ebx,1), %ecx
	xorl	%ecx, %r15d
	rorl	$16, %r15d
	leal	0(%r14d,%r15d,1), %r14d
	xorl	%r14d, %ebx
	rorl	$20, %ebx
	leal	0(%ecx,%ebx,1), %ecx
	xorl	%ecx, %r15d
	rorl	$24, %r15d
	movl	%r15d, 60(%rsp)
	movl	60(%rsp), %r15d
	leal	0(%r14d,%r15d,1), %r14d
	xorl	%r14d, %ebx
	rorl	$25, %ebx
	leal	0(%edi,%esi,1), %edi
	movq	%r12, %r15
	movq	%rdi, %r12
	xorl	%r12d, %r15d
	rorl	$16, %r15d
	movq	%r10, %rdi
	movq	%r15, %r10
	leal	0(%edi,%r10d,1), %edi
	xorl	%edi, %esi
	rorl	$20, %esi
	leal	0(%r12d,%esi,1), %r12d
	xorl	%r12d, %r10d
	rorl	$24, %r10d
	leal	0(%edi,%r10d,1), %edi
	xorl	%edi, %esi
	rorl	$25, %esi
	leal	0(%r13d,%eax,1), %r13d
	xorl	%r13d, %r9d
	rorl	$16, %r9d
	movq	%r11, %r15
	movq	%r9, %r11
	leal	0(%r15d,%r11d,1), %r9d
	xorl	%r9d, %eax
	rorl	$20, %eax
	leal	0(%r13d,%eax,1), %r13d
	xorl	%r13d, %r11d
	rorl	$24, %r11d
	movq	%r11, %r15
	leal	0(%r9d,%r15d,1), %r9d
	xorl	%r9d, %eax
	rorl	$25, %eax
	movq	%r8, %r11
	leal	0(%r11d,%edx,1), %r8d
	xorl	%r8d, %ebp
	rorl	$16, %ebp
	movl	56(%rsp), %r11d
	leal	0(%r11d,%ebp,1), %r11d
	xorl	%r11d, %edx
	rorl	$20, %edx
	leal	0(%r8d,%edx,1), %r8d
	xorl	%r8d, %ebp
	rorl	$24, %ebp
	leal	0(%r11d,%ebp,1), %r11d
	xorl	%r11d, %edx
	rorl	$25, %edx
	leal	0(%ecx,%edx,1), %ecx
	xorl	%ecx, %r10d
	rorl	$16, %r10d
	leal	0(%r9d,%r10d,1), %r9d
	xorl	%r9d, %edx
	rorl	$20, %edx
	leal	0(%ecx,%edx,1), %ecx
	xorl	%ecx, %r10d
	rorl	$24, %r10d
	leal	0(%r9d,%r10d,1), %r9d
	xorl	%r9d, %edx
	rorl	$25, %edx
	movl	%edx, 56(%rsp)
	movq	%r12, %rdx
	leal	0(%edx,%ebx,1), %edx
	movq	%r15, %r12
	xorl	%edx, %r12d
	rorl	$16, %r12d
	movq	%r12, %r15
	leal	0(%r11d,%r15d,1), %r11d
	xorl	%r11d, %ebx
	rorl	$20, %ebx
	movq	%rdx, %r12
	movq	%rbx, %rdx
	leal	0(%r12d,%edx,1), %ebx
	movq	%rbx, %r12
	xorl	%r12d, %r15d
	rorl	$24, %r15d
	movq	%r15, %rbx
	leal	0(%r11d,%ebx,1), %r11d
	xorl	%r11d, %edx
	rorl	$25, %edx
	leal	0(%r13d,%esi,1), %r15d
	movq	%rbp, %r13
	movq	%r15, %rbp
	xorl	%ebp, %r13d
	rorl	$16, %r13d
	movq	%r13, %r15
	leal	0(%r14d,%r15d,1), %r13d
	movq	%rsi, %r14
	movq	%r13, %rsi
	xorl	%esi, %r14d
	rorl	$20, %r14d
	movq	%rbp, %r13
	movq	%r14, %rbp
	leal	0(%r13d,%ebp,1), %r13d
	xorl	%r13d, %r15d
	rorl	$24, %r15d
	leal	0(%esi,%r15d,1), %esi
	xorl	%esi, %ebp
	rorl	$25, %ebp
	leal	0(%r8d,%eax,1), %r8d
	movl	60(%rsp), %r14d
	xorl	%r8d, %r14d
	rorl	$16, %r14d
	leal	0(%edi,%r14d,1), %edi
	xorl	%edi, %eax
	rorl	$20, %eax
	leal	0(%r8d,%eax,1), %r8d
	xorl	%r8d, %r14d
	rorl	$24, %r14d
	leal	0(%edi,%r14d,1), %edi
	xorl	%edi, %eax
	rorl	$25, %eax
	leal	0(%ecx,%edx,1), %ecx
	xorl	%ecx, %r14d
	rorl	$16, %r14d
	leal	0(%esi,%r14d,1), %esi
	xorl	%esi, %edx
	rorl	$20, %edx
	leal	0(%ecx,%edx,1), %ecx
	xorl	%ecx, %r14d
	rorl	$24, %r14d
	movl	%r14d, 60(%rsp)
	movl	60(%rsp), %r14d
	leal	0(%esi,%r14d,1), %esi
	xorl	%esi, %edx
	rorl	$25, %edx
	leal	0(%r12d,%ebp,1), %r12d
	xorl	%r12d, %r10d
	rorl	$16, %r10d
	leal	0(%edi,%r10d,1), %edi
	xorl	%edi, %ebp
	rorl	$20, %ebp
	movq	%r12, %r14
	movq	%rbp, %r12
	leal	0(%r14d,%r12d,1), %ebp
	xorl	%ebp, %r10d
	rorl	$24, %r10d
	movq	%rdi, %r14
	movq	%r10, %rdi
	leal	0(%r14d,%edi,1), %r10d
	xorl	%r10d, %r12d
	rorl	$25, %r12d
	leal	0(%r13d,%eax,1), %r13d
	xorl	%r13d, %ebx
	rorl	$16, %ebx
	leal	0(%r9d,%ebx,1), %r9d
	movq	%r9, %r14
	xorl	%r14d, %eax
	rorl	$20, %eax
	movq	%r13, %r9
	leal	0(%r9d,%eax,1), %r9d
	xorl	%r9d, %ebx
	rorl	$24, %ebx
	movq	%rbx, %r13
	leal	0(%r14d,%r13d,1), %r14d
	xorl	%r14d, %eax
	rorl	$25, %eax
	movl	56(%rsp), %ebx
	leal	0(%r8d,%ebx,1), %r8d
	xorl	%r8d, %r15d
	rorl	$16, %r15d
	leal	0(%r11d,%r15d,1), %r11d
	xorl	%r11d, %ebx
	rorl	$20, %ebx
	leal	0(%r8d,%ebx,1), %r8d
	xorl	%r8d, %r15d
	rorl	$24, %r15d
	leal	0(%r11d,%r15d,1), %r11d
	xorl	%r11d, %ebx
	rorl	$25, %ebx
	leal	0(%ecx,%ebx,1), %ecx
	xorl	%ecx, %edi
	rorl	$16, %edi
	leal	0(%r14d,%edi,1), %r14d
	xorl	%r14d, %ebx
	rorl	$20, %ebx
	leal	0(%ecx,%ebx,1), %ecx
	xorl	%ecx, %edi
	rorl	$24, %edi
	leal	0(%r14d,%edi,1), %r14d
	movl	%r14d, 56(%rsp)
	movl	56(%rsp), %r14d
	xorl	%r14d, %ebx
	rorl	$25, %ebx
	leal	0(%ebp,%edx,1), %ebp
	xorl	%ebp, %r13d
	rorl	$16, %r13d
	leal	0(%r11d,%r13d,1), %r11d
	xorl	%r11d, %edx
	rorl	$20, %edx
	leal	0(%ebp,%edx,1), %ebp
	xorl	%ebp, %r13d
	rorl	$24, %r13d
	movq	%r13, %r14
	leal	0(%r11d,%r14d,1), %r11d
	movq	%r11, %r13
	movq	%r13, %r11
	xorl	%r11d, %edx
	rorl	$25, %edx
	leal	0(%r9d,%r12d,1), %r11d
	movq	%r15, %r9
	movq	%r11, %r15
	xorl	%r15d, %r9d
	rorl	$16, %r9d
	leal	0(%esi,%r9d,1), %r11d
	xorl	%r11d, %r12d
	rorl	$20, %r12d
	movq	%r15, %rsi
	leal	0(%esi,%r12d,1), %esi
	xorl	%esi, %r9d
	rorl	$24, %r9d
	movq	%r11, %r15
	movq	%r9, %r11
	leal	0(%r15d,%r11d,1), %r9d
	xorl	%r9d, %r12d
	rorl	$25, %r12d
	leal	0(%r8d,%eax,1), %r8d
	movl	60(%rsp), %r15d
	xorl	%r8d, %r15d
	rorl	$16, %r15d
	leal	0(%r10d,%r15d,1), %r10d
	xorl	%r10d, %eax
	rorl	$20, %eax
	leal	0(%r8d,%eax,1), %r8d
	xorl	%r8d, %r15d
	rorl	$24, %r15d
	leal	0(%r10d,%r15d,1), %r10d
	xorl	%r10d, %eax
	rorl	$25, %eax
	leal	0(%ecx,%edx,1), %ecx
	xorl	%ecx, %r15d
	rorl	$16, %r15d
	leal	0(%r9d,%r15d,1), %r9d
	xorl	%r9d, %edx
	rorl	$20, %edx
	leal	0(%ecx,%edx,1), %ecx
	xorl	%ecx, %r15d
	rorl	$24, %r15d
	movl	%r15d, 60(%rsp)
	movl	60(%rsp), %r15d
	leal	0(%r9d,%r15d,1), %r9d
	xorl	%r9d, %edx
	rorl	$25, %edx
	leal	0(%ebp,%r12d,1), %ebp
	movq	%rbp, %r15
	xorl	%r15d, %edi
	rorl	$16, %edi
	leal	0(%r10d,%edi,1), %ebp
	movq	%r12, %r10
	xorl	%ebp, %r10d
	rorl	$20, %r10d
	movq	%r15, %r12
	leal	0(%r12d,%r10d,1), %r15d
	xorl	%r15d, %edi
	rorl	$24, %edi
	movq	%rdi, %r12
	leal	0(%ebp,%r12d,1), %edi
	xorl	%edi, %r10d
	rorl	$25, %r10d
	leal	0(%esi,%eax,1), %esi
	movq	%rsi, %rbp
	xorl	%ebp, %r14d
	rorl	$16, %r14d
	movl	56(%rsp), %esi
	leal	0(%esi,%r14d,1), %esi
	xorl	%esi, %eax
	rorl	$20, %eax
	leal	0(%ebp,%eax,1), %ebp
	xorl	%ebp, %r14d
	rorl	$24, %r14d
	leal	0(%esi,%r14d,1), %esi
	xorl	%esi, %eax
	rorl	$25, %eax
	leal	0(%r8d,%ebx,1), %r8d
	xorl	%r8d, %r11d
	rorl	$16, %r11d
	leal	0(%r13d,%r11d,1), %r13d
	xorl	%r13d, %ebx
	rorl	$20, %ebx
	leal	0(%r8d,%ebx,1), %r8d
	xorl	%r8d, %r11d
	rorl	$24, %r11d
	leal	0(%r13d,%r11d,1), %r13d
	xorl	%r13d, %ebx
	rorl	$25, %ebx
	leal	0(%ecx,%ebx,1), %ecx
	xorl	%ecx, %r12d
	rorl	$16, %r12d
	leal	0(%esi,%r12d,1), %esi
	xorl	%esi, %ebx
	rorl	$20, %ebx
	leal	0(%ecx,%ebx,1), %ecx
	xorl	%ecx, %r12d
	rorl	$24, %r12d
	leal	0(%esi,%r12d,1), %esi
	xorl	%esi, %ebx
	rorl	$25, %ebx
	movl	%ebx, 64(%rsp)
	movq	%r15, %rbx
	leal	0(%ebx,%edx,1), %r15d
	movq	%r14, %rbx
	movq	%r15, %r14
	xorl	%r14d, %ebx
	rorl	$16, %ebx
	leal	0(%r13d,%ebx,1), %r13d
	movq	%r13, %r15
	xorl	%r15d, %edx
	rorl	$20, %edx
	movq	%r14, %r13
	leal	0(%r13d,%edx,1), %r13d
	xorl	%r13d, %ebx
	rorl	$24, %ebx
	movq	%r15, %r14
	leal	0(%r14d,%ebx,1), %r15d
	xorl	%r15d, %edx
	rorl	$25, %edx
	leal	0(%ebp,%r10d,1), %r14d
	movq	%r11, %rbp
	movq	%r14, %r11
	xorl	%r11d, %ebp
	rorl	$16, %ebp
	leal	0(%r9d,%ebp,1), %r9d
	xorl	%r9d, %r10d
	rorl	$20, %r10d
	leal	0(%r11d,%r10d,1), %r14d
	movq	%rbp, %r11
	movq	%r14, %rbp
	xorl	%ebp, %r11d
	rorl	$24, %r11d
	leal	0(%r9d,%r11d,1), %r9d
	xorl	%r9d, %r10d
	rorl	$25, %r10d
	leal	0(%r8d,%eax,1), %r8d
	movl	60(%rsp), %r14d
	xorl	%r8d, %r14d
	rorl	$16, %r14d
	leal	0(%edi,%r14d,1), %edi
	xorl	%edi, %eax
	rorl	$20, %eax
	leal	0(%r8d,%eax,1), %r8d
	xorl	%r8d, %r14d
	rorl	$24, %r14d
	leal	0(%edi,%r14d,1), %edi
	xorl	%edi, %eax
	rorl	$25, %eax
	leal	0(%ecx,%edx,1), %ecx
	xorl	%ecx, %r14d
	rorl	$16, %r14d
	leal	0(%r9d,%r14d,1), %r9d
	xorl	%r9d, %edx
	rorl	$20, %edx
	leal	0(%ecx,%edx,1), %ecx
	xorl	%ecx, %r14d
	rorl	$24, %r14d
	movl	%r14d, 56(%rsp)
	movl	56(%rsp), %r14d
	leal	0(%r9d,%r14d,1), %r9d
	xorl	%r9d, %edx
	rorl	$25, %edx
	leal	0(%r13d,%r10d,1), %r14d
	movq	%r12, %r13
	movq	%r14, %r12
	xorl	%r12d, %r13d
	rorl	$16, %r13d
	leal	0(%edi,%r13d,1), %edi
	xorl	%edi, %r10d
	rorl	$20, %r10d
	leal	0(%r12d,%r10d,1), %r12d
	xorl	%r12d, %r13d
	rorl	$24, %r13d
	movq	%r13, %r14
	leal	0(%edi,%r14d,1), %edi
	xorl	%edi, %r10d
	rorl	$25, %r10d
	movq	%rax, %r13
	leal	0(%ebp,%r13d,1), %eax
	xorl	%eax, %ebx
	rorl	$16, %ebx
	leal	0(%esi,%ebx,1), %esi
	xorl	%esi, %r13d
	rorl	$20, %r13d
	leal	0(%eax,%r13d,1), %ebp
	movq	%rbx, %rax
	xorl	%ebp, %eax
	rorl	$24, %eax
	leal	0(%esi,%eax,1), %ebx
	movq	%r13, %rsi
	xorl	%ebx, %esi
	rorl	$25, %esi
	movl	64(%rsp), %r13d
	leal	0(%r8d,%r13d,1), %r8d
	xorl	%r8d, %r11d
	rorl	$16, %r11d
	leal	0(%r15d,%r11d,1), %r15d
	xorl	%r15d, %r13d
	rorl	$20, %r13d
	leal	0(%r8d,%r13d,1), %r8d
	xorl	%r8d, %r11d
	rorl	$24, %r11d
	leal	0(%r15d,%r11d,1), %r15d
	xorl	%r15d, %r13d
	rorl	$25, %r13d
	leal	0(%ecx,%r13d,1), %ecx
	xorl	%ecx, %r14d
	rorl	$16, %r14d
	leal	0(%ebx,%r14d,1), %ebx
	xorl	%ebx, %r13d
	rorl	$20, %r13d
	leal	0(%ecx,%r13d,1), %ecx
	xorl	%ecx, %r14d
	rorl	$24, %r14d
	leal	0(%ebx,%r14d,1), %ebx
	xorl	%ebx, %r13d
	rorl	$25, %r13d
	leal	0(%r12d,%edx,1), %r12d
	xorl	%r12d, %eax
	rorl	$16, %eax
	leal	0(%r15d,%eax,1), %r15d
	xorl	%r15d, %edx
	rorl	$20, %edx
	leal	0(%r12d,%edx,1), %r12d
	xorl	%r12d, %eax
	rorl	$24, %eax
	leal	0(%r15d,%eax,1), %r15d
	movl	%r15d, 64(%rsp)
	xorl	%r15d, %edx
	rorl	$25, %edx
	leal	0(%ebp,%r10d,1), %ebp
	xorl	%ebp, %r11d
	rorl	$16, %r11d
	leal	0(%r9d,%r11d,1), %r9d
	xorl	%r9d, %r10d
	rorl	$20, %r10d
	leal	0(%ebp,%r10d,1), %ebp
	xorl	%ebp, %r11d
	rorl	$24, %r11d
	leal	0(%r9d,%r11d,1), %r9d
	xorl	%r9d, %r10d
	rorl	$25, %r10d
	leal	0(%r8d,%esi,1), %r8d
	movl	56(%rsp), %r15d
	xorl	%r8d, %r15d
	rorl	$16, %r15d
	leal	0(%edi,%r15d,1), %edi
	xorl	%edi, %esi
	rorl	$20, %esi
	leal	0(%r8d,%esi,1), %r8d
	xorl	%r8d, %r15d
	rorl	$24, %r15d
	leal	0(%edi,%r15d,1), %edi
	xorl	%edi, %esi
	rorl	$25, %esi
	leal	0(%ecx,%edx,1), %ecx
	xorl	%ecx, %r15d
	rorl	$16, %r15d
	leal	0(%r9d,%r15d,1), %r9d
	xorl	%r9d, %edx
	rorl	$20, %edx
	leal	0(%ecx,%edx,1), %ecx
	xorl	%ecx, %r15d
	rorl	$24, %r15d
	movl	%r15d, 60(%rsp)
	movl	60(%rsp), %r15d
	leal	0(%r9d,%r15d,1), %r9d
	xorl	%r9d, %edx
	rorl	$25, %edx
	leal	0(%r12d,%r10d,1), %r12d
	movq	%r14, %r15
	movq	%r12, %r14
	xorl	%r14d, %r15d
	rorl	$16, %r15d
	movq	%r15, %r12
	leal	0(%edi,%r12d,1), %edi
	xorl	%edi, %r10d
	rorl	$20, %r10d
	leal	0(%r14d,%r10d,1), %r15d
	xorl	%r15d, %r12d
	rorl	$24, %r12d
	movq	%r12, %r14
	leal	0(%edi,%r14d,1), %edi
	xorl	%edi, %r10d
	rorl	$25, %r10d
	leal	0(%ebp,%esi,1), %ebp
	movq	%rax, %r12
	movq	%rbp, %rax
	xorl	%eax, %r12d
	rorl	$16, %r12d
	movq	%r12, %rbp
	leal	0(%ebx,%ebp,1), %ebx
	xorl	%ebx, %esi
	rorl	$20, %esi
	leal	0(%eax,%esi,1), %eax
	xorl	%eax, %ebp
	rorl	$24, %ebp
	leal	0(%ebx,%ebp,1), %ebx
	movq	%rbx, %r12
	xorl	%r12d, %esi
	rorl	$25, %esi
	leal	0(%r8d,%r13d,1), %r8d
	xorl	%r8d, %r11d
	rorl	$16, %r11d
	movl	64(%rsp), %ebx
	leal	0(%ebx,%r11d,1), %ebx
	xorl	%ebx, %r13d
	rorl	$20, %r13d
	leal	0(%r8d,%r13d,1), %r8d
	xorl	%r8d, %r11d
	rorl	$24, %r11d
	leal	0(%ebx,%r11d,1), %ebx
	xorl	%ebx, %r13d
	rorl	$25, %r13d
	leal	0(%ecx,%r13d,1), %ecx
	xorl	%ecx, %r14d
	rorl	$16, %r14d
	leal	0(%r12d,%r14d,1), %r12d
	xorl	%r12d, %r13d
	rorl	$20, %r13d
	leal	0(%ecx,%r13d,1), %ecx
	xorl	%ecx, %r14d
	rorl	$24, %r14d
	leal	0(%r12d,%r14d,1), %r12d
	xorl	%r12d, %r13d
	rorl	$25, %r13d
	leal	0(%r15d,%edx,1), %r15d
	xorl	%r15d, %ebp
	rorl	$16, %ebp
	leal	0(%ebx,%ebp,1), %ebx
	xorl	%ebx, %edx
	rorl	$20, %edx
	leal	0(%r15d,%edx,1), %r15d
	movl	%r15d, 56(%rsp)
	xorl	%r15d, %ebp
	rorl	$24, %ebp
	leal	0(%ebx,%ebp,1), %ebx
	xorl	%ebx, %edx
	rorl	$25, %edx
	leal	0(%eax,%r10d,1), %eax
	xorl	%eax, %r11d
	rorl	$16, %r11d
	leal	0(%r9d,%r11d,1), %r9d
	xorl	%r9d, %r10d
	rorl	$20, %r10d
	leal	0(%eax,%r10d,1), %eax
	xorl	%eax, %r11d
	rorl	$24, %r11d
	leal	0(%r9d,%r11d,1), %r9d
	xorl	%r9d, %r10d
	rorl	$25, %r10d
	leal	0(%r8d,%esi,1), %r8d
	movl	60(%rsp), %r15d
	xorl	%r8d, %r15d
	rorl	$16, %r15d
	leal	0(%edi,%r15d,1), %edi
	xorl	%edi, %esi
	rorl	$20, %esi
	leal	0(%r8d,%esi,1), %r8d
	xorl	%r8d, %r15d
	rorl	$24, %r15d
	leal	0(%edi,%r15d,1), %edi
	xorl	%edi, %esi
	rorl	$25, %esi
	leal	0(%ecx,%edx,1), %ecx
	xorl	%ecx, %r15d
	rorl	$16, %r15d
	leal	0(%r9d,%r15d,1), %r9d
	xorl	%r9d, %edx
	rorl	$20, %edx
	leal	0(%ecx,%edx,1), %ecx
	movl	%ecx, 200(%rsp)
	xorl	%ecx, %r15d
	rorl	$24, %r15d
	movl	%r15d, 260(%rsp)
	leal	0(%r9d,%r15d,1), %r9d
	movl	%r9d, 240(%rsp)
	xorl	%r9d, %edx
	rorl	$25, %edx
	movl	%edx, 220(%rsp)
	movl	56(%rsp), %edx
	leal	0(%edx,%r10d,1), %r9d
	xorl	%r9d, %r14d
	rorl	$16, %r14d
	leal	0(%edi,%r14d,1), %edi
	xorl	%edi, %r10d
	rorl	$20, %r10d
	leal	0(%r9d,%r10d,1), %r9d
	movl	%r9d, 204(%rsp)
	xorl	%r9d, %r14d
	rorl	$24, %r14d
	movl	%r14d, 248(%rsp)
	leal	0(%edi,%r14d,1), %ecx
	movl	%ecx, 244(%rsp)
	xorl	%ecx, %r10d
	rorl	$25, %r10d
	movl	%r10d, 224(%rsp)
	leal	0(%eax,%esi,1), %eax
	xorl	%eax, %ebp
	rorl	$16, %ebp
	leal	0(%r12d,%ebp,1), %ecx
	xorl	%ecx, %esi
	rorl	$20, %esi
	leal	0(%eax,%esi,1), %edx
	movl	%edx, 208(%rsp)
	xorl	%edx, %ebp
	rorl	$24, %ebp
	movl	%ebp, 252(%rsp)
	leal	0(%ecx,%ebp,1), %r9d
	movl	%r9d, 232(%rsp)
	xorl	%r9d, %esi
	rorl	$25, %esi
	movl	%esi, 228(%rsp)
	leal	0(%r8d,%r13d,1), %edx
	xorl	%edx, %r11d
	rorl	$16, %r11d
	leal	0(%ebx,%r11d,1), %esi
	xorl	%esi, %r13d
	rorl	$20, %r13d
	leal	0(%edx,%r13d,1), %edi
	movl	%edi, 212(%rsp)
	xorl	%edi, %r11d
	rorl	$24, %r11d
	movl	%r11d, 256(%rsp)
	leal	0(%esi,%r11d,1), %r11d
	movl	%r11d, 236(%rsp)
	xorl	%r11d, %r13d
	rorl	$25, %r13d
	movl	%r13d, 216(%rsp)
	xorl	%edx, %edx
.L117:
	leaq	200(%rsp), %rsi
	movl	%edx, %r11d
	movq	80(%rsp), %rdi
	movl	0(%rdi,%r11,4), %eax
	movq	88(%rsp), %rcx
	movl	0(%rcx,%r11,4), %ecx
	leal	0(%eax,%ecx,1), %r8d
	movl	%r8d, 0(%rsi,%r11,4)
	leal	1(%edx), %edx
	cmpl	$16, %edx
	jb	.L117
	movl	248(%rsp), %edx
	movl	76(%rsp), %r11d
	leal	0(%edx,%r11d,1), %eax
	movl	%eax, 248(%rsp)
	xorl	%ebx, %ebx
.L118:
	leal	0(,%ebx,4), %ecx
	movl	%ecx, %r8d
	movq	128(%rsp), %rax
	leaq	0(%rax,%r8,1), %rsi
	leaq	392(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	392(%rsp), %r8d
	leaq	200(%rsp), %rcx
	movl	%ebx, %esi
	movl	0(%rcx,%rsi,4), %r10d
	xorl	%r10d, %r8d
	leal	0(,%ebx,4), %edx
	movl	%edx, %r10d
	movq	120(%rsp), %r9
	leaq	0(%r9,%r10,1), %rdi
	movl	%r8d, 392(%rsp)
	leaq	392(%rsp), %rsi
	movq	$4, %rdx
	call	memcpy
	leal	1(%ebx), %ebx
	cmpl	$16, %ebx
	jb	.L118
	movl	68(%rsp), %ecx
	leal	1(%ecx), %r8d
	movl	%r8d, 68(%rsp)
	jmp	.L115
.L116:
	movl	112(%rsp), %esi
	cmpl	$0, %esi
	jbe	.L119
	movl	72(%rsp), %r9d
	sall	$6, %r9d
	movl	%r9d, %r8d
	movq	104(%rsp), %rdi
	leaq	0(%rdi,%r8,1), %rdx
	movq	%rdx, 88(%rsp)
	movq	96(%rsp), %r9
	leaq	0(%r9,%r8,1), %rsi
	xorl	%r8d, %r8d
	movb	%r8b, 264(%rsp)
	xorl	%r10d, %r10d
	movb	%r10b, 265(%rsp)
	xorl	%r10d, %r10d
	movb	%r10b, 266(%rsp)
	xorl	%r9d, %r9d
	movb	%r9b, 267(%rsp)
	xorl	%r11d, %r11d
	movb	%r11b, 268(%rsp)
	xorl	%r8d, %r8d
	movb	%r8b, 269(%rsp)
	xorl	%edi, %edi
	movb	%dil, 270(%rsp)
	xorl	%r10d, %r10d
	movb	%r10b, 271(%rsp)
	xorl	%ecx, %ecx
	movb	%cl, 272(%rsp)
	xorl	%edx, %edx
	movb	%dl, 273(%rsp)
	xorl	%ecx, %ecx
	movb	%cl, 274(%rsp)
	xorl	%r10d, %r10d
	movb	%r10b, 275(%rsp)
	xorl	%r9d, %r9d
	movb	%r9b, 276(%rsp)
	xorl	%eax, %eax
	movb	%al, 277(%rsp)
	xorl	%edi, %edi
	movb	%dil, 278(%rsp)
	xorl	%r10d, %r10d
	movb	%r10b, 279(%rsp)
	xorl	%ecx, %ecx
	movb	%cl, 280(%rsp)
	xorl	%r10d, %r10d
	movb	%r10b, 281(%rsp)
	xorl	%ecx, %ecx
	movb	%cl, 282(%rsp)
	xorl	%ecx, %ecx
	movb	%cl, 283(%rsp)
	xorl	%edi, %edi
	movb	%dil, 284(%rsp)
	xorl	%r11d, %r11d
	movb	%r11b, 285(%rsp)
	xorl	%edi, %edi
	movb	%dil, 286(%rsp)
	xorl	%edi, %edi
	movb	%dil, 287(%rsp)
	xorl	%edx, %edx
	movb	%dl, 288(%rsp)
	xorl	%edi, %edi
	movb	%dil, 289(%rsp)
	xorl	%r8d, %r8d
	movb	%r8b, 290(%rsp)
	xorl	%r9d, %r9d
	movb	%r9b, 291(%rsp)
	xorl	%edx, %edx
	movb	%dl, 292(%rsp)
	xorl	%edi, %edi
	movb	%dil, 293(%rsp)
	xorl	%r11d, %r11d
	movb	%r11b, 294(%rsp)
	xorl	%r11d, %r11d
	movb	%r11b, 295(%rsp)
	xorl	%eax, %eax
	movb	%al, 296(%rsp)
	xorl	%eax, %eax
	movb	%al, 297(%rsp)
	xorl	%r8d, %r8d
	movb	%r8b, 298(%rsp)
	xorl	%ecx, %ecx
	movb	%cl, 299(%rsp)
	xorl	%edx, %edx
	movb	%dl, 300(%rsp)
	xorl	%r11d, %r11d
	movb	%r11b, 301(%rsp)
	xorl	%edi, %edi
	movb	%dil, 302(%rsp)
	xorl	%r8d, %r8d
	movb	%r8b, 303(%rsp)
	xorl	%edi, %edi
	movb	%dil, 304(%rsp)
	xorl	%r8d, %r8d
	movb	%r8b, 305(%rsp)
	xorl	%r11d, %r11d
	movb	%r11b, 306(%rsp)
	xorl	%edi, %edi
	movb	%dil, 307(%rsp)
	xorl	%r8d, %r8d
	movb	%r8b, 308(%rsp)
	xorl	%r10d, %r10d
	movb	%r10b, 309(%rsp)
	xorl	%r10d, %r10d
	movb	%r10b, 310(%rsp)
	xorl	%edi, %edi
	movb	%dil, 311(%rsp)
	xorl	%r8d, %r8d
	movb	%r8b, 312(%rsp)
	xorl	%r11d, %r11d
	movb	%r11b, 313(%rsp)
	xorl	%ecx, %ecx
	movb	%cl, 314(%rsp)
	xorl	%r10d, %r10d
	movb	%r10b, 315(%rsp)
	xorl	%edx, %edx
	movb	%dl, 316(%rsp)
	xorl	%r9d, %r9d
	movb	%r9b, 317(%rsp)
	xorl	%edx, %edx
	movb	%dl, 318(%rsp)
	xorl	%edi, %edi
	movb	%dil, 319(%rsp)
	xorl	%r8d, %r8d
	movb	%r8b, 320(%rsp)
	xorl	%ecx, %ecx
	movb	%cl, 321(%rsp)
	xorl	%edi, %edi
	movb	%dil, 322(%rsp)
	xorl	%r10d, %r10d
	movb	%r10b, 323(%rsp)
	xorl	%r10d, %r10d
	movb	%r10b, 324(%rsp)
	xorl	%r8d, %r8d
	movb	%r8b, 325(%rsp)
	xorl	%edi, %edi
	movb	%dil, 326(%rsp)
	xorl	%r11d, %r11d
	movb	%r11b, 327(%rsp)
	leaq	264(%rsp), %rdi
	movl	116(%rsp), %ecx
	movl	%ecx, %edx
	call	memcpy
	xorl	%edi, %edi
	movl	%edi, 328(%rsp)
	xorl	%r9d, %r9d
	movl	%r9d, 332(%rsp)
	xorl	%esi, %esi
	movl	%esi, 336(%rsp)
	xorl	%r8d, %r8d
	movl	%r8d, 340(%rsp)
	xorl	%eax, %eax
	movl	%eax, 344(%rsp)
	xorl	%edx, %edx
	movl	%edx, 348(%rsp)
	xorl	%ecx, %ecx
	movl	%ecx, 352(%rsp)
	xorl	%r11d, %r11d
	movl	%r11d, 356(%rsp)
	xorl	%r10d, %r10d
	movl	%r10d, 360(%rsp)
	xorl	%edi, %edi
	movl	%edi, 364(%rsp)
	xorl	%edx, %edx
	movl	%edx, 368(%rsp)
	xorl	%r8d, %r8d
	movl	%r8d, 372(%rsp)
	xorl	%r11d, %r11d
	movl	%r11d, 376(%rsp)
	xorl	%r10d, %r10d
	movl	%r10d, 380(%rsp)
	xorl	%edx, %edx
	movl	%edx, 384(%rsp)
	xorl	%esi, %esi
	movl	%esi, 388(%rsp)
	movl	72(%rsp), %ebx
	leaq	136(%rsp), %r10
	movq	%r10, 72(%rsp)
	leaq	328(%rsp), %r8
	movq	%r8, 80(%rsp)
	movq	$64, %rdx
	movq	72(%rsp), %rsi
	movq	80(%rsp), %rdi
	call	memcpy
	movl	%ebx, 68(%rsp)
	movl	376(%rsp), %eax
	movl	68(%rsp), %edi
	leal	0(%eax,%edi,1), %r12d
	movl	328(%rsp), %r9d
	movl	344(%rsp), %r10d
	leal	0(%r9d,%r10d,1), %eax
	xorl	%eax, %r12d
	rorl	$16, %r12d
	movl	360(%rsp), %esi
	leal	0(%esi,%r12d,1), %edx
	xorl	%edx, %r10d
	rorl	$20, %r10d
	leal	0(%eax,%r10d,1), %r14d
	xorl	%r14d, %r12d
	rorl	$24, %r12d
	leal	0(%edx,%r12d,1), %edi
	movl	%edi, 56(%rsp)
	xorl	%edi, %r10d
	rorl	$25, %r10d
	movl	332(%rsp), %edi
	movl	348(%rsp), %ebx
	leal	0(%edi,%ebx,1), %ecx
	movl	380(%rsp), %r11d
	xorl	%ecx, %r11d
	rorl	$16, %r11d
	movl	364(%rsp), %r9d
	leal	0(%r9d,%r11d,1), %eax
	xorl	%eax, %ebx
	rorl	$20, %ebx
	leal	0(%ecx,%ebx,1), %r13d
	xorl	%r13d, %r11d
	rorl	$24, %r11d
	leal	0(%eax,%r11d,1), %ebp
	xorl	%ebp, %ebx
	rorl	$25, %ebx
	movl	336(%rsp), %eax
	movl	352(%rsp), %esi
	leal	0(%eax,%esi,1), %r9d
	movl	384(%rsp), %r8d
	xorl	%r9d, %r8d
	rorl	$16, %r8d
	movl	368(%rsp), %eax
	leal	0(%eax,%r8d,1), %eax
	xorl	%eax, %esi
	rorl	$20, %esi
	leal	0(%r9d,%esi,1), %r9d
	xorl	%r9d, %r8d
	rorl	$24, %r8d
	leal	0(%eax,%r8d,1), %edi
	xorl	%edi, %esi
	rorl	$25, %esi
	movl	340(%rsp), %ecx
	movl	356(%rsp), %eax
	leal	0(%ecx,%eax,1), %ecx
	movl	388(%rsp), %edx
	movq	%rcx, %r15
	xorl	%ecx, %edx
	rorl	$16, %edx
	movl	372(%rsp), %ecx
	leal	0(%ecx,%edx,1), %ecx
	xorl	%ecx, %eax
	rorl	$20, %eax
	leal	0(%r15d,%eax,1), %r15d
	movl	%r15d, 60(%rsp)
	xorl	%r15d, %edx
	rorl	$24, %edx
	leal	0(%ecx,%edx,1), %ecx
	xorl	%ecx, %eax
	rorl	$25, %eax
	leal	0(%r14d,%ebx,1), %r14d
	xorl	%r14d, %edx
	rorl	$16, %edx
	leal	0(%edi,%edx,1), %edi
	xorl	%edi, %ebx
	rorl	$20, %ebx
	leal	0(%r14d,%ebx,1), %r14d
	xorl	%r14d, %edx
	rorl	$24, %edx
	movl	%edx, 64(%rsp)
	movl	64(%rsp), %edx
	leal	0(%edi,%edx,1), %edi
	movq	%rdi, %rdx
	xorl	%edx, %ebx
	rorl	$25, %ebx
	leal	0(%r13d,%esi,1), %edi
	xorl	%edi, %r12d
	rorl	$16, %r12d
	movq	%rcx, %r13
	movq	%r12, %rcx
	leal	0(%r13d,%ecx,1), %r12d
	xorl	%r12d, %esi
	rorl	$20, %esi
	leal	0(%edi,%esi,1), %edi
	xorl	%edi, %ecx
	rorl	$24, %ecx
	leal	0(%r12d,%ecx,1), %r12d
	xorl	%r12d, %esi
	rorl	$25, %esi
	leal	0(%r9d,%eax,1), %r9d
	movq	%r9, %r13
	xorl	%r13d, %r11d
	rorl	$16, %r11d
	movl	56(%rsp), %r15d
	movq	%r11, %r9
	leal	0(%r15d,%r9d,1), %r11d
	xorl	%r11d, %eax
	rorl	$20, %eax
	leal	0(%r13d,%eax,1), %r13d
	xorl	%r13d, %r9d
	rorl	$24, %r9d
	movq	%r9, %r15
	leal	0(%r11d,%r15d,1), %r9d
	xorl	%r9d, %eax
	rorl	$25, %eax
	movl	60(%rsp), %r11d
	leal	0(%r11d,%r10d,1), %r11d
	xorl	%r11d, %r8d
	rorl	$16, %r8d
	leal	0(%ebp,%r8d,1), %ebp
	xorl	%ebp, %r10d
	rorl	$20, %r10d
	leal	0(%r11d,%r10d,1), %r11d
	xorl	%r11d, %r8d
	rorl	$24, %r8d
	leal	0(%ebp,%r8d,1), %ebp
	xorl	%ebp, %r10d
	rorl	$25, %r10d
	leal	0(%r14d,%r10d,1), %r14d
	xorl	%r14d, %ecx
	rorl	$16, %ecx
	leal	0(%r9d,%ecx,1), %r9d
	xorl	%r9d, %r10d
	rorl	$20, %r10d
	leal	0(%r14d,%r10d,1), %r14d
	xorl	%r14d, %ecx
	rorl	$24, %ecx
	leal	0(%r9d,%ecx,1), %r9d
	xorl	%r9d, %r10d
	rorl	$25, %r10d
	movl	%r10d, 60(%rsp)
	leal	0(%edi,%ebx,1), %edi
	movq	%r15, %r10
	xorl	%edi, %r10d
	rorl	$16, %r10d
	leal	0(%ebp,%r10d,1), %ebp
	xorl	%ebp, %ebx
	rorl	$20, %ebx
	movq	%rdi, %r15
	movq	%rbx, %rdi
	leal	0(%r15d,%edi,1), %ebx
	xorl	%ebx, %r10d
	rorl	$24, %r10d
	movq	%rbp, %r15
	movq	%r10, %rbp
	leal	0(%r15d,%ebp,1), %r10d
	xorl	%r10d, %edi
	rorl	$25, %edi
	leal	0(%r13d,%esi,1), %r13d
	xorl	%r13d, %r8d
	rorl	$16, %r8d
	leal	0(%edx,%r8d,1), %edx
	movq	%rdx, %r15
	xorl	%r15d, %esi
	rorl	$20, %esi
	movq	%r13, %rdx
	leal	0(%edx,%esi,1), %edx
	xorl	%edx, %r8d
	rorl	$24, %r8d
	movq	%r8, %r13
	leal	0(%r15d,%r13d,1), %r8d
	xorl	%r8d, %esi
	rorl	$25, %esi
	leal	0(%r11d,%eax,1), %r11d
	movl	64(%rsp), %r15d
	xorl	%r11d, %r15d
	rorl	$16, %r15d
	leal	0(%r12d,%r15d,1), %r12d
	xorl	%r12d, %eax
	rorl	$20, %eax
	leal	0(%r11d,%eax,1), %r11d
	xorl	%r11d, %r15d
	rorl	$24, %r15d
	leal	0(%r12d,%r15d,1), %r12d
	xorl	%r12d, %eax
	rorl	$25, %eax
	leal	0(%r14d,%edi,1), %r14d
	xorl	%r14d, %r15d
	rorl	$16, %r15d
	leal	0(%r8d,%r15d,1), %r8d
	xorl	%r8d, %edi
	rorl	$20, %edi
	leal	0(%r14d,%edi,1), %r14d
	xorl	%r14d, %r15d
	rorl	$24, %r15d
	movl	%r15d, 64(%rsp)
	movl	64(%rsp), %r15d
	leal	0(%r8d,%r15d,1), %r8d
	movl	%r8d, 56(%rsp)
	movl	56(%rsp), %r8d
	xorl	%r8d, %edi
	rorl	$25, %edi
	movq	%rbx, %r8
	leal	0(%r8d,%esi,1), %r8d
	xorl	%r8d, %ecx
	rorl	$16, %ecx
	leal	0(%r12d,%ecx,1), %ebx
	movq	%rbx, %r12
	xorl	%r12d, %esi
	rorl	$20, %esi
	leal	0(%r8d,%esi,1), %r8d
	movq	%r8, %rbx
	xorl	%ebx, %ecx
	rorl	$24, %ecx
	movq	%r12, %r8
	leal	0(%r8d,%ecx,1), %r8d
	xorl	%r8d, %esi
	rorl	$25, %esi
	leal	0(%edx,%eax,1), %r12d
	movq	%rbp, %rdx
	movq	%r12, %rbp
	xorl	%ebp, %edx
	rorl	$16, %edx
	leal	0(%r9d,%edx,1), %r9d
	xorl	%r9d, %eax
	rorl	$20, %eax
	leal	0(%ebp,%eax,1), %ebp
	movq	%rbp, %r12
	xorl	%r12d, %edx
	rorl	$24, %edx
	movq	%rdx, %rbp
	leal	0(%r9d,%ebp,1), %edx
	xorl	%edx, %eax
	rorl	$25, %eax
	movq	%r11, %r9
	movl	60(%rsp), %r11d
	leal	0(%r9d,%r11d,1), %r9d
	movq	%r13, %r15
	movq	%r9, %r13
	xorl	%r13d, %r15d
	rorl	$16, %r15d
	movq	%r10, %r9
	movq	%r15, %r10
	leal	0(%r9d,%r10d,1), %r9d
	xorl	%r9d, %r11d
	rorl	$20, %r11d
	movq	%r13, %r15
	movq	%r11, %r13
	leal	0(%r15d,%r13d,1), %r11d
	xorl	%r11d, %r10d
	rorl	$24, %r10d
	leal	0(%r9d,%r10d,1), %r9d
	xorl	%r9d, %r13d
	rorl	$25, %r13d
	leal	0(%r14d,%r13d,1), %r15d
	movq	%rcx, %r14
	movq	%r15, %rcx
	xorl	%ecx, %r14d
	rorl	$16, %r14d
	leal	0(%edx,%r14d,1), %r15d
	movq	%r13, %rdx
	movq	%r15, %r13
	xorl	%r13d, %edx
	rorl	$20, %edx
	leal	0(%ecx,%edx,1), %ecx
	xorl	%ecx, %r14d
	rorl	$24, %r14d
	leal	0(%r13d,%r14d,1), %r13d
	xorl	%r13d, %edx
	rorl	$25, %edx
	leal	0(%ebx,%edi,1), %r15d
	movq	%rbp, %rbx
	xorl	%r15d, %ebx
	rorl	$16, %ebx
	leal	0(%r9d,%ebx,1), %ebp
	movq	%rdi, %r9
	xorl	%ebp, %r9d
	rorl	$20, %r9d
	movq	%r15, %rdi
	leal	0(%edi,%r9d,1), %edi
	xorl	%edi, %ebx
	rorl	$24, %ebx
	leal	0(%ebp,%ebx,1), %ebp
	movl	%ebp, 60(%rsp)
	movl	60(%rsp), %ebp
	xorl	%ebp, %r9d
	rorl	$25, %r9d
	leal	0(%r12d,%esi,1), %r12d
	movq	%r10, %rbp
	movq	%r12, %r10
	xorl	%r10d, %ebp
	rorl	$16, %ebp
	movl	56(%rsp), %r12d
	leal	0(%r12d,%ebp,1), %r12d
	movq	%rsi, %r15
	xorl	%r12d, %r15d
	rorl	$20, %r15d
	movq	%r10, %rsi
	movq	%r15, %r10
	leal	0(%esi,%r10d,1), %esi
	xorl	%esi, %ebp
	rorl	$24, %ebp
	leal	0(%r12d,%ebp,1), %r12d
	xorl	%r12d, %r10d
	rorl	$25, %r10d
	leal	0(%r11d,%eax,1), %r11d
	movl	64(%rsp), %r15d
	xorl	%r11d, %r15d
	rorl	$16, %r15d
	leal	0(%r8d,%r15d,1), %r8d
	xorl	%r8d, %eax
	rorl	$20, %eax
	leal	0(%r11d,%eax,1), %r11d
	xorl	%r11d, %r15d
	rorl	$24, %r15d
	leal	0(%r8d,%r15d,1), %r8d
	xorl	%r8d, %eax
	rorl	$25, %eax
	leal	0(%ecx,%r9d,1), %ecx
	xorl	%ecx, %r15d
	rorl	$16, %r15d
	leal	0(%r12d,%r15d,1), %r12d
	xorl	%r12d, %r9d
	rorl	$20, %r9d
	leal	0(%ecx,%r9d,1), %ecx
	xorl	%ecx, %r15d
	rorl	$24, %r15d
	movl	%r15d, 56(%rsp)
	movl	56(%rsp), %r15d
	leal	0(%r12d,%r15d,1), %r12d
	xorl	%r12d, %r9d
	rorl	$25, %r9d
	leal	0(%edi,%r10d,1), %r15d
	movq	%r14, %rdi
	xorl	%r15d, %edi
	rorl	$16, %edi
	leal	0(%r8d,%edi,1), %r8d
	movq	%r8, %r14
	xorl	%r14d, %r10d
	rorl	$20, %r10d
	movq	%r15, %r8
	leal	0(%r8d,%r10d,1), %r8d
	xorl	%r8d, %edi
	rorl	$24, %edi
	movq	%r14, %r15
	movq	%rdi, %r14
	leal	0(%r15d,%r14d,1), %edi
	xorl	%edi, %r10d
	rorl	$25, %r10d
	leal	0(%esi,%eax,1), %esi
	xorl	%esi, %ebx
	rorl	$16, %ebx
	leal	0(%r13d,%ebx,1), %r13d
	movq	%rax, %r15
	xorl	%r13d, %r15d
	rorl	$20, %r15d
	movq	%rsi, %rax
	movq	%r15, %rsi
	leal	0(%eax,%esi,1), %eax
	xorl	%eax, %ebx
	rorl	$24, %ebx
	leal	0(%r13d,%ebx,1), %r13d
	xorl	%r13d, %esi
	rorl	$25, %esi
	leal	0(%r11d,%edx,1), %r11d
	xorl	%r11d, %ebp
	rorl	$16, %ebp
	movl	60(%rsp), %r15d
	leal	0(%r15d,%ebp,1), %r15d
	xorl	%r15d, %edx
	rorl	$20, %edx
	leal	0(%r11d,%edx,1), %r11d
	xorl	%r11d, %ebp
	rorl	$24, %ebp
	leal	0(%r15d,%ebp,1), %r15d
	xorl	%r15d, %edx
	rorl	$25, %edx
	leal	0(%ecx,%edx,1), %ecx
	xorl	%ecx, %r14d
	rorl	$16, %r14d
	leal	0(%r13d,%r14d,1), %r13d
	xorl	%r13d, %edx
	rorl	$20, %edx
	leal	0(%ecx,%edx,1), %ecx
	xorl	%ecx, %r14d
	rorl	$24, %r14d
	leal	0(%r13d,%r14d,1), %r13d
	xorl	%r13d, %edx
	rorl	$25, %edx
	movl	%edx, 60(%rsp)
	movq	%r9, %rdx
	leal	0(%r8d,%edx,1), %r8d
	xorl	%r8d, %ebx
	rorl	$16, %ebx
	leal	0(%r15d,%ebx,1), %r9d
	xorl	%r9d, %edx
	rorl	$20, %edx
	leal	0(%r8d,%edx,1), %r15d
	movq	%rbx, %r8
	movq	%r15, %rbx
	xorl	%ebx, %r8d
	rorl	$24, %r8d
	leal	0(%r9d,%r8d,1), %r9d
	xorl	%r9d, %edx
	rorl	$25, %edx
	leal	0(%eax,%r10d,1), %r15d
	movq	%rbp, %rax
	movq	%r15, %rbp
	xorl	%ebp, %eax
	rorl	$16, %eax
	leal	0(%r12d,%eax,1), %r12d
	movq	%r12, %r15
	xorl	%r15d, %r10d
	rorl	$20, %r10d
	leal	0(%ebp,%r10d,1), %r12d
	movq	%rax, %rbp
	xorl	%r12d, %ebp
	rorl	$24, %ebp
	movq	%r15, %rax
	leal	0(%eax,%ebp,1), %eax
	xorl	%eax, %r10d
	rorl	$25, %r10d
	leal	0(%r11d,%esi,1), %r11d
	movl	56(%rsp), %r15d
	xorl	%r11d, %r15d
	rorl	$16, %r15d
	leal	0(%edi,%r15d,1), %edi
	xorl	%edi, %esi
	rorl	$20, %esi
	leal	0(%r11d,%esi,1), %r11d
	xorl	%r11d, %r15d
	rorl	$24, %r15d
	leal	0(%edi,%r15d,1), %edi
	xorl	%edi, %esi
	rorl	$25, %esi
	leal	0(%ecx,%edx,1), %ecx
	xorl	%ecx, %r15d
	rorl	$16, %r15d
	leal	0(%eax,%r15d,1), %eax
	xorl	%eax, %edx
	rorl	$20, %edx
	leal	0(%ecx,%edx,1), %ecx
	xorl	%ecx, %r15d
	rorl	$24, %r15d
	movl	%r15d, 56(%rsp)
	movl	56(%rsp), %r15d
	leal	0(%eax,%r15d,1), %eax
	xorl	%eax, %edx
	rorl	$25, %edx
	leal	0(%ebx,%r10d,1), %ebx
	xorl	%ebx, %r14d
	rorl	$16, %r14d
	leal	0(%edi,%r14d,1), %edi
	xorl	%edi, %r10d
	rorl	$20, %r10d
	leal	0(%ebx,%r10d,1), %r15d
	movq	%r14, %rbx
	movq	%r15, %r14
	xorl	%r14d, %ebx
	rorl	$24, %ebx
	leal	0(%edi,%ebx,1), %edi
	xorl	%edi, %r10d
	rorl	$25, %r10d
	leal	0(%r12d,%esi,1), %r15d
	movq	%r8, %r12
	movq	%r15, %r8
	xorl	%r8d, %r12d
	rorl	$16, %r12d
	leal	0(%r13d,%r12d,1), %r13d
	movq	%r13, %r15
	xorl	%r15d, %esi
	rorl	$20, %esi
	leal	0(%r8d,%esi,1), %r8d
	movq	%r8, %r13
	xorl	%r13d, %r12d
	rorl	$24, %r12d
	movq	%r15, %r8
	leal	0(%r8d,%r12d,1), %r8d
	xorl	%r8d, %esi
	rorl	$25, %esi
	movq	%r11, %r15
	movl	60(%rsp), %r11d
	leal	0(%r15d,%r11d,1), %r15d
	xorl	%r15d, %ebp
	rorl	$16, %ebp
	leal	0(%r9d,%ebp,1), %r9d
	xorl	%r9d, %r11d
	rorl	$20, %r11d
	leal	0(%r15d,%r11d,1), %r15d
	xorl	%r15d, %ebp
	rorl	$24, %ebp
	leal	0(%r9d,%ebp,1), %r9d
	xorl	%r9d, %r11d
	rorl	$25, %r11d
	leal	0(%ecx,%r11d,1), %ecx
	xorl	%ecx, %ebx
	rorl	$16, %ebx
	leal	0(%r8d,%ebx,1), %r8d
	xorl	%r8d, %r11d
	rorl	$20, %r11d
	leal	0(%ecx,%r11d,1), %ecx
	xorl	%ecx, %ebx
	rorl	$24, %ebx
	leal	0(%r8d,%ebx,1), %r8d
	movl	%r8d, 60(%rsp)
	movl	60(%rsp), %r8d
	xorl	%r8d, %r11d
	rorl	$25, %r11d
	leal	0(%r14d,%edx,1), %r8d
	xorl	%r8d, %r12d
	rorl	$16, %r12d
	leal	0(%r9d,%r12d,1), %r9d
	xorl	%r9d, %edx
	rorl	$20, %edx
	leal	0(%r8d,%edx,1), %r8d
	xorl	%r8d, %r12d
	rorl	$24, %r12d
	movq	%r9, %r14
	movq	%r12, %r9
	leal	0(%r14d,%r9d,1), %r12d
	xorl	%r12d, %edx
	rorl	$25, %edx
	leal	0(%r13d,%r10d,1), %r13d
	movq	%r13, %r14
	xorl	%r14d, %ebp
	rorl	$16, %ebp
	leal	0(%eax,%ebp,1), %eax
	movq	%rax, %r13
	xorl	%r13d, %r10d
	rorl	$20, %r10d
	movq	%r14, %rax
	leal	0(%eax,%r10d,1), %eax
	xorl	%eax, %ebp
	rorl	$24, %ebp
	movq	%r13, %r14
	movq	%rbp, %r13
	leal	0(%r14d,%r13d,1), %ebp
	xorl	%ebp, %r10d
	rorl	$25, %r10d
	movq	%r15, %r14
	leal	0(%r14d,%esi,1), %r14d
	movl	56(%rsp), %r15d
	xorl	%r14d, %r15d
	rorl	$16, %r15d
	leal	0(%edi,%r15d,1), %edi
	xorl	%edi, %esi
	rorl	$20, %esi
	leal	0(%r14d,%esi,1), %r14d
	xorl	%r14d, %r15d
	rorl	$24, %r15d
	leal	0(%edi,%r15d,1), %edi
	xorl	%edi, %esi
	rorl	$25, %esi
	leal	0(%ecx,%edx,1), %ecx
	xorl	%ecx, %r15d
	rorl	$16, %r15d
	leal	0(%ebp,%r15d,1), %ebp
	xorl	%ebp, %edx
	rorl	$20, %edx
	leal	0(%ecx,%edx,1), %ecx
	xorl	%ecx, %r15d
	rorl	$24, %r15d
	movl	%r15d, 64(%rsp)
	movl	64(%rsp), %r15d
	leal	0(%ebp,%r15d,1), %ebp
	xorl	%ebp, %edx
	rorl	$25, %edx
	leal	0(%r8d,%r10d,1), %r8d
	movq	%r8, %r15
	xorl	%r15d, %ebx
	rorl	$16, %ebx
	movq	%rbx, %r8
	leal	0(%edi,%r8d,1), %edi
	movq	%rdi, %rbx
	xorl	%ebx, %r10d
	rorl	$20, %r10d
	movq	%r15, %rdi
	leal	0(%edi,%r10d,1), %edi
	xorl	%edi, %r8d
	rorl	$24, %r8d
	leal	0(%ebx,%r8d,1), %ebx
	movl	%ebx, 56(%rsp)
	movl	56(%rsp), %ebx
	xorl	%ebx, %r10d
	rorl	$25, %r10d
	leal	0(%eax,%esi,1), %eax
	xorl	%eax, %r9d
	rorl	$16, %r9d
	movl	60(%rsp), %ebx
	leal	0(%ebx,%r9d,1), %ebx
	xorl	%ebx, %esi
	rorl	$20, %esi
	leal	0(%eax,%esi,1), %eax
	xorl	%eax, %r9d
	rorl	$24, %r9d
	leal	0(%ebx,%r9d,1), %r15d
	xorl	%r15d, %esi
	rorl	$25, %esi
	leal	0(%r14d,%r11d,1), %ebx
	xorl	%ebx, %r13d
	rorl	$16, %r13d
	leal	0(%r12d,%r13d,1), %r12d
	xorl	%r12d, %r11d
	rorl	$20, %r11d
	leal	0(%ebx,%r11d,1), %ebx
	xorl	%ebx, %r13d
	rorl	$24, %r13d
	leal	0(%r12d,%r13d,1), %r12d
	xorl	%r12d, %r11d
	rorl	$25, %r11d
	leal	0(%ecx,%r11d,1), %ecx
	xorl	%ecx, %r8d
	rorl	$16, %r8d
	leal	0(%r15d,%r8d,1), %r14d
	movq	%r14, %r15
	xorl	%r15d, %r11d
	rorl	$20, %r11d
	movq	%r11, %r14
	leal	0(%ecx,%r14d,1), %r11d
	xorl	%r11d, %r8d
	rorl	$24, %r8d
	movq	%r8, %rcx
	leal	0(%r15d,%ecx,1), %r8d
	xorl	%r8d, %r14d
	rorl	$25, %r14d
	movl	%r14d, 60(%rsp)
	leal	0(%edi,%edx,1), %r14d
	movq	%r9, %rdi
	movq	%r14, %r9
	xorl	%r9d, %edi
	rorl	$16, %edi
	leal	0(%r12d,%edi,1), %r12d
	xorl	%r12d, %edx
	rorl	$20, %edx
	leal	0(%r9d,%edx,1), %r9d
	movq	%r9, %r14
	xorl	%r14d, %edi
	rorl	$24, %edi
	movq	%r12, %r9
	movq	%rdi, %r12
	leal	0(%r9d,%r12d,1), %r9d
	xorl	%r9d, %edx
	rorl	$25, %edx
	leal	0(%eax,%r10d,1), %edi
	movq	%r13, %rax
	xorl	%edi, %eax
	rorl	$16, %eax
	leal	0(%ebp,%eax,1), %ebp
	movq	%rbp, %r13
	xorl	%r13d, %r10d
	rorl	$20, %r10d
	leal	0(%edi,%r10d,1), %edi
	movq	%rdi, %rbp
	xorl	%ebp, %eax
	rorl	$24, %eax
	movq	%r13, %rdi
	movq	%rax, %r13
	leal	0(%edi,%r13d,1), %eax
	xorl	%eax, %r10d
	rorl	$25, %r10d
	movq	%rbx, %rdi
	leal	0(%edi,%esi,1), %r15d
	movl	64(%rsp), %ebx
	xorl	%r15d, %ebx
	rorl	$16, %ebx
	movl	56(%rsp), %edi
	leal	0(%edi,%ebx,1), %edi
	xorl	%edi, %esi
	rorl	$20, %esi
	leal	0(%r15d,%esi,1), %r15d
	xorl	%r15d, %ebx
	rorl	$24, %ebx
	leal	0(%edi,%ebx,1), %edi
	xorl	%edi, %esi
	rorl	$25, %esi
	leal	0(%r11d,%edx,1), %r11d
	xorl	%r11d, %ebx
	rorl	$16, %ebx
	leal	0(%eax,%ebx,1), %eax
	xorl	%eax, %edx
	rorl	$20, %edx
	leal	0(%r11d,%edx,1), %r11d
	xorl	%r11d, %ebx
	rorl	$24, %ebx
	movl	%ebx, 56(%rsp)
	movl	56(%rsp), %ebx
	leal	0(%eax,%ebx,1), %eax
	movq	%rax, %rbx
	movq	%rbx, %rax
	xorl	%eax, %edx
	rorl	$25, %edx
	movq	%r14, %rax
	leal	0(%eax,%r10d,1), %r14d
	movq	%rcx, %rax
	movq	%r14, %rcx
	xorl	%ecx, %eax
	rorl	$16, %eax
	leal	0(%edi,%eax,1), %edi
	xorl	%edi, %r10d
	rorl	$20, %r10d
	movq	%rcx, %r14
	movq	%r10, %rcx
	leal	0(%r14d,%ecx,1), %r10d
	movq	%r10, %r14
	xorl	%r14d, %eax
	rorl	$24, %eax
	movq	%rax, %r10
	leal	0(%edi,%r10d,1), %edi
	xorl	%edi, %ecx
	rorl	$25, %ecx
	movq	%rbp, %rax
	leal	0(%eax,%esi,1), %ebp
	movq	%r12, %rax
	xorl	%ebp, %eax
	rorl	$16, %eax
	leal	0(%r8d,%eax,1), %r8d
	movq	%rsi, %r12
	movq	%r8, %rsi
	xorl	%esi, %r12d
	rorl	$20, %r12d
	movq	%r12, %r8
	leal	0(%ebp,%r8d,1), %ebp
	movq	%rbp, %r12
	xorl	%r12d, %eax
	rorl	$24, %eax
	movq	%rax, %rbp
	leal	0(%esi,%ebp,1), %esi
	xorl	%esi, %r8d
	rorl	$25, %r8d
	movl	60(%rsp), %eax
	leal	0(%r15d,%eax,1), %r15d
	xorl	%r15d, %r13d
	rorl	$16, %r13d
	leal	0(%r9d,%r13d,1), %r9d
	xorl	%r9d, %eax
	rorl	$20, %eax
	leal	0(%r15d,%eax,1), %r15d
	xorl	%r15d, %r13d
	rorl	$24, %r13d
	leal	0(%r9d,%r13d,1), %r9d
	xorl	%r9d, %eax
	rorl	$25, %eax
	leal	0(%r11d,%eax,1), %r11d
	xorl	%r11d, %r10d
	rorl	$16, %r10d
	leal	0(%esi,%r10d,1), %esi
	xorl	%esi, %eax
	rorl	$20, %eax
	leal	0(%r11d,%eax,1), %r11d
	xorl	%r11d, %r10d
	rorl	$24, %r10d
	leal	0(%esi,%r10d,1), %esi
	movl	%esi, 60(%rsp)
	movl	60(%rsp), %esi
	xorl	%esi, %eax
	rorl	$25, %eax
	leal	0(%r14d,%edx,1), %esi
	xorl	%esi, %ebp
	rorl	$16, %ebp
	leal	0(%r9d,%ebp,1), %r9d
	movq	%rdx, %r14
	xorl	%r9d, %r14d
	rorl	$20, %r14d
	movq	%rsi, %rdx
	movq	%r14, %rsi
	leal	0(%edx,%esi,1), %edx
	xorl	%edx, %ebp
	rorl	$24, %ebp
	movq	%r9, %r14
	movq	%rbp, %r9
	leal	0(%r14d,%r9d,1), %ebp
	xorl	%ebp, %esi
	rorl	$25, %esi
	leal	0(%r12d,%ecx,1), %r14d
	movq	%r13, %r12
	xorl	%r14d, %r12d
	rorl	$16, %r12d
	leal	0(%ebx,%r12d,1), %r13d
	movq	%rcx, %rbx
	xorl	%r13d, %ebx
	rorl	$20, %ebx
	movq	%r14, %rcx
	leal	0(%ecx,%ebx,1), %ecx
	xorl	%ecx, %r12d
	rorl	$24, %r12d
	leal	0(%r13d,%r12d,1), %r13d
	xorl	%r13d, %ebx
	rorl	$25, %ebx
	movq	%r15, %r14
	leal	0(%r14d,%r8d,1), %r14d
	movl	56(%rsp), %r15d
	xorl	%r14d, %r15d
	rorl	$16, %r15d
	leal	0(%edi,%r15d,1), %edi
	xorl	%edi, %r8d
	rorl	$20, %r8d
	leal	0(%r14d,%r8d,1), %r14d
	xorl	%r14d, %r15d
	rorl	$24, %r15d
	leal	0(%edi,%r15d,1), %edi
	xorl	%edi, %r8d
	rorl	$25, %r8d
	leal	0(%r11d,%esi,1), %r11d
	xorl	%r11d, %r15d
	rorl	$16, %r15d
	leal	0(%r13d,%r15d,1), %r13d
	xorl	%r13d, %esi
	rorl	$20, %esi
	leal	0(%r11d,%esi,1), %r11d
	xorl	%r11d, %r15d
	rorl	$24, %r15d
	movl	%r15d, 64(%rsp)
	movl	64(%rsp), %r15d
	leal	0(%r13d,%r15d,1), %r13d
	xorl	%r13d, %esi
	rorl	$25, %esi
	leal	0(%edx,%ebx,1), %edx
	movq	%r10, %r15
	xorl	%edx, %r15d
	rorl	$16, %r15d
	movq	%rdi, %r10
	leal	0(%r10d,%r15d,1), %edi
	xorl	%edi, %ebx
	rorl	$20, %ebx
	movq	%rdx, %r10
	movq	%rbx, %rdx
	leal	0(%r10d,%edx,1), %r10d
	movq	%r15, %rbx
	xorl	%r10d, %ebx
	rorl	$24, %ebx
	movq	%rdi, %r15
	movq	%rbx, %rdi
	leal	0(%r15d,%edi,1), %ebx
	movl	%ebx, 56(%rsp)
	movl	56(%rsp), %ebx
	xorl	%ebx, %edx
	rorl	$25, %edx
	movq	%rcx, %rbx
	movq	%r8, %rcx
	leal	0(%ebx,%ecx,1), %r8d
	movq	%r8, %rbx
	xorl	%ebx, %r9d
	rorl	$16, %r9d
	movl	60(%rsp), %r8d
	leal	0(%r8d,%r9d,1), %r8d
	xorl	%r8d, %ecx
	rorl	$20, %ecx
	leal	0(%ebx,%ecx,1), %ebx
	xorl	%ebx, %r9d
	rorl	$24, %r9d
	leal	0(%r8d,%r9d,1), %r8d
	xorl	%r8d, %ecx
	rorl	$25, %ecx
	leal	0(%r14d,%eax,1), %r14d
	movq	%r14, %r15
	xorl	%r15d, %r12d
	rorl	$16, %r12d
	leal	0(%ebp,%r12d,1), %ebp
	movq	%rbp, %r14
	xorl	%r14d, %eax
	rorl	$20, %eax
	movq	%r15, %rbp
	leal	0(%ebp,%eax,1), %ebp
	xorl	%ebp, %r12d
	rorl	$24, %r12d
	leal	0(%r14d,%r12d,1), %r14d
	movq	%rax, %r15
	movq	%r14, %rax
	xorl	%eax, %r15d
	rorl	$25, %r15d
	movq	%r11, %r14
	movq	%r15, %r11
	leal	0(%r14d,%r11d,1), %r14d
	movq	%r14, %r15
	xorl	%r15d, %edi
	rorl	$16, %edi
	leal	0(%r8d,%edi,1), %r8d
	movq	%r11, %r14
	movq	%r8, %r11
	xorl	%r11d, %r14d
	rorl	$20, %r14d
	movq	%r15, %r8
	leal	0(%r8d,%r14d,1), %r8d
	movq	%rdi, %r15
	xorl	%r8d, %r15d
	rorl	$24, %r15d
	movq	%r11, %rdi
	movq	%r15, %r11
	leal	0(%edi,%r11d,1), %edi
	xorl	%edi, %r14d
	rorl	$25, %r14d
	movl	%r14d, 60(%rsp)
	leal	0(%r10d,%esi,1), %r10d
	xorl	%r10d, %r9d
	rorl	$16, %r9d
	leal	0(%eax,%r9d,1), %eax
	movq	%rax, %r14
	xorl	%r14d, %esi
	rorl	$20, %esi
	leal	0(%r10d,%esi,1), %eax
	movq	%r9, %r10
	xorl	%eax, %r10d
	rorl	$24, %r10d
	movq	%r14, %r9
	leal	0(%r9d,%r10d,1), %r9d
	xorl	%r9d, %esi
	rorl	$25, %esi
	leal	0(%ebx,%edx,1), %ebx
	xorl	%ebx, %r12d
	rorl	$16, %r12d
	leal	0(%r13d,%r12d,1), %r13d
	movq	%r13, %r14
	xorl	%r14d, %edx
	rorl	$20, %edx
	leal	0(%ebx,%edx,1), %ebx
	movq	%rbx, %r13
	xorl	%r13d, %r12d
	rorl	$24, %r12d
	movq	%r14, %rbx
	movq	%r12, %r14
	leal	0(%ebx,%r14d,1), %r12d
	xorl	%r12d, %edx
	rorl	$25, %edx
	movq	%rbp, %rbx
	leal	0(%ebx,%ecx,1), %r15d
	movl	64(%rsp), %ebp
	xorl	%r15d, %ebp
	rorl	$16, %ebp
	movl	56(%rsp), %ebx
	leal	0(%ebx,%ebp,1), %ebx
	xorl	%ebx, %ecx
	rorl	$20, %ecx
	leal	0(%r15d,%ecx,1), %r15d
	xorl	%r15d, %ebp
	rorl	$24, %ebp
	leal	0(%ebx,%ebp,1), %ebx
	xorl	%ebx, %ecx
	rorl	$25, %ecx
	leal	0(%r8d,%esi,1), %r8d
	xorl	%r8d, %ebp
	rorl	$16, %ebp
	leal	0(%r12d,%ebp,1), %r12d
	xorl	%r12d, %esi
	rorl	$20, %esi
	leal	0(%r8d,%esi,1), %r8d
	xorl	%r8d, %ebp
	rorl	$24, %ebp
	movl	%ebp, 64(%rsp)
	movl	64(%rsp), %ebp
	leal	0(%r12d,%ebp,1), %ebp
	movl	%ebp, 56(%rsp)
	movl	56(%rsp), %ebp
	xorl	%ebp, %esi
	rorl	$25, %esi
	leal	0(%eax,%edx,1), %ebp
	movq	%r11, %rax
	movq	%rbp, %r11
	xorl	%r11d, %eax
	rorl	$16, %eax
	leal	0(%ebx,%eax,1), %ebx
	xorl	%ebx, %edx
	rorl	$20, %edx
	leal	0(%r11d,%edx,1), %ebp
	movq	%rax, %r11
	xorl	%ebp, %r11d
	rorl	$24, %r11d
	movq	%rbx, %rax
	leal	0(%eax,%r11d,1), %ebx
	xorl	%ebx, %edx
	rorl	$25, %edx
	movq	%r13, %r12
	movq	%rcx, %rax
	leal	0(%r12d,%eax,1), %ecx
	xorl	%ecx, %r10d
	rorl	$16, %r10d
	leal	0(%edi,%r10d,1), %edi
	xorl	%edi, %eax
	rorl	$20, %eax
	leal	0(%ecx,%eax,1), %ecx
	movq	%r10, %r12
	movq	%rcx, %r10
	xorl	%r10d, %r12d
	rorl	$24, %r12d
	movq	%rdi, %rcx
	movq	%r12, %rdi
	leal	0(%ecx,%edi,1), %r12d
	movq	%rax, %rcx
	xorl	%r12d, %ecx
	rorl	$25, %ecx
	movq	%r15, %r13
	movl	60(%rsp), %eax
	leal	0(%r13d,%eax,1), %r13d
	xorl	%r13d, %r14d
	rorl	$16, %r14d
	leal	0(%r9d,%r14d,1), %r15d
	movq	%rax, %r9
	movq	%r15, %rax
	xorl	%eax, %r9d
	rorl	$20, %r9d
	leal	0(%r13d,%r9d,1), %r13d
	xorl	%r13d, %r14d
	rorl	$24, %r14d
	leal	0(%eax,%r14d,1), %eax
	xorl	%eax, %r9d
	rorl	$25, %r9d
	leal	0(%r8d,%r9d,1), %r8d
	movq	%r11, %r15
	movq	%r8, %r11
	xorl	%r11d, %r15d
	rorl	$16, %r15d
	movq	%r15, %r8
	leal	0(%r12d,%r8d,1), %r12d
	xorl	%r12d, %r9d
	rorl	$20, %r9d
	leal	0(%r11d,%r9d,1), %r11d
	xorl	%r11d, %r8d
	rorl	$24, %r8d
	movq	%r12, %r15
	movq	%r8, %r12
	leal	0(%r15d,%r12d,1), %r8d
	xorl	%r8d, %r9d
	rorl	$25, %r9d
	movq	%rbp, %r15
	movq	%rsi, %rbp
	leal	0(%r15d,%ebp,1), %esi
	xorl	%esi, %edi
	rorl	$16, %edi
	movq	%rax, %r15
	movq	%rdi, %rax
	leal	0(%r15d,%eax,1), %edi
	xorl	%edi, %ebp
	rorl	$20, %ebp
	leal	0(%esi,%ebp,1), %esi
	xorl	%esi, %eax
	rorl	$24, %eax
	leal	0(%edi,%eax,1), %edi
	movl	%edi, 60(%rsp)
	movl	60(%rsp), %edi
	xorl	%edi, %ebp
	movq	%rbp, %rdi
	rorl	$25, %edi
	leal	0(%r10d,%edx,1), %r10d
	movq	%r10, %rbp
	xorl	%ebp, %r14d
	rorl	$16, %r14d
	movl	56(%rsp), %r15d
	movq	%r14, %r10
	leal	0(%r15d,%r10d,1), %r14d
	xorl	%r14d, %edx
	rorl	$20, %edx
	leal	0(%ebp,%edx,1), %ebp
	xorl	%ebp, %r10d
	rorl	$24, %r10d
	leal	0(%r14d,%r10d,1), %r14d
	xorl	%r14d, %edx
	rorl	$25, %edx
	leal	0(%r13d,%ecx,1), %r13d
	movl	64(%rsp), %r15d
	xorl	%r13d, %r15d
	rorl	$16, %r15d
	leal	0(%ebx,%r15d,1), %ebx
	xorl	%ebx, %ecx
	rorl	$20, %ecx
	leal	0(%r13d,%ecx,1), %r13d
	xorl	%r13d, %r15d
	rorl	$24, %r15d
	leal	0(%ebx,%r15d,1), %ebx
	xorl	%ebx, %ecx
	rorl	$25, %ecx
	leal	0(%r11d,%edi,1), %r11d
	xorl	%r11d, %r15d
	rorl	$16, %r15d
	leal	0(%r14d,%r15d,1), %r14d
	xorl	%r14d, %edi
	rorl	$20, %edi
	leal	0(%r11d,%edi,1), %r11d
	xorl	%r11d, %r15d
	rorl	$24, %r15d
	movl	%r15d, 64(%rsp)
	movl	64(%rsp), %r15d
	leal	0(%r14d,%r15d,1), %r14d
	movl	%r14d, 56(%rsp)
	movl	56(%rsp), %r14d
	xorl	%r14d, %edi
	rorl	$25, %edi
	leal	0(%esi,%edx,1), %esi
	movq	%r12, %r14
	movq	%rsi, %r12
	xorl	%r12d, %r14d
	rorl	$16, %r14d
	movq	%rbx, %rsi
	movq	%r14, %rbx
	leal	0(%esi,%ebx,1), %esi
	xorl	%esi, %edx
	rorl	$20, %edx
	leal	0(%r12d,%edx,1), %r12d
	xorl	%r12d, %ebx
	rorl	$24, %ebx
	movq	%rsi, %r14
	movq	%rbx, %rsi
	leal	0(%r14d,%esi,1), %ebx
	xorl	%ebx, %edx
	rorl	$25, %edx
	leal	0(%ebp,%ecx,1), %ebp
	movq	%rax, %r14
	movq	%rbp, %rax
	xorl	%eax, %r14d
	rorl	$16, %r14d
	movq	%r14, %rbp
	leal	0(%r8d,%ebp,1), %r8d
	xorl	%r8d, %ecx
	rorl	$20, %ecx
	leal	0(%eax,%ecx,1), %eax
	xorl	%eax, %ebp
	rorl	$24, %ebp
	leal	0(%r8d,%ebp,1), %r8d
	xorl	%r8d, %ecx
	rorl	$25, %ecx
	movq	%r9, %r14
	leal	0(%r13d,%r14d,1), %r9d
	xorl	%r9d, %r10d
	rorl	$16, %r10d
	movl	60(%rsp), %r13d
	leal	0(%r13d,%r10d,1), %r13d
	xorl	%r13d, %r14d
	rorl	$20, %r14d
	leal	0(%r9d,%r14d,1), %r9d
	xorl	%r9d, %r10d
	rorl	$24, %r10d
	leal	0(%r13d,%r10d,1), %r13d
	xorl	%r13d, %r14d
	rorl	$25, %r14d
	leal	0(%r11d,%r14d,1), %r15d
	movq	%rsi, %r11
	movq	%r15, %rsi
	xorl	%esi, %r11d
	rorl	$16, %r11d
	leal	0(%r8d,%r11d,1), %r8d
	xorl	%r8d, %r14d
	rorl	$20, %r14d
	movq	%rsi, %r15
	movq	%r14, %rsi
	leal	0(%r15d,%esi,1), %r14d
	xorl	%r14d, %r11d
	rorl	$24, %r11d
	leal	0(%r8d,%r11d,1), %r8d
	xorl	%r8d, %esi
	rorl	$25, %esi
	leal	0(%r12d,%edi,1), %r12d
	xorl	%r12d, %ebp
	rorl	$16, %ebp
	leal	0(%r13d,%ebp,1), %r13d
	movq	%r13, %r15
	xorl	%r15d, %edi
	rorl	$20, %edi
	leal	0(%r12d,%edi,1), %r12d
	movq	%r12, %r13
	xorl	%r13d, %ebp
	rorl	$24, %ebp
	movq	%rbp, %r12
	leal	0(%r15d,%r12d,1), %ebp
	xorl	%ebp, %edi
	rorl	$25, %edi
	leal	0(%eax,%edx,1), %eax
	movq	%rax, %r15
	xorl	%r15d, %r10d
	rorl	$16, %r10d
	movl	56(%rsp), %eax
	leal	0(%eax,%r10d,1), %eax
	xorl	%eax, %edx
	rorl	$20, %edx
	leal	0(%r15d,%edx,1), %r15d
	movl	%r15d, 56(%rsp)
	xorl	%r15d, %r10d
	rorl	$24, %r10d
	leal	0(%eax,%r10d,1), %eax
	xorl	%eax, %edx
	rorl	$25, %edx
	leal	0(%r9d,%ecx,1), %r9d
	movl	64(%rsp), %r15d
	xorl	%r9d, %r15d
	rorl	$16, %r15d
	leal	0(%ebx,%r15d,1), %ebx
	xorl	%ebx, %ecx
	rorl	$20, %ecx
	leal	0(%r9d,%ecx,1), %r9d
	xorl	%r9d, %r15d
	rorl	$24, %r15d
	leal	0(%ebx,%r15d,1), %ebx
	xorl	%ebx, %ecx
	rorl	$25, %ecx
	leal	0(%r14d,%edi,1), %r14d
	xorl	%r14d, %r15d
	rorl	$16, %r15d
	leal	0(%eax,%r15d,1), %eax
	xorl	%eax, %edi
	rorl	$20, %edi
	leal	0(%r14d,%edi,1), %r14d
	movl	%r14d, 328(%rsp)
	xorl	%r14d, %r15d
	rorl	$24, %r15d
	movl	%r15d, 388(%rsp)
	leal	0(%eax,%r15d,1), %eax
	movl	%eax, 368(%rsp)
	xorl	%eax, %edi
	rorl	$25, %edi
	movl	%edi, 348(%rsp)
	leal	0(%r13d,%edx,1), %edi
	xorl	%edi, %r11d
	rorl	$16, %r11d
	leal	0(%ebx,%r11d,1), %eax
	xorl	%eax, %edx
	rorl	$20, %edx
	leal	0(%edi,%edx,1), %edi
	movl	%edi, 332(%rsp)
	xorl	%edi, %r11d
	rorl	$24, %r11d
	movl	%r11d, 376(%rsp)
	leal	0(%eax,%r11d,1), %eax
	movl	%eax, 372(%rsp)
	xorl	%eax, %edx
	rorl	$25, %edx
	movl	%edx, 352(%rsp)
	movl	56(%rsp), %r11d
	leal	0(%r11d,%ecx,1), %edx
	xorl	%edx, %r12d
	rorl	$16, %r12d
	leal	0(%r8d,%r12d,1), %eax
	xorl	%eax, %ecx
	rorl	$20, %ecx
	leal	0(%edx,%ecx,1), %r8d
	movl	%r8d, 336(%rsp)
	xorl	%r8d, %r12d
	rorl	$24, %r12d
	movl	%r12d, 380(%rsp)
	leal	0(%eax,%r12d,1), %r11d
	movl	%r11d, 360(%rsp)
	xorl	%r11d, %ecx
	rorl	$25, %ecx
	movl	%ecx, 356(%rsp)
	leal	0(%r9d,%esi,1), %r8d
	xorl	%r8d, %r10d
	rorl	$16, %r10d
	leal	0(%ebp,%r10d,1), %ecx
	xorl	%ecx, %esi
	rorl	$20, %esi
	leal	0(%r8d,%esi,1), %r11d
	movl	%r11d, 340(%rsp)
	xorl	%r11d, %r10d
	rorl	$24, %r10d
	movl	%r10d, 384(%rsp)
	leal	0(%ecx,%r10d,1), %r10d
	movl	%r10d, 364(%rsp)
	xorl	%r10d, %esi
	rorl	$25, %esi
	movl	%esi, 344(%rsp)
	xorl	%r11d, %r11d
.L120:
	leaq	328(%rsp), %r9
	movl	%r11d, %r8d
	movq	80(%rsp), %rdi
	movl	0(%rdi,%r8,4), %ecx
	movq	72(%rsp), %rax
	movl	0(%rax,%r8,4), %edx
	leal	0(%ecx,%edx,1), %esi
	movl	%esi, 0(%r9,%r8,4)
	leal	1(%r11d), %r11d
	cmpl	$16, %r11d
	jb	.L120
	movl	376(%rsp), %edx
	movl	68(%rsp), %esi
	leal	0(%edx,%esi,1), %edx
	movl	%edx, 376(%rsp)
	xorl	%ebx, %ebx
.L121:
	leaq	264(%rsp), %rdi
	leal	0(,%ebx,4), %r11d
	movl	%r11d, %r8d
	leaq	0(%rdi,%r8,1), %rsi
	leaq	392(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	392(%rsp), %esi
	leaq	328(%rsp), %rdx
	movl	%ebx, %eax
	movl	0(%rdx,%rax,4), %r8d
	xorl	%r8d, %esi
	leaq	264(%rsp), %rdi
	leal	0(,%ebx,4), %r9d
	movl	%r9d, %r10d
	leaq	0(%rdi,%r10,1), %rdi
	movl	%esi, 392(%rsp)
	leaq	392(%rsp), %rsi
	movq	$4, %rdx
	call	memcpy
	leal	1(%ebx), %ebx
	cmpl	$16, %ebx
	jb	.L121
	leaq	264(%rsp), %rsi
	movl	116(%rsp), %eax
	movl	%eax, %edx
	movq	88(%rsp), %rdi
	call	memcpy
.L119:
	movq	8(%rsp), %rbx
	movq	16(%rsp), %rbp
	movq	24(%rsp), %r12
	movq	32(%rsp), %r13
	movq	40(%rsp), %r14
	movq	48(%rsp), %r15
	addq	$472, %rsp
	ret
	.cfi_endproc
	.type	Hacl_Chacha20_Vec32_chacha20_decrypt_32, @function
	.size	Hacl_Chacha20_Vec32_chacha20_decrypt_32, . - Hacl_Chacha20_Vec32_chacha20_decrypt_32
