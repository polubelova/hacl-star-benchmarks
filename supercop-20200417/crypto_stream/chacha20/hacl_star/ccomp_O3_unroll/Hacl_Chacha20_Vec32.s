# File generated by CompCert 3.7
# Command line: -O3 -S -I . -I ./kremlin/include/ -I ./kremlin/kremlib/dist/minimal/ -D_BSD_SOURCE -D_DEFAULT_SOURCE -DKRML_VERIFIED_UINT128 Hacl_Chacha20_Vec32.c
	.section	.rodata
	.align	4
	.globl	Hacl_Impl_Chacha20_Vec_chacha20_constants
Hacl_Impl_Chacha20_Vec_chacha20_constants:
	.long	1634760805
	.long	857760878
	.long	2036477234
	.long	1797285236
	.type	Hacl_Impl_Chacha20_Vec_chacha20_constants, @object
	.size	Hacl_Impl_Chacha20_Vec_chacha20_constants, . - Hacl_Impl_Chacha20_Vec_chacha20_constants
	.text
	.align	16
	.globl Hacl_Chacha20_Vec32_chacha20_encrypt_32
Hacl_Chacha20_Vec32_chacha20_encrypt_32:
	.cfi_startproc
	subq	$456, %rsp
	.cfi_adjust_cfa_offset	456
	leaq	464(%rsp), %rax
	movq	%rax, 0(%rsp)
	movq	%rbx, 8(%rsp)
	movq	%rbp, 16(%rsp)
	movq	%r12, 24(%rsp)
	movq	%r13, 32(%rsp)
	movq	%r14, 40(%rsp)
	movq	%r15, 48(%rsp)
	movq	%r9, %rbp
	movq	%r8, %r12
	movq	%rdx, 96(%rsp)
	movq	%rsi, 104(%rsp)
	movq	%rdi, %rbx
	xorl	%edx, %edx
	movl	%edx, 128(%rsp)
	xorl	%r9d, %r9d
	movl	%r9d, 132(%rsp)
	xorl	%r11d, %r11d
	movl	%r11d, 136(%rsp)
	xorl	%r9d, %r9d
	movl	%r9d, 140(%rsp)
	xorl	%r11d, %r11d
	movl	%r11d, 144(%rsp)
	xorl	%r11d, %r11d
	movl	%r11d, 148(%rsp)
	xorl	%eax, %eax
	movl	%eax, 152(%rsp)
	xorl	%r9d, %r9d
	movl	%r9d, 156(%rsp)
	xorl	%esi, %esi
	movl	%esi, 160(%rsp)
	xorl	%edx, %edx
	movl	%edx, 164(%rsp)
	xorl	%eax, %eax
	movl	%eax, 168(%rsp)
	xorl	%r8d, %r8d
	movl	%r8d, 172(%rsp)
	xorl	%edx, %edx
	movl	%edx, 176(%rsp)
	xorl	%edx, %edx
	movl	%edx, 180(%rsp)
	xorl	%edx, %edx
	movl	%edx, 184(%rsp)
	xorl	%r9d, %r9d
	movl	%r9d, 188(%rsp)
	movq	%rcx, %r13
	xorl	%r8d, %r8d
	movl	%r8d, 400(%rsp)
	xorl	%r10d, %r10d
	movl	%r10d, 404(%rsp)
	xorl	%r10d, %r10d
	movl	%r10d, 408(%rsp)
	xorl	%edi, %edi
	movl	%edi, 412(%rsp)
	xorl	%edi, %edi
	movl	%edi, 416(%rsp)
	xorl	%r9d, %r9d
	movl	%r9d, 420(%rsp)
	xorl	%edi, %edi
	movl	%edi, 424(%rsp)
	xorl	%eax, %eax
	movl	%eax, 428(%rsp)
	xorl	%ecx, %ecx
	movl	%ecx, 432(%rsp)
	xorl	%r8d, %r8d
	movl	%r8d, 436(%rsp)
	xorl	%r8d, %r8d
	movl	%r8d, 440(%rsp)
	xorl	%esi, %esi
	movl	%esi, 444(%rsp)
	movl	$1634760805, %edx
	movl	%edx, 384(%rsp)
	movl	$857760878, %eax
	movl	%eax, 388(%rsp)
	movl	$2036477234, %edi
	movl	%edi, 392(%rsp)
	movl	$1797285236, %edi
	movl	%edi, 396(%rsp)
	movq	%r13, %rsi
	leaq	448(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	448(%rsp), %eax
	movl	%eax, 400(%rsp)
	leaq	4(%r13), %rsi
	leaq	448(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	448(%rsp), %edx
	movl	%edx, 404(%rsp)
	leaq	8(%r13), %rsi
	leaq	448(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	448(%rsp), %esi
	movl	%esi, 408(%rsp)
	leaq	12(%r13), %rsi
	leaq	448(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	448(%rsp), %r11d
	movl	%r11d, 412(%rsp)
	leaq	16(%r13), %rsi
	leaq	448(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	448(%rsp), %esi
	movl	%esi, 416(%rsp)
	leaq	20(%r13), %rsi
	leaq	448(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	448(%rsp), %r8d
	movl	%r8d, 420(%rsp)
	leaq	24(%r13), %rsi
	leaq	448(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	448(%rsp), %r10d
	movl	%r10d, 424(%rsp)
	leaq	28(%r13), %rsi
	leaq	448(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	448(%rsp), %ecx
	movl	%ecx, 428(%rsp)
	movl	%ebp, 432(%rsp)
	movq	%r12, %rsi
	leaq	448(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	448(%rsp), %r9d
	movl	%r9d, 436(%rsp)
	leaq	4(%r12), %rsi
	leaq	448(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	448(%rsp), %r8d
	movl	%r8d, 440(%rsp)
	leaq	8(%r12), %rsi
	leaq	448(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	448(%rsp), %eax
	movl	%eax, 444(%rsp)
	movl	384(%rsp), %ecx
	movl	%ecx, 128(%rsp)
	movl	388(%rsp), %ecx
	movl	%ecx, 132(%rsp)
	movl	392(%rsp), %r11d
	movl	%r11d, 136(%rsp)
	movl	396(%rsp), %r11d
	movl	%r11d, 140(%rsp)
	movl	400(%rsp), %r10d
	movl	%r10d, 144(%rsp)
	movl	404(%rsp), %ecx
	movl	%ecx, 148(%rsp)
	movl	408(%rsp), %r11d
	movl	%r11d, 152(%rsp)
	movl	412(%rsp), %r9d
	movl	%r9d, 156(%rsp)
	movl	416(%rsp), %r8d
	movl	%r8d, 160(%rsp)
	movl	420(%rsp), %ecx
	movl	%ecx, 164(%rsp)
	movl	424(%rsp), %r10d
	movl	%r10d, 168(%rsp)
	movl	428(%rsp), %r11d
	movl	%r11d, 172(%rsp)
	movl	432(%rsp), %r10d
	movl	436(%rsp), %esi
	movl	%esi, 180(%rsp)
	movl	440(%rsp), %r11d
	movl	%r11d, 184(%rsp)
	movl	%eax, 188(%rsp)
	leal	0(%r10d), %ecx
	movl	%ecx, 176(%rsp)
	movq	%rbx, %rcx
	andl	$63, %ecx
	movl	%ecx, 120(%rsp)
	shrl	$6, %ebx
	movl	%ebx, 88(%rsp)
	movl	%ecx, 116(%rsp)
	xorl	%edi, %edi
	movl	%edi, 92(%rsp)
.L100:
	movl	92(%rsp), %r11d
	movl	88(%rsp), %ecx
	cmpl	%ecx, %r11d
	jae	.L101
	movl	92(%rsp), %r9d
	sall	$6, %r9d
	movl	%r9d, %r9d
	movq	104(%rsp), %r8
	leaq	0(%r8,%r9,1), %rdi
	movq	%rdi, 80(%rsp)
	movq	96(%rsp), %rcx
	leaq	0(%rcx,%r9,1), %r10
	movq	%r10, 72(%rsp)
	xorl	%r8d, %r8d
	movl	%r8d, 192(%rsp)
	xorl	%eax, %eax
	movl	%eax, 196(%rsp)
	xorl	%esi, %esi
	movl	%esi, 200(%rsp)
	xorl	%eax, %eax
	movl	%eax, 204(%rsp)
	xorl	%r10d, %r10d
	movl	%r10d, 208(%rsp)
	xorl	%ecx, %ecx
	movl	%ecx, 212(%rsp)
	xorl	%ecx, %ecx
	movl	%ecx, 216(%rsp)
	xorl	%edi, %edi
	movl	%edi, 220(%rsp)
	xorl	%r10d, %r10d
	movl	%r10d, 224(%rsp)
	xorl	%esi, %esi
	movl	%esi, 228(%rsp)
	xorl	%r9d, %r9d
	movl	%r9d, 232(%rsp)
	xorl	%r11d, %r11d
	movl	%r11d, 236(%rsp)
	xorl	%esi, %esi
	movl	%esi, 240(%rsp)
	xorl	%eax, %eax
	movl	%eax, 244(%rsp)
	xorl	%edi, %edi
	movl	%edi, 248(%rsp)
	xorl	%edi, %edi
	movl	%edi, 252(%rsp)
	movl	92(%rsp), %edi
	movq	%rdi, %rbx
	leaq	128(%rsp), %rsi
	leaq	192(%rsp), %rdi
	movq	$64, %rdx
	call	memcpy
	movq	%rbx, %r9
	movl	%r9d, 112(%rsp)
	movl	240(%rsp), %edx
	leal	0(%edx,%r9d,1), %r14d
	movl	192(%rsp), %ecx
	movl	208(%rsp), %r9d
	leal	0(%ecx,%r9d,1), %r11d
	xorl	%r11d, %r14d
	rorl	$16, %r14d
	movl	224(%rsp), %esi
	leal	0(%esi,%r14d,1), %r10d
	xorl	%r10d, %r9d
	rorl	$20, %r9d
	leal	0(%r11d,%r9d,1), %r13d
	xorl	%r13d, %r14d
	rorl	$24, %r14d
	leal	0(%r10d,%r14d,1), %r12d
	xorl	%r12d, %r9d
	rorl	$25, %r9d
	movl	196(%rsp), %r11d
	movl	212(%rsp), %ebp
	leal	0(%r11d,%ebp,1), %r8d
	movl	244(%rsp), %ebx
	xorl	%r8d, %ebx
	rorl	$16, %ebx
	movl	228(%rsp), %r10d
	leal	0(%r10d,%ebx,1), %edi
	xorl	%edi, %ebp
	rorl	$20, %ebp
	leal	0(%r8d,%ebp,1), %r11d
	xorl	%r11d, %ebx
	rorl	$24, %ebx
	leal	0(%edi,%ebx,1), %ecx
	xorl	%ecx, %ebp
	rorl	$25, %ebp
	movl	200(%rsp), %r10d
	movl	216(%rsp), %esi
	leal	0(%r10d,%esi,1), %r10d
	movl	248(%rsp), %edi
	xorl	%r10d, %edi
	rorl	$16, %edi
	movl	232(%rsp), %eax
	leal	0(%eax,%edi,1), %r8d
	xorl	%r8d, %esi
	rorl	$20, %esi
	leal	0(%r10d,%esi,1), %r10d
	movl	%r10d, 56(%rsp)
	xorl	%r10d, %edi
	rorl	$24, %edi
	leal	0(%r8d,%edi,1), %eax
	xorl	%eax, %esi
	rorl	$25, %esi
	movl	204(%rsp), %edx
	movl	220(%rsp), %r8d
	leal	0(%edx,%r8d,1), %edx
	movl	252(%rsp), %r15d
	xorl	%edx, %r15d
	rorl	$16, %r15d
	movl	236(%rsp), %r10d
	leal	0(%r10d,%r15d,1), %r10d
	xorl	%r10d, %r8d
	rorl	$20, %r8d
	leal	0(%edx,%r8d,1), %edx
	xorl	%edx, %r15d
	rorl	$24, %r15d
	leal	0(%r10d,%r15d,1), %r10d
	xorl	%r10d, %r8d
	rorl	$25, %r8d
	leal	0(%r13d,%ebp,1), %r13d
	xorl	%r13d, %r15d
	rorl	$16, %r15d
	leal	0(%eax,%r15d,1), %eax
	xorl	%eax, %ebp
	rorl	$20, %ebp
	leal	0(%r13d,%ebp,1), %r13d
	xorl	%r13d, %r15d
	rorl	$24, %r15d
	movl	%r15d, 68(%rsp)
	movl	68(%rsp), %r15d
	leal	0(%eax,%r15d,1), %eax
	movl	%eax, 64(%rsp)
	movl	64(%rsp), %eax
	xorl	%eax, %ebp
	rorl	$25, %ebp
	leal	0(%r11d,%esi,1), %r11d
	movq	%r14, %rax
	xorl	%r11d, %eax
	rorl	$16, %eax
	leal	0(%r10d,%eax,1), %r10d
	xorl	%r10d, %esi
	rorl	$20, %esi
	leal	0(%r11d,%esi,1), %r11d
	movq	%rax, %r14
	movq	%r11, %rax
	xorl	%eax, %r14d
	rorl	$24, %r14d
	movq	%r10, %r11
	movq	%r14, %r10
	leal	0(%r11d,%r10d,1), %r11d
	xorl	%r11d, %esi
	rorl	$25, %esi
	movl	56(%rsp), %r14d
	leal	0(%r14d,%r8d,1), %r15d
	movq	%rbx, %r14
	movq	%r15, %rbx
	xorl	%ebx, %r14d
	rorl	$16, %r14d
	leal	0(%r12d,%r14d,1), %r12d
	xorl	%r12d, %r8d
	rorl	$20, %r8d
	leal	0(%ebx,%r8d,1), %ebx
	movq	%r14, %r15
	xorl	%ebx, %r15d
	rorl	$24, %r15d
	movq	%r12, %r14
	movq	%r15, %r12
	leal	0(%r14d,%r12d,1), %r14d
	xorl	%r14d, %r8d
	rorl	$25, %r8d
	movq	%rdx, %r15
	movq	%r9, %rdx
	leal	0(%r15d,%edx,1), %r9d
	xorl	%r9d, %edi
	rorl	$16, %edi
	leal	0(%ecx,%edi,1), %ecx
	xorl	%ecx, %edx
	rorl	$20, %edx
	leal	0(%r9d,%edx,1), %r9d
	movq	%rdi, %r15
	xorl	%r9d, %r15d
	rorl	$24, %r15d
	movq	%rcx, %rdi
	movq	%r15, %rcx
	leal	0(%edi,%ecx,1), %edi
	xorl	%edi, %edx
	rorl	$25, %edx
	leal	0(%r13d,%edx,1), %r13d
	movq	%r10, %r15
	movq	%r13, %r10
	xorl	%r10d, %r15d
	rorl	$16, %r15d
	movq	%r15, %r13
	leal	0(%r14d,%r13d,1), %r14d
	xorl	%r14d, %edx
	rorl	$20, %edx
	movq	%r10, %r15
	movq	%rdx, %r10
	leal	0(%r15d,%r10d,1), %edx
	xorl	%edx, %r13d
	rorl	$24, %r13d
	movq	%r14, %r15
	movq	%r13, %r14
	leal	0(%r15d,%r14d,1), %r13d
	xorl	%r13d, %r10d
	rorl	$25, %r10d
	movq	%rbp, %r15
	leal	0(%eax,%r15d,1), %eax
	movq	%r12, %rbp
	xorl	%eax, %ebp
	rorl	$16, %ebp
	leal	0(%edi,%ebp,1), %edi
	xorl	%edi, %r15d
	rorl	$20, %r15d
	movq	%rax, %r12
	movq	%r15, %rax
	leal	0(%r12d,%eax,1), %r12d
	xorl	%r12d, %ebp
	rorl	$24, %ebp
	leal	0(%edi,%ebp,1), %edi
	movl	%edi, 60(%rsp)
	movl	60(%rsp), %edi
	xorl	%edi, %eax
	rorl	$25, %eax
	movq	%rbx, %rdi
	leal	0(%edi,%esi,1), %edi
	movq	%rdi, %rbx
	xorl	%ebx, %ecx
	rorl	$16, %ecx
	movl	64(%rsp), %edi
	leal	0(%edi,%ecx,1), %edi
	xorl	%edi, %esi
	rorl	$20, %esi
	leal	0(%ebx,%esi,1), %ebx
	xorl	%ebx, %ecx
	rorl	$24, %ecx
	leal	0(%edi,%ecx,1), %edi
	xorl	%edi, %esi
	rorl	$25, %esi
	leal	0(%r9d,%r8d,1), %r9d
	movl	68(%rsp), %r15d
	xorl	%r9d, %r15d
	rorl	$16, %r15d
	leal	0(%r11d,%r15d,1), %r11d
	xorl	%r11d, %r8d
	rorl	$20, %r8d
	leal	0(%r9d,%r8d,1), %r9d
	xorl	%r9d, %r15d
	rorl	$24, %r15d
	leal	0(%r11d,%r15d,1), %r11d
	xorl	%r11d, %r8d
	rorl	$25, %r8d
	leal	0(%edx,%eax,1), %edx
	xorl	%edx, %r15d
	rorl	$16, %r15d
	leal	0(%edi,%r15d,1), %edi
	xorl	%edi, %eax
	rorl	$20, %eax
	leal	0(%edx,%eax,1), %edx
	xorl	%edx, %r15d
	rorl	$24, %r15d
	movl	%r15d, 64(%rsp)
	movl	64(%rsp), %r15d
	leal	0(%edi,%r15d,1), %edi
	xorl	%edi, %eax
	rorl	$25, %eax
	leal	0(%r12d,%esi,1), %r12d
	movq	%r12, %r15
	xorl	%r15d, %r14d
	rorl	$16, %r14d
	leal	0(%r11d,%r14d,1), %r11d
	movq	%r11, %r12
	xorl	%r12d, %esi
	rorl	$20, %esi
	movq	%r15, %r11
	leal	0(%r11d,%esi,1), %r11d
	xorl	%r11d, %r14d
	rorl	$24, %r14d
	movq	%r12, %r15
	movq	%r14, %r12
	leal	0(%r15d,%r12d,1), %r14d
	movl	%r14d, 56(%rsp)
	movl	56(%rsp), %r14d
	xorl	%r14d, %esi
	rorl	$25, %esi
	leal	0(%ebx,%r8d,1), %ebx
	movq	%rbp, %r14
	movq	%rbx, %rbp
	xorl	%ebp, %r14d
	rorl	$16, %r14d
	movq	%r13, %rbx
	movq	%r14, %r13
	leal	0(%ebx,%r13d,1), %ebx
	xorl	%ebx, %r8d
	rorl	$20, %r8d
	leal	0(%ebp,%r8d,1), %ebp
	xorl	%ebp, %r13d
	rorl	$24, %r13d
	movq	%rbx, %r14
	movq	%r13, %rbx
	leal	0(%r14d,%ebx,1), %r13d
	xorl	%r13d, %r8d
	rorl	$25, %r8d
	movq	%r10, %r14
	leal	0(%r9d,%r14d,1), %r9d
	xorl	%r9d, %ecx
	rorl	$16, %ecx
	movl	60(%rsp), %r10d
	leal	0(%r10d,%ecx,1), %r10d
	xorl	%r10d, %r14d
	rorl	$20, %r14d
	leal	0(%r9d,%r14d,1), %r9d
	xorl	%r9d, %ecx
	rorl	$24, %ecx
	leal	0(%r10d,%ecx,1), %r10d
	xorl	%r10d, %r14d
	rorl	$25, %r14d
	leal	0(%edx,%r14d,1), %r15d
	movq	%r12, %rdx
	movq	%r15, %r12
	xorl	%r12d, %edx
	rorl	$16, %edx
	leal	0(%r13d,%edx,1), %r13d
	movq	%r13, %r15
	xorl	%r15d, %r14d
	rorl	$20, %r14d
	leal	0(%r12d,%r14d,1), %r12d
	movq	%r12, %r13
	xorl	%r13d, %edx
	rorl	$24, %edx
	movq	%rdx, %r12
	leal	0(%r15d,%r12d,1), %edx
	xorl	%edx, %r14d
	rorl	$25, %r14d
	movl	%r14d, 60(%rsp)
	leal	0(%r11d,%eax,1), %r11d
	xorl	%r11d, %ebx
	rorl	$16, %ebx
	leal	0(%r10d,%ebx,1), %r10d
	movq	%rax, %r14
	movq	%r10, %rax
	xorl	%eax, %r14d
	rorl	$20, %r14d
	movq	%r11, %r10
	movq	%r14, %r11
	leal	0(%r10d,%r11d,1), %r10d
	movq	%r10, %r14
	xorl	%r14d, %ebx
	rorl	$24, %ebx
	movq	%rbx, %r10
	leal	0(%eax,%r10d,1), %r10d
	xorl	%r10d, %r11d
	rorl	$25, %r11d
	movq	%rbp, %rax
	leal	0(%eax,%esi,1), %ebp
	movq	%rcx, %rax
	movq	%rbp, %rcx
	xorl	%ecx, %eax
	rorl	$16, %eax
	leal	0(%edi,%eax,1), %edi
	xorl	%edi, %esi
	rorl	$20, %esi
	leal	0(%ecx,%esi,1), %ecx
	movq	%rcx, %rbp
	xorl	%ebp, %eax
	rorl	$24, %eax
	movq	%rax, %rcx
	leal	0(%edi,%ecx,1), %edi
	xorl	%edi, %esi
	rorl	$25, %esi
	leal	0(%r9d,%r8d,1), %r9d
	movl	64(%rsp), %r15d
	xorl	%r9d, %r15d
	rorl	$16, %r15d
	movl	56(%rsp), %eax
	leal	0(%eax,%r15d,1), %eax
	xorl	%eax, %r8d
	rorl	$20, %r8d
	leal	0(%r9d,%r8d,1), %r9d
	xorl	%r9d, %r15d
	rorl	$24, %r15d
	leal	0(%eax,%r15d,1), %eax
	xorl	%eax, %r8d
	rorl	$25, %r8d
	leal	0(%r13d,%r11d,1), %r13d
	xorl	%r13d, %r15d
	rorl	$16, %r15d
	leal	0(%edi,%r15d,1), %edi
	xorl	%edi, %r11d
	rorl	$20, %r11d
	leal	0(%r13d,%r11d,1), %r13d
	xorl	%r13d, %r15d
	rorl	$24, %r15d
	movl	%r15d, 64(%rsp)
	movl	64(%rsp), %r15d
	leal	0(%edi,%r15d,1), %edi
	movl	%edi, 56(%rsp)
	movl	56(%rsp), %edi
	xorl	%edi, %r11d
	rorl	$25, %r11d
	movq	%r14, %rdi
	leal	0(%edi,%esi,1), %r14d
	movq	%r12, %rdi
	movq	%r14, %r12
	xorl	%r12d, %edi
	rorl	$16, %edi
	leal	0(%eax,%edi,1), %eax
	movq	%rax, %r14
	xorl	%r14d, %esi
	rorl	$20, %esi
	movq	%r12, %rax
	leal	0(%eax,%esi,1), %eax
	xorl	%eax, %edi
	rorl	$24, %edi
	movq	%r14, %r12
	movq	%rdi, %r14
	leal	0(%r12d,%r14d,1), %edi
	xorl	%edi, %esi
	rorl	$25, %esi
	leal	0(%ebp,%r8d,1), %ebp
	xorl	%ebp, %ebx
	rorl	$16, %ebx
	leal	0(%edx,%ebx,1), %edx
	movq	%r8, %r12
	movq	%rdx, %r8
	xorl	%r8d, %r12d
	rorl	$20, %r12d
	movq	%r12, %rdx
	leal	0(%ebp,%edx,1), %ebp
	xorl	%ebp, %ebx
	rorl	$24, %ebx
	leal	0(%r8d,%ebx,1), %r15d
	xorl	%r15d, %edx
	rorl	$25, %edx
	movl	60(%rsp), %r8d
	leal	0(%r9d,%r8d,1), %r12d
	movq	%rcx, %r9
	movq	%r12, %rcx
	xorl	%ecx, %r9d
	rorl	$16, %r9d
	movq	%r9, %r12
	leal	0(%r10d,%r12d,1), %r9d
	movq	%r9, %r10
	xorl	%r10d, %r8d
	rorl	$20, %r8d
	movq	%r8, %r9
	leal	0(%ecx,%r9d,1), %r8d
	xorl	%r8d, %r12d
	rorl	$24, %r12d
	leal	0(%r10d,%r12d,1), %ecx
	xorl	%ecx, %r9d
	rorl	$25, %r9d
	leal	0(%r13d,%r9d,1), %r10d
	movq	%r14, %r13
	xorl	%r10d, %r13d
	rorl	$16, %r13d
	movq	%r15, %r14
	leal	0(%r14d,%r13d,1), %r14d
	xorl	%r14d, %r9d
	rorl	$20, %r9d
	leal	0(%r10d,%r9d,1), %r10d
	xorl	%r10d, %r13d
	rorl	$24, %r13d
	movq	%r14, %r15
	movq	%r13, %r14
	leal	0(%r15d,%r14d,1), %r13d
	movl	%r13d, 60(%rsp)
	movl	60(%rsp), %r13d
	xorl	%r13d, %r9d
	rorl	$25, %r9d
	leal	0(%eax,%r11d,1), %eax
	xorl	%eax, %ebx
	rorl	$16, %ebx
	leal	0(%ecx,%ebx,1), %ecx
	movq	%r11, %r13
	movq	%rcx, %r11
	xorl	%r11d, %r13d
	rorl	$20, %r13d
	movq	%rax, %rcx
	movq	%r13, %rax
	leal	0(%ecx,%eax,1), %ecx
	xorl	%ecx, %ebx
	rorl	$24, %ebx
	leal	0(%r11d,%ebx,1), %r11d
	movq	%r11, %r13
	movq	%r13, %r11
	xorl	%r11d, %eax
	rorl	$25, %eax
	leal	0(%ebp,%esi,1), %r11d
	movq	%r12, %rbp
	xorl	%r11d, %ebp
	rorl	$16, %ebp
	movl	56(%rsp), %r12d
	leal	0(%r12d,%ebp,1), %r15d
	movq	%rsi, %r12
	movq	%r15, %rsi
	xorl	%esi, %r12d
	rorl	$20, %r12d
	leal	0(%r11d,%r12d,1), %r11d
	xorl	%r11d, %ebp
	rorl	$24, %ebp
	movq	%rsi, %r15
	movq	%rbp, %rsi
	leal	0(%r15d,%esi,1), %ebp
	xorl	%ebp, %r12d
	rorl	$25, %r12d
	leal	0(%r8d,%edx,1), %r8d
	movl	64(%rsp), %r15d
	xorl	%r8d, %r15d
	rorl	$16, %r15d
	leal	0(%edi,%r15d,1), %edi
	xorl	%edi, %edx
	rorl	$20, %edx
	leal	0(%r8d,%edx,1), %r8d
	xorl	%r8d, %r15d
	rorl	$24, %r15d
	leal	0(%edi,%r15d,1), %edi
	xorl	%edi, %edx
	rorl	$25, %edx
	leal	0(%r10d,%eax,1), %r10d
	xorl	%r10d, %r15d
	rorl	$16, %r15d
	leal	0(%ebp,%r15d,1), %ebp
	xorl	%ebp, %eax
	rorl	$20, %eax
	leal	0(%r10d,%eax,1), %r10d
	xorl	%r10d, %r15d
	rorl	$24, %r15d
	movl	%r15d, 64(%rsp)
	movl	64(%rsp), %r15d
	leal	0(%ebp,%r15d,1), %ebp
	xorl	%ebp, %eax
	rorl	$25, %eax
	leal	0(%ecx,%r12d,1), %ecx
	xorl	%ecx, %r14d
	rorl	$16, %r14d
	leal	0(%edi,%r14d,1), %edi
	movq	%r12, %r15
	xorl	%edi, %r15d
	rorl	$20, %r15d
	movq	%rcx, %r12
	movq	%r15, %rcx
	leal	0(%r12d,%ecx,1), %r12d
	xorl	%r12d, %r14d
	rorl	$24, %r14d
	movq	%rdi, %r15
	movq	%r14, %rdi
	leal	0(%r15d,%edi,1), %r14d
	movl	%r14d, 56(%rsp)
	movl	56(%rsp), %r14d
	xorl	%r14d, %ecx
	rorl	$25, %ecx
	movq	%r11, %r14
	movq	%rdx, %r11
	leal	0(%r14d,%r11d,1), %edx
	xorl	%edx, %ebx
	rorl	$16, %ebx
	movl	60(%rsp), %r14d
	leal	0(%r14d,%ebx,1), %r14d
	xorl	%r14d, %r11d
	rorl	$20, %r11d
	leal	0(%edx,%r11d,1), %edx
	xorl	%edx, %ebx
	rorl	$24, %ebx
	leal	0(%r14d,%ebx,1), %r14d
	xorl	%r14d, %r11d
	rorl	$25, %r11d
	leal	0(%r8d,%r9d,1), %r15d
	movq	%rsi, %r8
	movq	%r15, %rsi
	xorl	%esi, %r8d
	rorl	$16, %r8d
	leal	0(%r13d,%r8d,1), %r13d
	xorl	%r13d, %r9d
	rorl	$20, %r9d
	leal	0(%esi,%r9d,1), %esi
	xorl	%esi, %r8d
	rorl	$24, %r8d
	leal	0(%r13d,%r8d,1), %r13d
	xorl	%r13d, %r9d
	rorl	$25, %r9d
	leal	0(%r10d,%r9d,1), %r10d
	xorl	%r10d, %edi
	rorl	$16, %edi
	movq	%rdi, %r15
	leal	0(%r14d,%r15d,1), %r14d
	movq	%r9, %rdi
	movq	%r14, %r9
	xorl	%r9d, %edi
	rorl	$20, %edi
	leal	0(%r10d,%edi,1), %r10d
	movq	%r15, %r14
	xorl	%r10d, %r14d
	rorl	$24, %r14d
	leal	0(%r9d,%r14d,1), %r9d
	xorl	%r9d, %edi
	rorl	$25, %edi
	movl	%edi, 60(%rsp)
	movq	%r12, %rdi
	leal	0(%edi,%eax,1), %edi
	xorl	%edi, %ebx
	rorl	$16, %ebx
	leal	0(%r13d,%ebx,1), %r12d
	xorl	%r12d, %eax
	rorl	$20, %eax
	leal	0(%edi,%eax,1), %edi
	movq	%rdi, %r13
	xorl	%r13d, %ebx
	rorl	$24, %ebx
	movq	%r12, %rdi
	leal	0(%edi,%ebx,1), %edi
	xorl	%edi, %eax
	rorl	$25, %eax
	leal	0(%edx,%ecx,1), %edx
	xorl	%edx, %r8d
	rorl	$16, %r8d
	leal	0(%ebp,%r8d,1), %ebp
	movq	%rbp, %r12
	xorl	%r12d, %ecx
	rorl	$20, %ecx
	leal	0(%edx,%ecx,1), %ebp
	movq	%r8, %rdx
	xorl	%ebp, %edx
	rorl	$24, %edx
	movq	%r12, %r8
	leal	0(%r8d,%edx,1), %r8d
	xorl	%r8d, %ecx
	rorl	$25, %ecx
	leal	0(%esi,%r11d,1), %esi
	movl	64(%rsp), %r15d
	xorl	%esi, %r15d
	rorl	$16, %r15d
	movl	56(%rsp), %r12d
	leal	0(%r12d,%r15d,1), %r12d
	xorl	%r12d, %r11d
	rorl	$20, %r11d
	leal	0(%esi,%r11d,1), %esi
	xorl	%esi, %r15d
	rorl	$24, %r15d
	leal	0(%r12d,%r15d,1), %r12d
	xorl	%r12d, %r11d
	rorl	$25, %r11d
	leal	0(%r10d,%eax,1), %r10d
	xorl	%r10d, %r15d
	rorl	$16, %r15d
	leal	0(%r8d,%r15d,1), %r8d
	xorl	%r8d, %eax
	rorl	$20, %eax
	leal	0(%r10d,%eax,1), %r10d
	xorl	%r10d, %r15d
	rorl	$24, %r15d
	movl	%r15d, 64(%rsp)
	movl	64(%rsp), %r15d
	leal	0(%r8d,%r15d,1), %r8d
	movl	%r8d, 56(%rsp)
	movl	56(%rsp), %r8d
	xorl	%r8d, %eax
	rorl	$25, %eax
	movq	%r13, %r8
	leal	0(%r8d,%ecx,1), %r13d
	movq	%r14, %r8
	xorl	%r13d, %r8d
	rorl	$16, %r8d
	leal	0(%r12d,%r8d,1), %r12d
	xorl	%r12d, %ecx
	rorl	$20, %ecx
	leal	0(%r13d,%ecx,1), %r13d
	movq	%r8, %r14
	xorl	%r13d, %r14d
	rorl	$24, %r14d
	movq	%r12, %r8
	leal	0(%r8d,%r14d,1), %r8d
	xorl	%r8d, %ecx
	rorl	$25, %ecx
	leal	0(%ebp,%r11d,1), %ebp
	xorl	%ebp, %ebx
	rorl	$16, %ebx
	leal	0(%r9d,%ebx,1), %r9d
	xorl	%r9d, %r11d
	rorl	$20, %r11d
	leal	0(%ebp,%r11d,1), %r12d
	movq	%rbx, %rbp
	movq	%r12, %rbx
	xorl	%ebx, %ebp
	rorl	$24, %ebp
	movq	%rbp, %r12
	leal	0(%r9d,%r12d,1), %r9d
	xorl	%r9d, %r11d
	rorl	$25, %r11d
	movq	%rsi, %rbp
	movl	60(%rsp), %esi
	leal	0(%ebp,%esi,1), %ebp
	xorl	%ebp, %edx
	rorl	$16, %edx
	leal	0(%edi,%edx,1), %edi
	xorl	%edi, %esi
	rorl	$20, %esi
	leal	0(%ebp,%esi,1), %r15d
	movq	%rdx, %rbp
	movq	%r15, %rdx
	xorl	%edx, %ebp
	rorl	$24, %ebp
	leal	0(%edi,%ebp,1), %edi
	xorl	%edi, %esi
	rorl	$25, %esi
	leal	0(%r10d,%esi,1), %r10d
	xorl	%r10d, %r14d
	rorl	$16, %r14d
	movq	%r9, %r15
	movq	%r14, %r9
	leal	0(%r15d,%r9d,1), %r14d
	xorl	%r14d, %esi
	rorl	$20, %esi
	leal	0(%r10d,%esi,1), %r10d
	xorl	%r10d, %r9d
	rorl	$24, %r9d
	leal	0(%r14d,%r9d,1), %r14d
	movl	%r14d, 60(%rsp)
	movl	60(%rsp), %r14d
	xorl	%r14d, %esi
	rorl	$25, %esi
	leal	0(%r13d,%eax,1), %r13d
	xorl	%r13d, %r12d
	rorl	$16, %r12d
	leal	0(%edi,%r12d,1), %edi
	xorl	%edi, %eax
	rorl	$20, %eax
	leal	0(%r13d,%eax,1), %r13d
	xorl	%r13d, %r12d
	rorl	$24, %r12d
	movq	%rdi, %r14
	movq	%r12, %rdi
	leal	0(%r14d,%edi,1), %r12d
	xorl	%r12d, %eax
	rorl	$25, %eax
	leal	0(%ebx,%ecx,1), %ebx
	movq	%rbp, %r14
	movq	%rbx, %rbp
	xorl	%ebp, %r14d
	rorl	$16, %r14d
	movl	56(%rsp), %ebx
	leal	0(%ebx,%r14d,1), %ebx
	xorl	%ebx, %ecx
	rorl	$20, %ecx
	leal	0(%ebp,%ecx,1), %ebp
	xorl	%ebp, %r14d
	rorl	$24, %r14d
	leal	0(%ebx,%r14d,1), %ebx
	xorl	%ebx, %ecx
	rorl	$25, %ecx
	leal	0(%edx,%r11d,1), %edx
	movl	64(%rsp), %r15d
	xorl	%edx, %r15d
	rorl	$16, %r15d
	leal	0(%r8d,%r15d,1), %r8d
	xorl	%r8d, %r11d
	rorl	$20, %r11d
	leal	0(%edx,%r11d,1), %edx
	xorl	%edx, %r15d
	rorl	$24, %r15d
	leal	0(%r8d,%r15d,1), %r8d
	xorl	%r8d, %r11d
	rorl	$25, %r11d
	leal	0(%r10d,%eax,1), %r10d
	xorl	%r10d, %r15d
	rorl	$16, %r15d
	leal	0(%ebx,%r15d,1), %ebx
	xorl	%ebx, %eax
	rorl	$20, %eax
	leal	0(%r10d,%eax,1), %r10d
	xorl	%r10d, %r15d
	rorl	$24, %r15d
	movl	%r15d, 64(%rsp)
	movl	64(%rsp), %r15d
	leal	0(%ebx,%r15d,1), %ebx
	xorl	%ebx, %eax
	rorl	$25, %eax
	movq	%r13, %r15
	movq	%rcx, %r13
	leal	0(%r15d,%r13d,1), %ecx
	xorl	%ecx, %r9d
	rorl	$16, %r9d
	movq	%r8, %r15
	movq	%r9, %r8
	leal	0(%r15d,%r8d,1), %r9d
	xorl	%r9d, %r13d
	rorl	$20, %r13d
	leal	0(%ecx,%r13d,1), %ecx
	xorl	%ecx, %r8d
	rorl	$24, %r8d
	leal	0(%r9d,%r8d,1), %r15d
	movq	%r13, %r9
	movl	%r15d, 56(%rsp)
	movl	56(%rsp), %r13d
	xorl	%r13d, %r9d
	rorl	$25, %r9d
	movq	%r11, %r13
	leal	0(%ebp,%r13d,1), %r11d
	movq	%r11, %rbp
	xorl	%ebp, %edi
	rorl	$16, %edi
	movl	60(%rsp), %r11d
	leal	0(%r11d,%edi,1), %r11d
	xorl	%r11d, %r13d
	rorl	$20, %r13d
	leal	0(%ebp,%r13d,1), %ebp
	xorl	%ebp, %edi
	rorl	$24, %edi
	leal	0(%r11d,%edi,1), %r11d
	xorl	%r11d, %r13d
	rorl	$25, %r13d
	leal	0(%edx,%esi,1), %edx
	movq	%rdx, %r15
	xorl	%r15d, %r14d
	rorl	$16, %r14d
	movq	%r14, %rdx
	leal	0(%r12d,%edx,1), %r12d
	movq	%r12, %r14
	xorl	%r14d, %esi
	rorl	$20, %esi
	movq	%r15, %r12
	leal	0(%r12d,%esi,1), %r12d
	xorl	%r12d, %edx
	rorl	$24, %edx
	leal	0(%r14d,%edx,1), %r15d
	movq	%rsi, %r14
	movq	%r15, %rsi
	xorl	%esi, %r14d
	rorl	$25, %r14d
	leal	0(%r10d,%r14d,1), %r10d
	movq	%r10, %r15
	xorl	%r15d, %r8d
	rorl	$16, %r8d
	movq	%r11, %r10
	leal	0(%r10d,%r8d,1), %r10d
	movq	%r10, %r11
	xorl	%r11d, %r14d
	rorl	$20, %r14d
	movq	%r15, %r10
	leal	0(%r10d,%r14d,1), %r10d
	xorl	%r10d, %r8d
	rorl	$24, %r8d
	leal	0(%r11d,%r8d,1), %r11d
	xorl	%r11d, %r14d
	rorl	$25, %r14d
	movl	%r14d, 60(%rsp)
	leal	0(%ecx,%eax,1), %ecx
	xorl	%ecx, %edi
	rorl	$16, %edi
	leal	0(%esi,%edi,1), %esi
	movq	%rax, %r14
	movq	%rsi, %rax
	xorl	%eax, %r14d
	rorl	$20, %r14d
	movq	%rcx, %rsi
	movq	%r14, %rcx
	leal	0(%esi,%ecx,1), %esi
	movq	%rsi, %r14
	xorl	%r14d, %edi
	rorl	$24, %edi
	movq	%rdi, %rsi
	leal	0(%eax,%esi,1), %edi
	xorl	%edi, %ecx
	rorl	$25, %ecx
	movq	%rbp, %rax
	leal	0(%eax,%r9d,1), %ebp
	movq	%rdx, %rax
	movq	%rbp, %rdx
	xorl	%edx, %eax
	rorl	$16, %eax
	leal	0(%ebx,%eax,1), %ebx
	movq	%r9, %rbp
	movq	%rbx, %r9
	xorl	%r9d, %ebp
	rorl	$20, %ebp
	movq	%rbp, %rbx
	leal	0(%edx,%ebx,1), %edx
	movq	%rdx, %rbp
	xorl	%ebp, %eax
	rorl	$24, %eax
	movq	%r9, %rdx
	movq	%rax, %r9
	leal	0(%edx,%r9d,1), %edx
	xorl	%edx, %ebx
	rorl	$25, %ebx
	movq	%r12, %rax
	leal	0(%eax,%r13d,1), %r12d
	movl	64(%rsp), %r15d
	xorl	%r12d, %r15d
	rorl	$16, %r15d
	movl	56(%rsp), %eax
	leal	0(%eax,%r15d,1), %eax
	xorl	%eax, %r13d
	rorl	$20, %r13d
	leal	0(%r12d,%r13d,1), %r12d
	xorl	%r12d, %r15d
	rorl	$24, %r15d
	leal	0(%eax,%r15d,1), %eax
	xorl	%eax, %r13d
	rorl	$25, %r13d
	leal	0(%r10d,%ecx,1), %r10d
	xorl	%r10d, %r15d
	rorl	$16, %r15d
	leal	0(%edx,%r15d,1), %edx
	xorl	%edx, %ecx
	rorl	$20, %ecx
	leal	0(%r10d,%ecx,1), %r10d
	xorl	%r10d, %r15d
	rorl	$24, %r15d
	movl	%r15d, 64(%rsp)
	movl	64(%rsp), %r15d
	leal	0(%edx,%r15d,1), %edx
	movl	%edx, 56(%rsp)
	movl	56(%rsp), %edx
	xorl	%edx, %ecx
	rorl	$25, %ecx
	movq	%r14, %rdx
	leal	0(%edx,%ebx,1), %r14d
	movq	%r8, %rdx
	movq	%r14, %r8
	xorl	%r8d, %edx
	rorl	$16, %edx
	leal	0(%eax,%edx,1), %eax
	movq	%rax, %r14
	xorl	%r14d, %ebx
	rorl	$20, %ebx
	movq	%r8, %rax
	leal	0(%eax,%ebx,1), %r8d
	movq	%r8, %rax
	xorl	%eax, %edx
	rorl	$24, %edx
	movq	%r14, %r8
	leal	0(%r8d,%edx,1), %r8d
	xorl	%r8d, %ebx
	rorl	$25, %ebx
	leal	0(%ebp,%r13d,1), %r14d
	movq	%rsi, %rbp
	movq	%r14, %rsi
	xorl	%esi, %ebp
	rorl	$16, %ebp
	leal	0(%r11d,%ebp,1), %r11d
	xorl	%r11d, %r13d
	rorl	$20, %r13d
	movq	%rsi, %r14
	movq	%r13, %rsi
	leal	0(%r14d,%esi,1), %r13d
	xorl	%r13d, %ebp
	rorl	$24, %ebp
	movq	%r11, %r14
	movq	%rbp, %r11
	leal	0(%r14d,%r11d,1), %ebp
	xorl	%ebp, %esi
	rorl	$25, %esi
	movq	%r12, %r14
	movl	60(%rsp), %r12d
	leal	0(%r14d,%r12d,1), %r14d
	xorl	%r14d, %r9d
	rorl	$16, %r9d
	leal	0(%edi,%r9d,1), %edi
	movq	%r12, %r15
	movq	%rdi, %r12
	xorl	%r12d, %r15d
	rorl	$20, %r15d
	movq	%r14, %rdi
	movq	%r15, %r14
	leal	0(%edi,%r14d,1), %edi
	xorl	%edi, %r9d
	rorl	$24, %r9d
	leal	0(%r12d,%r9d,1), %r12d
	xorl	%r12d, %r14d
	rorl	$25, %r14d
	movq	%r14, %r15
	leal	0(%r10d,%r15d,1), %r10d
	movq	%rdx, %r14
	xorl	%r10d, %r14d
	rorl	$16, %r14d
	movq	%rbp, %rdx
	leal	0(%edx,%r14d,1), %ebp
	movq	%r15, %rdx
	xorl	%ebp, %edx
	rorl	$20, %edx
	leal	0(%r10d,%edx,1), %r10d
	xorl	%r10d, %r14d
	rorl	$24, %r14d
	leal	0(%ebp,%r14d,1), %ebp
	xorl	%ebp, %edx
	rorl	$25, %edx
	leal	0(%eax,%ecx,1), %eax
	movq	%r11, %r15
	movq	%rax, %r11
	xorl	%r11d, %r15d
	rorl	$16, %r15d
	movq	%r12, %rax
	leal	0(%eax,%r15d,1), %eax
	movq	%rcx, %r12
	movq	%rax, %rcx
	xorl	%ecx, %r12d
	rorl	$20, %r12d
	movq	%r11, %rax
	movq	%r12, %r11
	leal	0(%eax,%r11d,1), %eax
	movq	%r15, %r12
	xorl	%eax, %r12d
	rorl	$24, %r12d
	leal	0(%ecx,%r12d,1), %ecx
	movl	%ecx, 60(%rsp)
	movl	60(%rsp), %ecx
	xorl	%ecx, %r11d
	rorl	$25, %r11d
	leal	0(%r13d,%ebx,1), %r13d
	movq	%r9, %rcx
	movq	%r13, %r9
	xorl	%r9d, %ecx
	rorl	$16, %ecx
	movl	56(%rsp), %r13d
	leal	0(%r13d,%ecx,1), %r13d
	xorl	%r13d, %ebx
	rorl	$20, %ebx
	leal	0(%r9d,%ebx,1), %r9d
	xorl	%r9d, %ecx
	rorl	$24, %ecx
	leal	0(%r13d,%ecx,1), %r13d
	xorl	%r13d, %ebx
	rorl	$25, %ebx
	leal	0(%edi,%esi,1), %edi
	movl	64(%rsp), %r15d
	xorl	%edi, %r15d
	rorl	$16, %r15d
	leal	0(%r8d,%r15d,1), %r8d
	xorl	%r8d, %esi
	rorl	$20, %esi
	leal	0(%edi,%esi,1), %edi
	xorl	%edi, %r15d
	rorl	$24, %r15d
	leal	0(%r8d,%r15d,1), %r8d
	xorl	%r8d, %esi
	rorl	$25, %esi
	leal	0(%r10d,%r11d,1), %r10d
	xorl	%r10d, %r15d
	rorl	$16, %r15d
	leal	0(%r13d,%r15d,1), %r13d
	xorl	%r13d, %r11d
	rorl	$20, %r11d
	leal	0(%r10d,%r11d,1), %r10d
	xorl	%r10d, %r15d
	rorl	$24, %r15d
	movl	%r15d, 64(%rsp)
	movl	64(%rsp), %r15d
	leal	0(%r13d,%r15d,1), %r13d
	xorl	%r13d, %r11d
	rorl	$25, %r11d
	leal	0(%eax,%ebx,1), %r15d
	movq	%r14, %rax
	movq	%r15, %r14
	xorl	%r14d, %eax
	rorl	$16, %eax
	leal	0(%r8d,%eax,1), %r15d
	movq	%rbx, %r8
	movq	%r15, %rbx
	xorl	%ebx, %r8d
	rorl	$20, %r8d
	leal	0(%r14d,%r8d,1), %r14d
	xorl	%r14d, %eax
	rorl	$24, %eax
	movq	%rbx, %r15
	movq	%rax, %rbx
	leal	0(%r15d,%ebx,1), %eax
	xorl	%eax, %r8d
	rorl	$25, %r8d
	leal	0(%r9d,%esi,1), %r15d
	movq	%r12, %r9
	movq	%r15, %r12
	xorl	%r12d, %r9d
	rorl	$16, %r9d
	leal	0(%ebp,%r9d,1), %ebp
	xorl	%ebp, %esi
	rorl	$20, %esi
	leal	0(%r12d,%esi,1), %r12d
	xorl	%r12d, %r9d
	rorl	$24, %r9d
	movq	%r9, %r15
	leal	0(%ebp,%r15d,1), %r9d
	xorl	%r9d, %esi
	rorl	$25, %esi
	leal	0(%edi,%edx,1), %edi
	xorl	%edi, %ecx
	rorl	$16, %ecx
	movl	60(%rsp), %ebp
	leal	0(%ebp,%ecx,1), %ebp
	xorl	%ebp, %edx
	rorl	$20, %edx
	leal	0(%edi,%edx,1), %edi
	xorl	%edi, %ecx
	rorl	$24, %ecx
	leal	0(%ebp,%ecx,1), %ebp
	xorl	%ebp, %edx
	rorl	$25, %edx
	leal	0(%r10d,%edx,1), %r10d
	xorl	%r10d, %ebx
	rorl	$16, %ebx
	leal	0(%r9d,%ebx,1), %r9d
	xorl	%r9d, %edx
	rorl	$20, %edx
	leal	0(%r10d,%edx,1), %r10d
	xorl	%r10d, %ebx
	rorl	$24, %ebx
	leal	0(%r9d,%ebx,1), %r9d
	xorl	%r9d, %edx
	rorl	$25, %edx
	movl	%edx, 68(%rsp)
	movq	%r11, %rdx
	leal	0(%r14d,%edx,1), %r11d
	xorl	%r11d, %r15d
	rorl	$16, %r15d
	leal	0(%ebp,%r15d,1), %ebp
	movq	%rbp, %r14
	xorl	%r14d, %edx
	rorl	$20, %edx
	leal	0(%r11d,%edx,1), %ebp
	movq	%r15, %r11
	xorl	%ebp, %r11d
	rorl	$24, %r11d
	leal	0(%r14d,%r11d,1), %r15d
	movq	%rdx, %r14
	movl	%r15d, 56(%rsp)
	movl	56(%rsp), %edx
	xorl	%edx, %r14d
	rorl	$25, %r14d
	movq	%r12, %rdx
	leal	0(%edx,%r8d,1), %edx
	xorl	%edx, %ecx
	rorl	$16, %ecx
	leal	0(%r13d,%ecx,1), %r12d
	xorl	%r12d, %r8d
	rorl	$20, %r8d
	leal	0(%edx,%r8d,1), %edx
	movq	%rcx, %r13
	movq	%rdx, %rcx
	xorl	%ecx, %r13d
	rorl	$24, %r13d
	movq	%r12, %rdx
	leal	0(%edx,%r13d,1), %r15d
	xorl	%r15d, %r8d
	rorl	$25, %r8d
	movq	%rsi, %rdx
	leal	0(%edi,%edx,1), %esi
	movl	64(%rsp), %edi
	xorl	%esi, %edi
	rorl	$16, %edi
	leal	0(%eax,%edi,1), %eax
	xorl	%eax, %edx
	rorl	$20, %edx
	leal	0(%esi,%edx,1), %r12d
	movq	%rdi, %rsi
	movq	%r12, %rdi
	xorl	%edi, %esi
	rorl	$24, %esi
	leal	0(%eax,%esi,1), %eax
	movq	%rax, %r12
	xorl	%r12d, %edx
	rorl	$25, %edx
	movq	%r10, %rax
	movq	%r14, %r10
	leal	0(%eax,%r10d,1), %r14d
	movq	%rsi, %rax
	movq	%r14, %rsi
	xorl	%esi, %eax
	rorl	$16, %eax
	movq	%rax, %r14
	leal	0(%r15d,%r14d,1), %eax
	xorl	%eax, %r10d
	rorl	$20, %r10d
	leal	0(%esi,%r10d,1), %esi
	xorl	%esi, %r14d
	rorl	$24, %r14d
	movl	%r14d, 60(%rsp)
	movl	60(%rsp), %r14d
	leal	0(%eax,%r14d,1), %eax
	xorl	%eax, %r10d
	rorl	$25, %r10d
	leal	0(%ebp,%r8d,1), %r14d
	movq	%rbx, %rbp
	movq	%r14, %rbx
	xorl	%ebx, %ebp
	rorl	$16, %ebp
	leal	0(%r12d,%ebp,1), %r12d
	movq	%r12, %r14
	xorl	%r14d, %r8d
	rorl	$20, %r8d
	leal	0(%ebx,%r8d,1), %ebx
	movq	%rbx, %r12
	xorl	%r12d, %ebp
	rorl	$24, %ebp
	movq	%r14, %rbx
	leal	0(%ebx,%ebp,1), %ebx
	xorl	%ebx, %r8d
	rorl	$25, %r8d
	leal	0(%ecx,%edx,1), %r14d
	movq	%r11, %rcx
	movq	%r14, %r11
	xorl	%r11d, %ecx
	rorl	$16, %ecx
	leal	0(%r9d,%ecx,1), %r9d
	xorl	%r9d, %edx
	rorl	$20, %edx
	leal	0(%r11d,%edx,1), %r11d
	xorl	%r11d, %ecx
	rorl	$24, %ecx
	movq	%rcx, %r14
	leal	0(%r9d,%r14d,1), %r15d
	xorl	%r15d, %edx
	rorl	$25, %edx
	movl	68(%rsp), %ecx
	leal	0(%edi,%ecx,1), %edi
	xorl	%edi, %r13d
	rorl	$16, %r13d
	movl	56(%rsp), %r9d
	leal	0(%r9d,%r13d,1), %r9d
	xorl	%r9d, %ecx
	rorl	$20, %ecx
	leal	0(%edi,%ecx,1), %edi
	xorl	%edi, %r13d
	rorl	$24, %r13d
	leal	0(%r9d,%r13d,1), %r9d
	xorl	%r9d, %ecx
	rorl	$25, %ecx
	leal	0(%esi,%ecx,1), %esi
	xorl	%esi, %ebp
	rorl	$16, %ebp
	leal	0(%r15d,%ebp,1), %r15d
	xorl	%r15d, %ecx
	rorl	$20, %ecx
	leal	0(%esi,%ecx,1), %esi
	xorl	%esi, %ebp
	rorl	$24, %ebp
	leal	0(%r15d,%ebp,1), %r15d
	movl	%r15d, 68(%rsp)
	movl	68(%rsp), %r15d
	xorl	%r15d, %ecx
	rorl	$25, %ecx
	leal	0(%r12d,%r10d,1), %r12d
	movq	%r14, %r15
	movq	%r12, %r14
	xorl	%r14d, %r15d
	rorl	$16, %r15d
	movq	%r15, %r12
	leal	0(%r9d,%r12d,1), %r9d
	xorl	%r9d, %r10d
	rorl	$20, %r10d
	leal	0(%r14d,%r10d,1), %r14d
	xorl	%r14d, %r12d
	rorl	$24, %r12d
	leal	0(%r9d,%r12d,1), %r9d
	movl	%r9d, 64(%rsp)
	movl	64(%rsp), %r9d
	xorl	%r9d, %r10d
	rorl	$25, %r10d
	movq	%r11, %r9
	leal	0(%r9d,%r8d,1), %r11d
	movq	%r13, %r9
	xorl	%r11d, %r9d
	rorl	$16, %r9d
	leal	0(%eax,%r9d,1), %eax
	xorl	%eax, %r8d
	rorl	$20, %r8d
	leal	0(%r11d,%r8d,1), %r11d
	xorl	%r11d, %r9d
	rorl	$24, %r9d
	leal	0(%eax,%r9d,1), %r13d
	movq	%r8, %rax
	movq	%r13, %r8
	xorl	%r8d, %eax
	rorl	$25, %eax
	movq	%rdi, %r13
	movq	%rdx, %rdi
	leal	0(%r13d,%edi,1), %edx
	movl	60(%rsp), %r13d
	xorl	%edx, %r13d
	rorl	$16, %r13d
	movq	%rbx, %r15
	movq	%r13, %rbx
	leal	0(%r15d,%ebx,1), %r13d
	xorl	%r13d, %edi
	rorl	$20, %edi
	leal	0(%edx,%edi,1), %edx
	movq	%rbx, %r15
	movq	%rdx, %rbx
	xorl	%ebx, %r15d
	rorl	$24, %r15d
	movq	%r15, %rdx
	leal	0(%r13d,%edx,1), %r13d
	movq	%r13, %r15
	xorl	%r15d, %edi
	rorl	$25, %edi
	leal	0(%esi,%r10d,1), %esi
	xorl	%esi, %edx
	rorl	$16, %edx
	leal	0(%r8d,%edx,1), %r8d
	xorl	%r8d, %r10d
	rorl	$20, %r10d
	leal	0(%esi,%r10d,1), %esi
	xorl	%esi, %edx
	rorl	$24, %edx
	leal	0(%r8d,%edx,1), %r13d
	movq	%r10, %r8
	xorl	%r13d, %r8d
	rorl	$25, %r8d
	leal	0(%r14d,%eax,1), %r10d
	movq	%r10, %r14
	xorl	%r14d, %ebp
	rorl	$16, %ebp
	movq	%rbp, %r10
	leal	0(%r15d,%r10d,1), %ebp
	xorl	%ebp, %eax
	rorl	$20, %eax
	leal	0(%r14d,%eax,1), %r14d
	movl	%r14d, 56(%rsp)
	movl	56(%rsp), %r14d
	xorl	%r14d, %r10d
	rorl	$24, %r10d
	movl	%r10d, 60(%rsp)
	movl	60(%rsp), %r10d
	leal	0(%ebp,%r10d,1), %r10d
	xorl	%r10d, %eax
	rorl	$25, %eax
	leal	0(%r11d,%edi,1), %r11d
	xorl	%r11d, %r12d
	rorl	$16, %r12d
	movl	68(%rsp), %ebp
	leal	0(%ebp,%r12d,1), %ebp
	xorl	%ebp, %edi
	rorl	$20, %edi
	leal	0(%r11d,%edi,1), %r11d
	xorl	%r11d, %r12d
	rorl	$24, %r12d
	leal	0(%ebp,%r12d,1), %r14d
	xorl	%r14d, %edi
	rorl	$25, %edi
	leal	0(%ebx,%ecx,1), %ebx
	xorl	%ebx, %r9d
	rorl	$16, %r9d
	movl	64(%rsp), %ebp
	leal	0(%ebp,%r9d,1), %ebp
	xorl	%ebp, %ecx
	rorl	$20, %ecx
	leal	0(%ebx,%ecx,1), %ebx
	xorl	%ebx, %r9d
	rorl	$24, %r9d
	leal	0(%ebp,%r9d,1), %r15d
	xorl	%r15d, %ecx
	rorl	$25, %ecx
	movl	128(%rsp), %ebp
	leal	0(%esi,%ebp,1), %esi
	movl	%esi, 192(%rsp)
	movl	56(%rsp), %esi
	movl	132(%rsp), %ebp
	leal	0(%esi,%ebp,1), %esi
	movl	%esi, 196(%rsp)
	movl	136(%rsp), %esi
	leal	0(%r11d,%esi,1), %r11d
	movl	%r11d, 200(%rsp)
	movl	140(%rsp), %r11d
	leal	0(%ebx,%r11d,1), %esi
	movl	%esi, 204(%rsp)
	movl	144(%rsp), %esi
	leal	0(%ecx,%esi,1), %r11d
	movl	%r11d, 208(%rsp)
	movl	148(%rsp), %r11d
	leal	0(%r8d,%r11d,1), %esi
	movl	%esi, 212(%rsp)
	movl	152(%rsp), %r8d
	leal	0(%eax,%r8d,1), %r8d
	movl	%r8d, 216(%rsp)
	movl	156(%rsp), %r11d
	leal	0(%edi,%r11d,1), %esi
	movl	%esi, 220(%rsp)
	movl	160(%rsp), %esi
	leal	0(%r14d,%esi,1), %r11d
	movl	%r11d, 224(%rsp)
	movl	164(%rsp), %r11d
	leal	0(%r15d,%r11d,1), %esi
	movl	%esi, 228(%rsp)
	movl	168(%rsp), %esi
	leal	0(%r13d,%esi,1), %r8d
	movl	%r8d, 232(%rsp)
	movl	172(%rsp), %r11d
	leal	0(%r10d,%r11d,1), %r10d
	movl	%r10d, 236(%rsp)
	movl	60(%rsp), %esi
	movl	176(%rsp), %r10d
	leal	0(%esi,%r10d,1), %edi
	movl	180(%rsp), %r8d
	leal	0(%r12d,%r8d,1), %r8d
	movl	%r8d, 244(%rsp)
	movl	184(%rsp), %r8d
	leal	0(%r9d,%r8d,1), %r8d
	movl	%r8d, 248(%rsp)
	movl	188(%rsp), %r8d
	leal	0(%edx,%r8d,1), %r9d
	movl	%r9d, 252(%rsp)
	movl	112(%rsp), %esi
	leal	0(%edi,%esi,1), %r8d
	movl	%r8d, 240(%rsp)
	leaq	384(%rsp), %rdi
	movq	$4, %rdx
	movq	72(%rsp), %rsi
	call	memcpy
	movl	384(%rsp), %eax
	movl	192(%rsp), %r8d
	xorl	%r8d, %eax
	movq	80(%rsp), %rdi
	movl	%eax, 384(%rsp)
	leaq	384(%rsp), %rsi
	movq	$4, %rdx
	call	memcpy
	movq	72(%rsp), %rax
	leaq	4(%rax), %rsi
	leaq	384(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	384(%rsp), %r11d
	movl	196(%rsp), %esi
	xorl	%esi, %r11d
	movq	80(%rsp), %r10
	leaq	4(%r10), %rdi
	movl	%r11d, 384(%rsp)
	leaq	384(%rsp), %rsi
	movq	$4, %rdx
	call	memcpy
	movq	72(%rsp), %rdi
	leaq	8(%rdi), %rsi
	leaq	384(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	384(%rsp), %r11d
	movl	200(%rsp), %esi
	xorl	%esi, %r11d
	movq	80(%rsp), %rsi
	leaq	8(%rsi), %rdi
	movl	%r11d, 384(%rsp)
	leaq	384(%rsp), %rsi
	movq	$4, %rdx
	call	memcpy
	movq	72(%rsp), %rdx
	leaq	12(%rdx), %rsi
	leaq	384(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	384(%rsp), %r8d
	movl	204(%rsp), %ecx
	xorl	%ecx, %r8d
	movq	80(%rsp), %rsi
	leaq	12(%rsi), %rdi
	movl	%r8d, 384(%rsp)
	leaq	384(%rsp), %rsi
	movq	$4, %rdx
	call	memcpy
	movq	72(%rsp), %r8
	leaq	16(%r8), %rsi
	leaq	384(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	384(%rsp), %edx
	movl	208(%rsp), %r11d
	xorl	%r11d, %edx
	movq	80(%rsp), %rcx
	leaq	16(%rcx), %rdi
	movl	%edx, 384(%rsp)
	leaq	384(%rsp), %rsi
	movq	$4, %rdx
	call	memcpy
	movq	72(%rsp), %rax
	leaq	20(%rax), %rsi
	leaq	384(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	384(%rsp), %r11d
	movl	212(%rsp), %r10d
	xorl	%r10d, %r11d
	movq	80(%rsp), %rdi
	leaq	20(%rdi), %rdi
	movl	%r11d, 384(%rsp)
	leaq	384(%rsp), %rsi
	movq	$4, %rdx
	call	memcpy
	movq	72(%rsp), %rcx
	leaq	24(%rcx), %rsi
	leaq	384(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	384(%rsp), %edx
	movl	216(%rsp), %r11d
	xorl	%r11d, %edx
	movq	80(%rsp), %rax
	leaq	24(%rax), %rdi
	movl	%edx, 384(%rsp)
	leaq	384(%rsp), %rsi
	movq	$4, %rdx
	call	memcpy
	movq	72(%rsp), %r9
	leaq	28(%r9), %rsi
	leaq	384(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	384(%rsp), %r8d
	movl	220(%rsp), %r11d
	xorl	%r11d, %r8d
	movq	80(%rsp), %rcx
	leaq	28(%rcx), %rdi
	movl	%r8d, 384(%rsp)
	leaq	384(%rsp), %rsi
	movq	$4, %rdx
	call	memcpy
	movq	72(%rsp), %r10
	leaq	32(%r10), %rsi
	leaq	384(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	384(%rsp), %r8d
	movl	224(%rsp), %edx
	xorl	%edx, %r8d
	movq	80(%rsp), %rdx
	leaq	32(%rdx), %rdi
	movl	%r8d, 384(%rsp)
	leaq	384(%rsp), %rsi
	movq	$4, %rdx
	call	memcpy
	movq	72(%rsp), %r8
	leaq	36(%r8), %rsi
	leaq	384(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	384(%rsp), %ecx
	movl	228(%rsp), %edi
	xorl	%edi, %ecx
	movq	80(%rsp), %r11
	leaq	36(%r11), %rdi
	movl	%ecx, 384(%rsp)
	leaq	384(%rsp), %rsi
	movq	$4, %rdx
	call	memcpy
	movq	72(%rsp), %rdx
	leaq	40(%rdx), %rsi
	leaq	384(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	384(%rsp), %ecx
	movl	232(%rsp), %r10d
	xorl	%r10d, %ecx
	movq	80(%rsp), %r9
	leaq	40(%r9), %rdi
	movl	%ecx, 384(%rsp)
	leaq	384(%rsp), %rsi
	movq	$4, %rdx
	call	memcpy
	movq	72(%rsp), %r9
	leaq	44(%r9), %rsi
	leaq	384(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	384(%rsp), %eax
	movl	236(%rsp), %ecx
	xorl	%ecx, %eax
	movq	80(%rsp), %r8
	leaq	44(%r8), %rdi
	movl	%eax, 384(%rsp)
	leaq	384(%rsp), %rsi
	movq	$4, %rdx
	call	memcpy
	movq	72(%rsp), %rsi
	leaq	48(%rsi), %rsi
	leaq	384(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	384(%rsp), %r8d
	movl	240(%rsp), %r9d
	xorl	%r9d, %r8d
	movq	80(%rsp), %r11
	leaq	48(%r11), %rdi
	movl	%r8d, 384(%rsp)
	leaq	384(%rsp), %rsi
	movq	$4, %rdx
	call	memcpy
	movq	72(%rsp), %r10
	leaq	52(%r10), %rsi
	leaq	384(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	384(%rsp), %r8d
	movl	244(%rsp), %edi
	xorl	%edi, %r8d
	movq	80(%rsp), %rax
	leaq	52(%rax), %rdi
	movl	%r8d, 384(%rsp)
	leaq	384(%rsp), %rsi
	movq	$4, %rdx
	call	memcpy
	movq	72(%rsp), %rdi
	leaq	56(%rdi), %rsi
	leaq	384(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	384(%rsp), %esi
	movl	248(%rsp), %r11d
	xorl	%r11d, %esi
	movq	80(%rsp), %rax
	leaq	56(%rax), %rdi
	movl	%esi, 384(%rsp)
	leaq	384(%rsp), %rsi
	movq	$4, %rdx
	call	memcpy
	movq	72(%rsp), %r8
	leaq	60(%r8), %rsi
	leaq	384(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	384(%rsp), %r10d
	movl	252(%rsp), %edx
	xorl	%edx, %r10d
	movq	80(%rsp), %rdi
	leaq	60(%rdi), %rdi
	movl	%r10d, 384(%rsp)
	leaq	384(%rsp), %rsi
	movq	$4, %rdx
	call	memcpy
	movl	92(%rsp), %r11d
	leal	1(%r11d), %esi
	movl	%esi, 92(%rsp)
	jmp	.L100
.L101:
	movl	116(%rsp), %r11d
	cmpl	$0, %r11d
	jbe	.L102
	movl	88(%rsp), %r11d
	sall	$6, %r11d
	movl	%r11d, %eax
	movq	104(%rsp), %rdx
	leaq	0(%rdx,%rax,1), %r8
	movq	%r8, 72(%rsp)
	movq	96(%rsp), %r10
	leaq	0(%r10,%rax,1), %rsi
	xorl	%edi, %edi
	movb	%dil, 256(%rsp)
	xorl	%r10d, %r10d
	movb	%r10b, 257(%rsp)
	xorl	%r11d, %r11d
	movb	%r11b, 258(%rsp)
	xorl	%r9d, %r9d
	movb	%r9b, 259(%rsp)
	xorl	%r9d, %r9d
	movb	%r9b, 260(%rsp)
	xorl	%edx, %edx
	movb	%dl, 261(%rsp)
	xorl	%eax, %eax
	movb	%al, 262(%rsp)
	xorl	%ecx, %ecx
	movb	%cl, 263(%rsp)
	xorl	%ecx, %ecx
	movb	%cl, 264(%rsp)
	xorl	%edi, %edi
	movb	%dil, 265(%rsp)
	xorl	%r8d, %r8d
	movb	%r8b, 266(%rsp)
	xorl	%edx, %edx
	movb	%dl, 267(%rsp)
	xorl	%eax, %eax
	movb	%al, 268(%rsp)
	xorl	%r10d, %r10d
	movb	%r10b, 269(%rsp)
	xorl	%eax, %eax
	movb	%al, 270(%rsp)
	xorl	%r10d, %r10d
	movb	%r10b, 271(%rsp)
	xorl	%eax, %eax
	movb	%al, 272(%rsp)
	xorl	%r10d, %r10d
	movb	%r10b, 273(%rsp)
	xorl	%r11d, %r11d
	movb	%r11b, 274(%rsp)
	xorl	%r9d, %r9d
	movb	%r9b, 275(%rsp)
	xorl	%r8d, %r8d
	movb	%r8b, 276(%rsp)
	xorl	%edi, %edi
	movb	%dil, 277(%rsp)
	xorl	%edx, %edx
	movb	%dl, 278(%rsp)
	xorl	%ecx, %ecx
	movb	%cl, 279(%rsp)
	xorl	%ecx, %ecx
	movb	%cl, 280(%rsp)
	xorl	%r9d, %r9d
	movb	%r9b, 281(%rsp)
	xorl	%edx, %edx
	movb	%dl, 282(%rsp)
	xorl	%r8d, %r8d
	movb	%r8b, 283(%rsp)
	xorl	%edi, %edi
	movb	%dil, 284(%rsp)
	xorl	%r9d, %r9d
	movb	%r9b, 285(%rsp)
	xorl	%ecx, %ecx
	movb	%cl, 286(%rsp)
	xorl	%r9d, %r9d
	movb	%r9b, 287(%rsp)
	xorl	%ecx, %ecx
	movb	%cl, 288(%rsp)
	xorl	%ecx, %ecx
	movb	%cl, 289(%rsp)
	xorl	%edx, %edx
	movb	%dl, 290(%rsp)
	xorl	%eax, %eax
	movb	%al, 291(%rsp)
	xorl	%r10d, %r10d
	movb	%r10b, 292(%rsp)
	xorl	%eax, %eax
	movb	%al, 293(%rsp)
	xorl	%r9d, %r9d
	movb	%r9b, 294(%rsp)
	xorl	%r11d, %r11d
	movb	%r11b, 295(%rsp)
	xorl	%r9d, %r9d
	movb	%r9b, 296(%rsp)
	xorl	%edx, %edx
	movb	%dl, 297(%rsp)
	xorl	%r11d, %r11d
	movb	%r11b, 298(%rsp)
	xorl	%eax, %eax
	movb	%al, 299(%rsp)
	xorl	%edx, %edx
	movb	%dl, 300(%rsp)
	xorl	%ecx, %ecx
	movb	%cl, 301(%rsp)
	xorl	%r8d, %r8d
	movb	%r8b, 302(%rsp)
	xorl	%edi, %edi
	movb	%dil, 303(%rsp)
	xorl	%edi, %edi
	movb	%dil, 304(%rsp)
	xorl	%r10d, %r10d
	movb	%r10b, 305(%rsp)
	xorl	%edx, %edx
	movb	%dl, 306(%rsp)
	xorl	%edx, %edx
	movb	%dl, 307(%rsp)
	xorl	%edx, %edx
	movb	%dl, 308(%rsp)
	xorl	%ecx, %ecx
	movb	%cl, 309(%rsp)
	xorl	%edi, %edi
	movb	%dil, 310(%rsp)
	xorl	%r8d, %r8d
	movb	%r8b, 311(%rsp)
	xorl	%r11d, %r11d
	movb	%r11b, 312(%rsp)
	xorl	%r10d, %r10d
	movb	%r10b, 313(%rsp)
	xorl	%r9d, %r9d
	movb	%r9b, 314(%rsp)
	xorl	%r9d, %r9d
	movb	%r9b, 315(%rsp)
	xorl	%edi, %edi
	movb	%dil, 316(%rsp)
	xorl	%ecx, %ecx
	movb	%cl, 317(%rsp)
	xorl	%edi, %edi
	movb	%dil, 318(%rsp)
	xorl	%r9d, %r9d
	movb	%r9b, 319(%rsp)
	leaq	256(%rsp), %rdi
	movl	120(%rsp), %r10d
	movl	%r10d, %edx
	call	memcpy
	xorl	%edi, %edi
	movl	%edi, 320(%rsp)
	xorl	%esi, %esi
	movl	%esi, 324(%rsp)
	xorl	%r9d, %r9d
	movl	%r9d, 328(%rsp)
	xorl	%r8d, %r8d
	movl	%r8d, 332(%rsp)
	xorl	%ecx, %ecx
	movl	%ecx, 336(%rsp)
	xorl	%r11d, %r11d
	movl	%r11d, 340(%rsp)
	xorl	%r10d, %r10d
	movl	%r10d, 344(%rsp)
	xorl	%r8d, %r8d
	movl	%r8d, 348(%rsp)
	xorl	%eax, %eax
	movl	%eax, 352(%rsp)
	xorl	%edx, %edx
	movl	%edx, 356(%rsp)
	xorl	%ecx, %ecx
	movl	%ecx, 360(%rsp)
	xorl	%edx, %edx
	movl	%edx, 364(%rsp)
	xorl	%r10d, %r10d
	movl	%r10d, 368(%rsp)
	xorl	%ecx, %ecx
	movl	%ecx, 372(%rsp)
	xorl	%ecx, %ecx
	movl	%ecx, 376(%rsp)
	xorl	%edx, %edx
	movl	%edx, 380(%rsp)
	leaq	128(%rsp), %rsi
	leaq	320(%rsp), %rdi
	movq	$64, %rdx
	call	memcpy
	movl	368(%rsp), %r10d
	movl	88(%rsp), %r9d
	leal	0(%r10d,%r9d,1), %edi
	movl	320(%rsp), %eax
	movl	336(%rsp), %r14d
	leal	0(%eax,%r14d,1), %r10d
	xorl	%r10d, %edi
	rorl	$16, %edi
	movl	352(%rsp), %r8d
	leal	0(%r8d,%edi,1), %edx
	xorl	%edx, %r14d
	rorl	$20, %r14d
	leal	0(%r10d,%r14d,1), %r11d
	xorl	%r11d, %edi
	rorl	$24, %edi
	leal	0(%edx,%edi,1), %ebp
	xorl	%ebp, %r14d
	rorl	$25, %r14d
	movl	324(%rsp), %r10d
	movl	340(%rsp), %edx
	leal	0(%r10d,%edx,1), %r9d
	movl	372(%rsp), %ebx
	xorl	%r9d, %ebx
	rorl	$16, %ebx
	movl	356(%rsp), %esi
	leal	0(%esi,%ebx,1), %ecx
	xorl	%ecx, %edx
	rorl	$20, %edx
	leal	0(%r9d,%edx,1), %eax
	xorl	%eax, %ebx
	rorl	$24, %ebx
	leal	0(%ecx,%ebx,1), %r12d
	xorl	%r12d, %edx
	rorl	$25, %edx
	movl	328(%rsp), %r8d
	movl	344(%rsp), %r10d
	leal	0(%r8d,%r10d,1), %r8d
	movl	376(%rsp), %r9d
	xorl	%r8d, %r9d
	rorl	$16, %r9d
	movl	360(%rsp), %ecx
	leal	0(%ecx,%r9d,1), %ecx
	xorl	%ecx, %r10d
	rorl	$20, %r10d
	leal	0(%r8d,%r10d,1), %esi
	movl	%esi, 56(%rsp)
	xorl	%esi, %r9d
	rorl	$24, %r9d
	leal	0(%ecx,%r9d,1), %ecx
	xorl	%ecx, %r10d
	rorl	$25, %r10d
	movl	332(%rsp), %r8d
	movl	348(%rsp), %esi
	leal	0(%r8d,%esi,1), %r8d
	movl	380(%rsp), %r15d
	xorl	%r8d, %r15d
	rorl	$16, %r15d
	movl	364(%rsp), %r13d
	leal	0(%r13d,%r15d,1), %r13d
	xorl	%r13d, %esi
	rorl	$20, %esi
	leal	0(%r8d,%esi,1), %r8d
	xorl	%r8d, %r15d
	rorl	$24, %r15d
	leal	0(%r13d,%r15d,1), %r13d
	xorl	%r13d, %esi
	rorl	$25, %esi
	leal	0(%r11d,%edx,1), %r11d
	xorl	%r11d, %r15d
	rorl	$16, %r15d
	leal	0(%ecx,%r15d,1), %ecx
	xorl	%ecx, %edx
	rorl	$20, %edx
	leal	0(%r11d,%edx,1), %r11d
	xorl	%r11d, %r15d
	rorl	$24, %r15d
	movl	%r15d, 64(%rsp)
	movl	64(%rsp), %r15d
	leal	0(%ecx,%r15d,1), %ecx
	xorl	%ecx, %edx
	rorl	$25, %edx
	leal	0(%eax,%r10d,1), %eax
	xorl	%eax, %edi
	rorl	$16, %edi
	leal	0(%r13d,%edi,1), %r13d
	xorl	%r13d, %r10d
	rorl	$20, %r10d
	leal	0(%eax,%r10d,1), %eax
	xorl	%eax, %edi
	rorl	$24, %edi
	leal	0(%r13d,%edi,1), %r13d
	movl	%r13d, 60(%rsp)
	movl	60(%rsp), %r13d
	xorl	%r13d, %r10d
	rorl	$25, %r10d
	movl	56(%rsp), %r13d
	leal	0(%r13d,%esi,1), %r15d
	movq	%rbx, %r13
	movq	%r15, %rbx
	xorl	%ebx, %r13d
	rorl	$16, %r13d
	leal	0(%ebp,%r13d,1), %ebp
	movq	%rbp, %r15
	xorl	%r15d, %esi
	rorl	$20, %esi
	leal	0(%ebx,%esi,1), %ebx
	movq	%rbx, %rbp
	xorl	%ebp, %r13d
	rorl	$24, %r13d
	movq	%r15, %rbx
	leal	0(%ebx,%r13d,1), %ebx
	xorl	%ebx, %esi
	rorl	$25, %esi
	leal	0(%r8d,%r14d,1), %r8d
	xorl	%r8d, %r9d
	rorl	$16, %r9d
	movq	%r9, %r15
	leal	0(%r12d,%r15d,1), %r12d
	movq	%r14, %r9
	xorl	%r12d, %r9d
	rorl	$20, %r9d
	leal	0(%r8d,%r9d,1), %r8d
	movq	%r15, %r14
	xorl	%r8d, %r14d
	rorl	$24, %r14d
	leal	0(%r12d,%r14d,1), %r15d
	xorl	%r15d, %r9d
	rorl	$25, %r9d
	leal	0(%r11d,%r9d,1), %r11d
	xorl	%r11d, %edi
	rorl	$16, %edi
	movq	%rbx, %r12
	movq	%rdi, %rbx
	leal	0(%r12d,%ebx,1), %edi
	xorl	%edi, %r9d
	rorl	$20, %r9d
	leal	0(%r11d,%r9d,1), %r11d
	xorl	%r11d, %ebx
	rorl	$24, %ebx
	leal	0(%edi,%ebx,1), %edi
	movq	%rdi, %r12
	movq	%r12, %rdi
	xorl	%edi, %r9d
	rorl	$25, %r9d
	leal	0(%eax,%edx,1), %eax
	movq	%r13, %rdi
	xorl	%eax, %edi
	rorl	$16, %edi
	movq	%rdi, %r13
	leal	0(%r15d,%r13d,1), %edi
	xorl	%edi, %edx
	rorl	$20, %edx
	leal	0(%eax,%edx,1), %eax
	xorl	%eax, %r13d
	rorl	$24, %r13d
	leal	0(%edi,%r13d,1), %edi
	movl	%edi, 56(%rsp)
	movl	56(%rsp), %edi
	xorl	%edi, %edx
	rorl	$25, %edx
	movq	%rbp, %rdi
	movq	%r10, %rbp
	leal	0(%edi,%ebp,1), %r10d
	movq	%r14, %rdi
	xorl	%r10d, %edi
	rorl	$16, %edi
	leal	0(%ecx,%edi,1), %ecx
	xorl	%ecx, %ebp
	rorl	$20, %ebp
	leal	0(%r10d,%ebp,1), %r10d
	xorl	%r10d, %edi
	rorl	$24, %edi
	leal	0(%ecx,%edi,1), %ecx
	movq	%rcx, %r14
	xorl	%r14d, %ebp
	rorl	$25, %ebp
	leal	0(%r8d,%esi,1), %ecx
	movl	64(%rsp), %r15d
	xorl	%ecx, %r15d
	rorl	$16, %r15d
	movl	60(%rsp), %r8d
	leal	0(%r8d,%r15d,1), %r8d
	xorl	%r8d, %esi
	rorl	$20, %esi
	leal	0(%ecx,%esi,1), %ecx
	xorl	%ecx, %r15d
	rorl	$24, %r15d
	leal	0(%r8d,%r15d,1), %r8d
	xorl	%r8d, %esi
	rorl	$25, %esi
	leal	0(%r11d,%edx,1), %r11d
	xorl	%r11d, %r15d
	rorl	$16, %r15d
	leal	0(%r14d,%r15d,1), %r14d
	xorl	%r14d, %edx
	rorl	$20, %edx
	leal	0(%r11d,%edx,1), %r11d
	xorl	%r11d, %r15d
	rorl	$24, %r15d
	movl	%r15d, 60(%rsp)
	movl	60(%rsp), %r15d
	leal	0(%r14d,%r15d,1), %r14d
	xorl	%r14d, %edx
	rorl	$25, %edx
	leal	0(%eax,%ebp,1), %eax
	movq	%rbx, %r15
	movq	%rax, %rbx
	xorl	%ebx, %r15d
	rorl	$16, %r15d
	movq	%r15, %rax
	leal	0(%r8d,%eax,1), %r8d
	xorl	%r8d, %ebp
	rorl	$20, %ebp
	leal	0(%ebx,%ebp,1), %ebx
	xorl	%ebx, %eax
	rorl	$24, %eax
	movq	%r8, %r15
	movq	%rax, %r8
	leal	0(%r15d,%r8d,1), %eax
	xorl	%eax, %ebp
	rorl	$25, %ebp
	leal	0(%r10d,%esi,1), %r10d
	movq	%r13, %r15
	movq	%r10, %r13
	xorl	%r13d, %r15d
	rorl	$16, %r15d
	movq	%r12, %r10
	movq	%r15, %r12
	leal	0(%r10d,%r12d,1), %r10d
	xorl	%r10d, %esi
	rorl	$20, %esi
	leal	0(%r13d,%esi,1), %r13d
	xorl	%r13d, %r12d
	rorl	$24, %r12d
	leal	0(%r10d,%r12d,1), %r10d
	xorl	%r10d, %esi
	rorl	$25, %esi
	leal	0(%ecx,%r9d,1), %ecx
	movq	%rcx, %r15
	xorl	%r15d, %edi
	rorl	$16, %edi
	movl	56(%rsp), %ecx
	leal	0(%ecx,%edi,1), %ecx
	xorl	%ecx, %r9d
	rorl	$20, %r9d
	leal	0(%r15d,%r9d,1), %r15d
	xorl	%r15d, %edi
	rorl	$24, %edi
	leal	0(%ecx,%edi,1), %ecx
	xorl	%ecx, %r9d
	rorl	$25, %r9d
	leal	0(%r11d,%r9d,1), %r11d
	xorl	%r11d, %r8d
	rorl	$16, %r8d
	leal	0(%r10d,%r8d,1), %r10d
	xorl	%r10d, %r9d
	rorl	$20, %r9d
	leal	0(%r11d,%r9d,1), %r11d
	xorl	%r11d, %r8d
	rorl	$24, %r8d
	leal	0(%r10d,%r8d,1), %r10d
	xorl	%r10d, %r9d
	rorl	$25, %r9d
	movl	%r9d, 56(%rsp)
	movq	%rbx, %r9
	leal	0(%r9d,%edx,1), %ebx
	movq	%r12, %r9
	xorl	%ebx, %r9d
	rorl	$16, %r9d
	leal	0(%ecx,%r9d,1), %ecx
	xorl	%ecx, %edx
	rorl	$20, %edx
	movq	%rbx, %r12
	movq	%rdx, %rbx
	leal	0(%r12d,%ebx,1), %edx
	movq	%rdx, %r12
	xorl	%r12d, %r9d
	rorl	$24, %r9d
	movq	%rcx, %rdx
	movq	%r9, %rcx
	leal	0(%edx,%ecx,1), %edx
	xorl	%edx, %ebx
	rorl	$25, %ebx
	leal	0(%r13d,%ebp,1), %r13d
	movq	%rdi, %r9
	movq	%r13, %rdi
	xorl	%edi, %r9d
	rorl	$16, %r9d
	leal	0(%r14d,%r9d,1), %r13d
	movq	%r13, %r14
	xorl	%r14d, %ebp
	rorl	$20, %ebp
	leal	0(%edi,%ebp,1), %edi
	movq	%rdi, %r13
	xorl	%r13d, %r9d
	rorl	$24, %r9d
	movq	%r14, %rdi
	movq	%r9, %r14
	leal	0(%edi,%r14d,1), %edi
	xorl	%edi, %ebp
	rorl	$25, %ebp
	movq	%r15, %r9
	leal	0(%r9d,%esi,1), %r9d
	movl	60(%rsp), %r15d
	xorl	%r9d, %r15d
	rorl	$16, %r15d
	leal	0(%eax,%r15d,1), %eax
	xorl	%eax, %esi
	rorl	$20, %esi
	leal	0(%r9d,%esi,1), %r9d
	xorl	%r9d, %r15d
	rorl	$24, %r15d
	leal	0(%eax,%r15d,1), %eax
	xorl	%eax, %esi
	rorl	$25, %esi
	leal	0(%r11d,%ebx,1), %r11d
	xorl	%r11d, %r15d
	rorl	$16, %r15d
	leal	0(%edi,%r15d,1), %edi
	xorl	%edi, %ebx
	rorl	$20, %ebx
	leal	0(%r11d,%ebx,1), %r11d
	xorl	%r11d, %r15d
	rorl	$24, %r15d
	movl	%r15d, 60(%rsp)
	movl	60(%rsp), %r15d
	leal	0(%edi,%r15d,1), %edi
	xorl	%edi, %ebx
	rorl	$25, %ebx
	leal	0(%r12d,%ebp,1), %r12d
	xorl	%r12d, %r8d
	rorl	$16, %r8d
	leal	0(%eax,%r8d,1), %eax
	xorl	%eax, %ebp
	rorl	$20, %ebp
	leal	0(%r12d,%ebp,1), %r12d
	xorl	%r12d, %r8d
	rorl	$24, %r8d
	movq	%rax, %r15
	movq	%r8, %rax
	leal	0(%r15d,%eax,1), %r8d
	xorl	%r8d, %ebp
	rorl	$25, %ebp
	leal	0(%r13d,%esi,1), %r15d
	movq	%rcx, %r13
	movq	%r15, %rcx
	xorl	%ecx, %r13d
	rorl	$16, %r13d
	leal	0(%r10d,%r13d,1), %r10d
	xorl	%r10d, %esi
	rorl	$20, %esi
	movq	%rcx, %r15
	movq	%rsi, %rcx
	leal	0(%r15d,%ecx,1), %esi
	xorl	%esi, %r13d
	rorl	$24, %r13d
	leal	0(%r10d,%r13d,1), %r15d
	xorl	%r15d, %ecx
	rorl	$25, %ecx
	movl	56(%rsp), %r10d
	leal	0(%r9d,%r10d,1), %r9d
	xorl	%r9d, %r14d
	rorl	$16, %r14d
	leal	0(%edx,%r14d,1), %edx
	xorl	%edx, %r10d
	rorl	$20, %r10d
	leal	0(%r9d,%r10d,1), %r9d
	xorl	%r9d, %r14d
	rorl	$24, %r14d
	leal	0(%edx,%r14d,1), %edx
	xorl	%edx, %r10d
	rorl	$25, %r10d
	leal	0(%r11d,%r10d,1), %r11d
	xorl	%r11d, %eax
	rorl	$16, %eax
	leal	0(%r15d,%eax,1), %r15d
	xorl	%r15d, %r10d
	rorl	$20, %r10d
	leal	0(%r11d,%r10d,1), %r11d
	xorl	%r11d, %eax
	rorl	$24, %eax
	leal	0(%r15d,%eax,1), %r15d
	movl	%r15d, 56(%rsp)
	movl	56(%rsp), %r15d
	xorl	%r15d, %r10d
	rorl	$25, %r10d
	leal	0(%r12d,%ebx,1), %r15d
	movq	%r13, %r12
	movq	%r15, %r13
	xorl	%r13d, %r12d
	rorl	$16, %r12d
	leal	0(%edx,%r12d,1), %r15d
	movq	%rbx, %rdx
	movq	%r15, %rbx
	xorl	%ebx, %edx
	rorl	$20, %edx
	leal	0(%r13d,%edx,1), %r13d
	xorl	%r13d, %r12d
	rorl	$24, %r12d
	leal	0(%ebx,%r12d,1), %ebx
	xorl	%ebx, %edx
	rorl	$25, %edx
	leal	0(%esi,%ebp,1), %r15d
	movq	%r14, %rsi
	movq	%r15, %r14
	xorl	%r14d, %esi
	rorl	$16, %esi
	leal	0(%edi,%esi,1), %edi
	movq	%rbp, %r15
	movq	%rdi, %rbp
	xorl	%ebp, %r15d
	rorl	$20, %r15d
	movq	%r14, %rdi
	movq	%r15, %r14
	leal	0(%edi,%r14d,1), %edi
	xorl	%edi, %esi
	rorl	$24, %esi
	movq	%rbp, %r15
	movq	%rsi, %rbp
	leal	0(%r15d,%ebp,1), %esi
	xorl	%esi, %r14d
	rorl	$25, %r14d
	leal	0(%r9d,%ecx,1), %r9d
	movl	60(%rsp), %r15d
	xorl	%r9d, %r15d
	rorl	$16, %r15d
	leal	0(%r8d,%r15d,1), %r8d
	xorl	%r8d, %ecx
	rorl	$20, %ecx
	leal	0(%r9d,%ecx,1), %r9d
	xorl	%r9d, %r15d
	rorl	$24, %r15d
	leal	0(%r8d,%r15d,1), %r8d
	xorl	%r8d, %ecx
	rorl	$25, %ecx
	leal	0(%r11d,%edx,1), %r11d
	xorl	%r11d, %r15d
	rorl	$16, %r15d
	leal	0(%esi,%r15d,1), %esi
	xorl	%esi, %edx
	rorl	$20, %edx
	leal	0(%r11d,%edx,1), %r11d
	xorl	%r11d, %r15d
	rorl	$24, %r15d
	movl	%r15d, 60(%rsp)
	movl	60(%rsp), %r15d
	leal	0(%esi,%r15d,1), %esi
	xorl	%esi, %edx
	rorl	$25, %edx
	leal	0(%r13d,%r14d,1), %r13d
	xorl	%r13d, %eax
	rorl	$16, %eax
	leal	0(%r8d,%eax,1), %r8d
	movq	%r14, %r15
	xorl	%r8d, %r15d
	rorl	$20, %r15d
	movq	%r13, %r14
	movq	%r15, %r13
	leal	0(%r14d,%r13d,1), %r15d
	xorl	%r15d, %eax
	rorl	$24, %eax
	movq	%r8, %r14
	movq	%rax, %r8
	leal	0(%r14d,%r8d,1), %eax
	xorl	%eax, %r13d
	rorl	$25, %r13d
	leal	0(%edi,%ecx,1), %edi
	movq	%rdi, %r14
	xorl	%r14d, %r12d
	rorl	$16, %r12d
	movl	56(%rsp), %edi
	leal	0(%edi,%r12d,1), %edi
	xorl	%edi, %ecx
	rorl	$20, %ecx
	leal	0(%r14d,%ecx,1), %r14d
	xorl	%r14d, %r12d
	rorl	$24, %r12d
	leal	0(%edi,%r12d,1), %edi
	xorl	%edi, %ecx
	rorl	$25, %ecx
	leal	0(%r9d,%r10d,1), %r9d
	xorl	%r9d, %ebp
	rorl	$16, %ebp
	leal	0(%ebx,%ebp,1), %ebx
	xorl	%ebx, %r10d
	rorl	$20, %r10d
	leal	0(%r9d,%r10d,1), %r9d
	xorl	%r9d, %ebp
	rorl	$24, %ebp
	leal	0(%ebx,%ebp,1), %ebx
	xorl	%ebx, %r10d
	rorl	$25, %r10d
	leal	0(%r11d,%r10d,1), %r11d
	xorl	%r11d, %r8d
	rorl	$16, %r8d
	leal	0(%edi,%r8d,1), %edi
	xorl	%edi, %r10d
	rorl	$20, %r10d
	leal	0(%r11d,%r10d,1), %r11d
	xorl	%r11d, %r8d
	rorl	$24, %r8d
	leal	0(%edi,%r8d,1), %edi
	xorl	%edi, %r10d
	rorl	$25, %r10d
	movl	%r10d, 56(%rsp)
	movq	%r15, %r10
	leal	0(%r10d,%edx,1), %r15d
	movq	%r12, %r10
	movq	%r15, %r12
	xorl	%r12d, %r10d
	rorl	$16, %r10d
	leal	0(%ebx,%r10d,1), %ebx
	xorl	%ebx, %edx
	rorl	$20, %edx
	leal	0(%r12d,%edx,1), %r12d
	movq	%r10, %r15
	xorl	%r12d, %r15d
	rorl	$24, %r15d
	movq	%rbx, %r10
	movq	%r15, %rbx
	leal	0(%r10d,%ebx,1), %r10d
	xorl	%r10d, %edx
	rorl	$25, %edx
	leal	0(%r14d,%r13d,1), %r15d
	movq	%rbp, %r14
	movq	%r15, %rbp
	xorl	%ebp, %r14d
	rorl	$16, %r14d
	leal	0(%esi,%r14d,1), %esi
	xorl	%esi, %r13d
	rorl	$20, %r13d
	leal	0(%ebp,%r13d,1), %ebp
	xorl	%ebp, %r14d
	rorl	$24, %r14d
	leal	0(%esi,%r14d,1), %esi
	xorl	%esi, %r13d
	rorl	$25, %r13d
	leal	0(%r9d,%ecx,1), %r9d
	movl	60(%rsp), %r15d
	xorl	%r9d, %r15d
	rorl	$16, %r15d
	leal	0(%eax,%r15d,1), %eax
	xorl	%eax, %ecx
	rorl	$20, %ecx
	leal	0(%r9d,%ecx,1), %r9d
	xorl	%r9d, %r15d
	rorl	$24, %r15d
	leal	0(%eax,%r15d,1), %eax
	xorl	%eax, %ecx
	rorl	$25, %ecx
	leal	0(%r11d,%edx,1), %r11d
	xorl	%r11d, %r15d
	rorl	$16, %r15d
	leal	0(%esi,%r15d,1), %esi
	xorl	%esi, %edx
	rorl	$20, %edx
	leal	0(%r11d,%edx,1), %r11d
	xorl	%r11d, %r15d
	rorl	$24, %r15d
	movl	%r15d, 60(%rsp)
	movl	60(%rsp), %r15d
	leal	0(%esi,%r15d,1), %esi
	xorl	%esi, %edx
	rorl	$25, %edx
	leal	0(%r12d,%r13d,1), %r15d
	movq	%r8, %r12
	movq	%r15, %r8
	xorl	%r8d, %r12d
	rorl	$16, %r12d
	leal	0(%eax,%r12d,1), %eax
	xorl	%eax, %r13d
	rorl	$20, %r13d
	leal	0(%r8d,%r13d,1), %r15d
	movq	%r12, %r8
	movq	%r15, %r12
	xorl	%r12d, %r8d
	rorl	$24, %r8d
	leal	0(%eax,%r8d,1), %eax
	xorl	%eax, %r13d
	rorl	$25, %r13d
	leal	0(%ebp,%ecx,1), %ebp
	xorl	%ebp, %ebx
	rorl	$16, %ebx
	leal	0(%edi,%ebx,1), %edi
	xorl	%edi, %ecx
	rorl	$20, %ecx
	leal	0(%ebp,%ecx,1), %r15d
	movq	%rbx, %rbp
	movq	%r15, %rbx
	xorl	%ebx, %ebp
	rorl	$24, %ebp
	leal	0(%edi,%ebp,1), %r15d
	movq	%rcx, %rdi
	xorl	%r15d, %edi
	rorl	$25, %edi
	movq	%r9, %rcx
	movl	56(%rsp), %r9d
	leal	0(%ecx,%r9d,1), %ecx
	xorl	%ecx, %r14d
	rorl	$16, %r14d
	leal	0(%r10d,%r14d,1), %r10d
	xorl	%r10d, %r9d
	rorl	$20, %r9d
	leal	0(%ecx,%r9d,1), %ecx
	xorl	%ecx, %r14d
	rorl	$24, %r14d
	leal	0(%r10d,%r14d,1), %r10d
	xorl	%r10d, %r9d
	rorl	$25, %r9d
	leal	0(%r11d,%r9d,1), %r11d
	xorl	%r11d, %r8d
	rorl	$16, %r8d
	leal	0(%r15d,%r8d,1), %r15d
	xorl	%r15d, %r9d
	rorl	$20, %r9d
	leal	0(%r11d,%r9d,1), %r11d
	xorl	%r11d, %r8d
	rorl	$24, %r8d
	leal	0(%r15d,%r8d,1), %r15d
	movl	%r15d, 56(%rsp)
	movl	56(%rsp), %r15d
	xorl	%r15d, %r9d
	rorl	$25, %r9d
	leal	0(%r12d,%edx,1), %r12d
	xorl	%r12d, %ebp
	rorl	$16, %ebp
	movq	%r10, %r15
	movq	%rbp, %r10
	leal	0(%r15d,%r10d,1), %ebp
	xorl	%ebp, %edx
	rorl	$20, %edx
	leal	0(%r12d,%edx,1), %r12d
	xorl	%r12d, %r10d
	rorl	$24, %r10d
	leal	0(%ebp,%r10d,1), %r15d
	movq	%rdx, %rbp
	movq	%r15, %rdx
	xorl	%edx, %ebp
	rorl	$25, %ebp
	leal	0(%ebx,%r13d,1), %ebx
	xorl	%ebx, %r14d
	rorl	$16, %r14d
	movq	%rsi, %r15
	movq	%r14, %rsi
	leal	0(%r15d,%esi,1), %r14d
	xorl	%r14d, %r13d
	rorl	$20, %r13d
	leal	0(%ebx,%r13d,1), %ebx
	xorl	%ebx, %esi
	rorl	$24, %esi
	leal	0(%r14d,%esi,1), %r14d
	xorl	%r14d, %r13d
	rorl	$25, %r13d
	leal	0(%ecx,%edi,1), %ecx
	movl	60(%rsp), %r15d
	xorl	%ecx, %r15d
	rorl	$16, %r15d
	leal	0(%eax,%r15d,1), %eax
	xorl	%eax, %edi
	rorl	$20, %edi
	leal	0(%ecx,%edi,1), %ecx
	xorl	%ecx, %r15d
	rorl	$24, %r15d
	leal	0(%eax,%r15d,1), %eax
	xorl	%eax, %edi
	rorl	$25, %edi
	leal	0(%r11d,%ebp,1), %r11d
	xorl	%r11d, %r15d
	rorl	$16, %r15d
	leal	0(%r14d,%r15d,1), %r14d
	xorl	%r14d, %ebp
	rorl	$20, %ebp
	leal	0(%r11d,%ebp,1), %r11d
	xorl	%r11d, %r15d
	rorl	$24, %r15d
	movl	%r15d, 60(%rsp)
	movl	60(%rsp), %r15d
	leal	0(%r14d,%r15d,1), %r14d
	movq	%r14, %r15
	xorl	%r15d, %ebp
	rorl	$25, %ebp
	leal	0(%r12d,%r13d,1), %r12d
	xorl	%r12d, %r8d
	rorl	$16, %r8d
	leal	0(%eax,%r8d,1), %eax
	xorl	%eax, %r13d
	rorl	$20, %r13d
	leal	0(%r12d,%r13d,1), %r12d
	xorl	%r12d, %r8d
	rorl	$24, %r8d
	movq	%rax, %r14
	movq	%r8, %rax
	leal	0(%r14d,%eax,1), %r8d
	xorl	%r8d, %r13d
	rorl	$25, %r13d
	leal	0(%ebx,%edi,1), %ebx
	movq	%rbx, %r14
	xorl	%r14d, %r10d
	rorl	$16, %r10d
	movl	56(%rsp), %ebx
	leal	0(%ebx,%r10d,1), %ebx
	xorl	%ebx, %edi
	rorl	$20, %edi
	leal	0(%r14d,%edi,1), %r14d
	xorl	%r14d, %r10d
	rorl	$24, %r10d
	leal	0(%ebx,%r10d,1), %ebx
	xorl	%ebx, %edi
	rorl	$25, %edi
	leal	0(%ecx,%r9d,1), %ecx
	xorl	%ecx, %esi
	rorl	$16, %esi
	leal	0(%edx,%esi,1), %edx
	xorl	%edx, %r9d
	rorl	$20, %r9d
	leal	0(%ecx,%r9d,1), %ecx
	xorl	%ecx, %esi
	rorl	$24, %esi
	leal	0(%edx,%esi,1), %edx
	xorl	%edx, %r9d
	rorl	$25, %r9d
	leal	0(%r11d,%r9d,1), %r11d
	xorl	%r11d, %eax
	rorl	$16, %eax
	leal	0(%ebx,%eax,1), %ebx
	xorl	%ebx, %r9d
	rorl	$20, %r9d
	leal	0(%r11d,%r9d,1), %r11d
	xorl	%r11d, %eax
	rorl	$24, %eax
	leal	0(%ebx,%eax,1), %ebx
	xorl	%ebx, %r9d
	rorl	$25, %r9d
	movl	%r9d, 56(%rsp)
	movq	%r12, %r9
	leal	0(%r9d,%ebp,1), %r12d
	movq	%r10, %r9
	movq	%r12, %r10
	xorl	%r10d, %r9d
	rorl	$16, %r9d
	leal	0(%edx,%r9d,1), %edx
	xorl	%edx, %ebp
	rorl	$20, %ebp
	movq	%r10, %r12
	movq	%rbp, %r10
	leal	0(%r12d,%r10d,1), %ebp
	movq	%rbp, %r12
	xorl	%r12d, %r9d
	rorl	$24, %r9d
	movq	%r9, %rbp
	leal	0(%edx,%ebp,1), %edx
	xorl	%edx, %r10d
	rorl	$25, %r10d
	movq	%r14, %r9
	leal	0(%r9d,%r13d,1), %r14d
	movq	%rsi, %r9
	movq	%r14, %rsi
	xorl	%esi, %r9d
	rorl	$16, %r9d
	leal	0(%r15d,%r9d,1), %r14d
	xorl	%r14d, %r13d
	rorl	$20, %r13d
	movq	%rsi, %r15
	movq	%r13, %rsi
	leal	0(%r15d,%esi,1), %r13d
	xorl	%r13d, %r9d
	rorl	$24, %r9d
	movq	%r9, %r15
	movq	%r15, %r9
	leal	0(%r14d,%r9d,1), %r9d
	xorl	%r9d, %esi
	rorl	$25, %esi
	leal	0(%ecx,%edi,1), %ecx
	movl	60(%rsp), %r14d
	xorl	%ecx, %r14d
	rorl	$16, %r14d
	leal	0(%r8d,%r14d,1), %r8d
	xorl	%r8d, %edi
	rorl	$20, %edi
	leal	0(%ecx,%edi,1), %ecx
	xorl	%ecx, %r14d
	rorl	$24, %r14d
	leal	0(%r8d,%r14d,1), %r8d
	xorl	%r8d, %edi
	rorl	$25, %edi
	leal	0(%r11d,%r10d,1), %r11d
	xorl	%r11d, %r14d
	rorl	$16, %r14d
	leal	0(%r9d,%r14d,1), %r9d
	xorl	%r9d, %r10d
	rorl	$20, %r10d
	leal	0(%r11d,%r10d,1), %r11d
	xorl	%r11d, %r14d
	rorl	$24, %r14d
	movl	%r14d, 60(%rsp)
	movl	60(%rsp), %r14d
	leal	0(%r9d,%r14d,1), %r9d
	xorl	%r9d, %r10d
	rorl	$25, %r10d
	leal	0(%r12d,%esi,1), %r14d
	movq	%rax, %r12
	movq	%r14, %rax
	xorl	%eax, %r12d
	rorl	$16, %r12d
	movq	%r12, %r14
	leal	0(%r8d,%r14d,1), %r8d
	movq	%rsi, %r12
	movq	%r8, %rsi
	xorl	%esi, %r12d
	rorl	$20, %r12d
	movq	%rax, %r8
	leal	0(%r8d,%r12d,1), %r8d
	movq	%r8, %rax
	xorl	%eax, %r14d
	rorl	$24, %r14d
	movq	%r14, %r8
	leal	0(%esi,%r8d,1), %esi
	xorl	%esi, %r12d
	rorl	$25, %r12d
	leal	0(%r13d,%edi,1), %r13d
	xorl	%r13d, %ebp
	rorl	$16, %ebp
	leal	0(%ebx,%ebp,1), %ebx
	xorl	%ebx, %edi
	rorl	$20, %edi
	leal	0(%r13d,%edi,1), %r13d
	movq	%r13, %r14
	xorl	%r14d, %ebp
	rorl	$24, %ebp
	leal	0(%ebx,%ebp,1), %r13d
	xorl	%r13d, %edi
	rorl	$25, %edi
	movl	56(%rsp), %ebx
	leal	0(%ecx,%ebx,1), %ecx
	xorl	%ecx, %r15d
	rorl	$16, %r15d
	leal	0(%edx,%r15d,1), %edx
	xorl	%edx, %ebx
	rorl	$20, %ebx
	leal	0(%ecx,%ebx,1), %ecx
	xorl	%ecx, %r15d
	rorl	$24, %r15d
	leal	0(%edx,%r15d,1), %edx
	xorl	%edx, %ebx
	rorl	$25, %ebx
	leal	0(%r11d,%ebx,1), %r11d
	xorl	%r11d, %r8d
	rorl	$16, %r8d
	leal	0(%r13d,%r8d,1), %r13d
	xorl	%r13d, %ebx
	rorl	$20, %ebx
	leal	0(%r11d,%ebx,1), %r11d
	xorl	%r11d, %r8d
	rorl	$24, %r8d
	leal	0(%r13d,%r8d,1), %r13d
	movl	%r13d, 56(%rsp)
	movl	56(%rsp), %r13d
	xorl	%r13d, %ebx
	rorl	$25, %ebx
	leal	0(%eax,%r10d,1), %eax
	xorl	%eax, %ebp
	rorl	$16, %ebp
	leal	0(%edx,%ebp,1), %edx
	movq	%r10, %r13
	xorl	%edx, %r13d
	rorl	$20, %r13d
	movq	%rax, %r10
	movq	%r13, %rax
	leal	0(%r10d,%eax,1), %r10d
	xorl	%r10d, %ebp
	rorl	$24, %ebp
	movq	%rbp, %r13
	leal	0(%edx,%r13d,1), %ebp
	xorl	%ebp, %eax
	rorl	$25, %eax
	leal	0(%r14d,%r12d,1), %edx
	movq	%r15, %r14
	xorl	%edx, %r14d
	rorl	$16, %r14d
	leal	0(%r9d,%r14d,1), %r15d
	movq	%r12, %r9
	movq	%r15, %r12
	xorl	%r12d, %r9d
	rorl	$20, %r9d
	leal	0(%edx,%r9d,1), %edx
	xorl	%edx, %r14d
	rorl	$24, %r14d
	movq	%r12, %r15
	movq	%r14, %r12
	leal	0(%r15d,%r12d,1), %r14d
	xorl	%r14d, %r9d
	rorl	$25, %r9d
	leal	0(%ecx,%edi,1), %ecx
	movl	60(%rsp), %r15d
	xorl	%ecx, %r15d
	rorl	$16, %r15d
	leal	0(%esi,%r15d,1), %esi
	xorl	%esi, %edi
	rorl	$20, %edi
	leal	0(%ecx,%edi,1), %ecx
	xorl	%ecx, %r15d
	rorl	$24, %r15d
	leal	0(%esi,%r15d,1), %esi
	xorl	%esi, %edi
	rorl	$25, %edi
	leal	0(%r11d,%eax,1), %r11d
	xorl	%r11d, %r15d
	rorl	$16, %r15d
	leal	0(%r14d,%r15d,1), %r14d
	xorl	%r14d, %eax
	rorl	$20, %eax
	leal	0(%r11d,%eax,1), %r11d
	xorl	%r11d, %r15d
	rorl	$24, %r15d
	movl	%r15d, 60(%rsp)
	movl	60(%rsp), %r15d
	leal	0(%r14d,%r15d,1), %r14d
	xorl	%r14d, %eax
	rorl	$25, %eax
	leal	0(%r10d,%r9d,1), %r15d
	movq	%r8, %r10
	movq	%r15, %r8
	xorl	%r8d, %r10d
	rorl	$16, %r10d
	leal	0(%esi,%r10d,1), %esi
	xorl	%esi, %r9d
	rorl	$20, %r9d
	leal	0(%r8d,%r9d,1), %r15d
	movq	%r10, %r8
	xorl	%r15d, %r8d
	rorl	$24, %r8d
	movq	%r8, %r10
	leal	0(%esi,%r10d,1), %esi
	movq	%rsi, %r8
	xorl	%r8d, %r9d
	rorl	$25, %r9d
	leal	0(%edx,%edi,1), %edx
	movq	%rdx, %r8
	xorl	%r8d, %r13d
	rorl	$16, %r13d
	movl	56(%rsp), %edx
	leal	0(%edx,%r13d,1), %edx
	xorl	%edx, %edi
	rorl	$20, %edi
	leal	0(%r8d,%edi,1), %r8d
	xorl	%r8d, %r13d
	rorl	$24, %r13d
	leal	0(%edx,%r13d,1), %edx
	xorl	%edx, %edi
	rorl	$25, %edi
	leal	0(%ecx,%ebx,1), %ecx
	xorl	%ecx, %r12d
	rorl	$16, %r12d
	leal	0(%ebp,%r12d,1), %ebp
	xorl	%ebp, %ebx
	rorl	$20, %ebx
	leal	0(%ecx,%ebx,1), %ecx
	xorl	%ecx, %r12d
	rorl	$24, %r12d
	leal	0(%ebp,%r12d,1), %ebp
	xorl	%ebp, %ebx
	rorl	$25, %ebx
	leal	0(%r11d,%ebx,1), %r11d
	xorl	%r11d, %r10d
	rorl	$16, %r10d
	leal	0(%edx,%r10d,1), %edx
	xorl	%edx, %ebx
	rorl	$20, %ebx
	leal	0(%r11d,%ebx,1), %r11d
	xorl	%r11d, %r10d
	rorl	$24, %r10d
	leal	0(%edx,%r10d,1), %edx
	xorl	%edx, %ebx
	rorl	$25, %ebx
	movl	%ebx, 64(%rsp)
	movq	%r15, %rbx
	leal	0(%ebx,%eax,1), %ebx
	xorl	%ebx, %r13d
	rorl	$16, %r13d
	leal	0(%ebp,%r13d,1), %ebp
	movq	%rbp, %r15
	xorl	%r15d, %eax
	rorl	$20, %eax
	leal	0(%ebx,%eax,1), %ebx
	movq	%rbx, %rbp
	xorl	%ebp, %r13d
	rorl	$24, %r13d
	movq	%r15, %rbx
	leal	0(%ebx,%r13d,1), %ebx
	xorl	%ebx, %eax
	rorl	$25, %eax
	leal	0(%r8d,%r9d,1), %r8d
	xorl	%r8d, %r12d
	rorl	$16, %r12d
	leal	0(%r14d,%r12d,1), %r14d
	movq	%r14, %r15
	xorl	%r15d, %r9d
	rorl	$20, %r9d
	leal	0(%r8d,%r9d,1), %r14d
	movq	%r12, %r8
	xorl	%r14d, %r8d
	rorl	$24, %r8d
	movq	%r15, %r12
	leal	0(%r12d,%r8d,1), %r12d
	xorl	%r12d, %r9d
	rorl	$25, %r9d
	leal	0(%ecx,%edi,1), %ecx
	movl	60(%rsp), %r15d
	xorl	%ecx, %r15d
	rorl	$16, %r15d
	leal	0(%esi,%r15d,1), %esi
	xorl	%esi, %edi
	rorl	$20, %edi
	leal	0(%ecx,%edi,1), %ecx
	xorl	%ecx, %r15d
	rorl	$24, %r15d
	leal	0(%esi,%r15d,1), %esi
	xorl	%esi, %edi
	rorl	$25, %edi
	leal	0(%r11d,%eax,1), %r11d
	xorl	%r11d, %r15d
	rorl	$16, %r15d
	leal	0(%r12d,%r15d,1), %r12d
	xorl	%r12d, %eax
	rorl	$20, %eax
	leal	0(%r11d,%eax,1), %r11d
	xorl	%r11d, %r15d
	rorl	$24, %r15d
	movl	%r15d, 56(%rsp)
	movl	56(%rsp), %r15d
	leal	0(%r12d,%r15d,1), %r12d
	xorl	%r12d, %eax
	rorl	$25, %eax
	movq	%r9, %r15
	leal	0(%ebp,%r15d,1), %r9d
	xorl	%r9d, %r10d
	rorl	$16, %r10d
	leal	0(%esi,%r10d,1), %esi
	xorl	%esi, %r15d
	rorl	$20, %r15d
	leal	0(%r9d,%r15d,1), %ebp
	movq	%r10, %r9
	xorl	%ebp, %r9d
	rorl	$24, %r9d
	movq	%rsi, %r10
	leal	0(%r10d,%r9d,1), %r10d
	movq	%r15, %rsi
	xorl	%r10d, %esi
	rorl	$25, %esi
	leal	0(%r14d,%edi,1), %r14d
	xorl	%r14d, %r13d
	rorl	$16, %r13d
	leal	0(%edx,%r13d,1), %edx
	xorl	%edx, %edi
	rorl	$20, %edi
	leal	0(%r14d,%edi,1), %r15d
	movq	%r13, %r14
	movq	%r15, %r13
	xorl	%r13d, %r14d
	rorl	$24, %r14d
	leal	0(%edx,%r14d,1), %r15d
	xorl	%r15d, %edi
	rorl	$25, %edi
	movq	%rcx, %rdx
	movl	64(%rsp), %ecx
	leal	0(%edx,%ecx,1), %edx
	xorl	%edx, %r8d
	rorl	$16, %r8d
	leal	0(%ebx,%r8d,1), %ebx
	xorl	%ebx, %ecx
	rorl	$20, %ecx
	leal	0(%edx,%ecx,1), %edx
	xorl	%edx, %r8d
	rorl	$24, %r8d
	leal	0(%ebx,%r8d,1), %ebx
	xorl	%ebx, %ecx
	rorl	$25, %ecx
	leal	0(%r11d,%ecx,1), %r11d
	xorl	%r11d, %r9d
	rorl	$16, %r9d
	leal	0(%r15d,%r9d,1), %r15d
	xorl	%r15d, %ecx
	rorl	$20, %ecx
	leal	0(%r11d,%ecx,1), %r11d
	xorl	%r11d, %r9d
	rorl	$24, %r9d
	leal	0(%r15d,%r9d,1), %r15d
	movl	%r15d, 64(%rsp)
	movl	64(%rsp), %r15d
	xorl	%r15d, %ecx
	rorl	$25, %ecx
	leal	0(%ebp,%eax,1), %ebp
	movq	%rbp, %r15
	xorl	%r15d, %r14d
	rorl	$16, %r14d
	leal	0(%ebx,%r14d,1), %ebx
	movq	%rbx, %rbp
	xorl	%ebp, %eax
	rorl	$20, %eax
	movq	%r15, %rbx
	leal	0(%ebx,%eax,1), %ebx
	xorl	%ebx, %r14d
	rorl	$24, %r14d
	movq	%rbp, %r15
	movq	%r14, %rbp
	leal	0(%r15d,%ebp,1), %r14d
	movl	%r14d, 68(%rsp)
	movl	68(%rsp), %r14d
	xorl	%r14d, %eax
	rorl	$25, %eax
	leal	0(%r13d,%esi,1), %r13d
	movq	%r8, %r14
	movq	%r13, %r8
	xorl	%r8d, %r14d
	rorl	$16, %r14d
	movq	%r14, %r13
	leal	0(%r12d,%r13d,1), %r12d
	xorl	%r12d, %esi
	rorl	$20, %esi
	leal	0(%r8d,%esi,1), %r8d
	xorl	%r8d, %r13d
	rorl	$24, %r13d
	movq	%r12, %r14
	movq	%r13, %r12
	leal	0(%r14d,%r12d,1), %r13d
	movq	%r13, %r14
	xorl	%r14d, %esi
	rorl	$25, %esi
	movq	%rdx, %r13
	movq	%rdi, %rdx
	leal	0(%r13d,%edx,1), %edi
	movl	56(%rsp), %r13d
	xorl	%edi, %r13d
	rorl	$16, %r13d
	leal	0(%r10d,%r13d,1), %r15d
	xorl	%r15d, %edx
	rorl	$20, %edx
	leal	0(%edi,%edx,1), %edi
	movq	%rdi, %r10
	xorl	%r10d, %r13d
	rorl	$24, %r13d
	movq	%r15, %rdi
	movq	%r13, %r15
	leal	0(%edi,%r15d,1), %r13d
	movq	%rdx, %rdi
	xorl	%r13d, %edi
	rorl	$25, %edi
	leal	0(%r11d,%eax,1), %edx
	movq	%r15, %r11
	xorl	%edx, %r11d
	rorl	$16, %r11d
	leal	0(%r14d,%r11d,1), %r14d
	xorl	%r14d, %eax
	rorl	$20, %eax
	leal	0(%edx,%eax,1), %edx
	xorl	%edx, %r11d
	rorl	$24, %r11d
	movl	%r11d, 60(%rsp)
	movl	60(%rsp), %r11d
	leal	0(%r14d,%r11d,1), %r11d
	xorl	%r11d, %eax
	rorl	$25, %eax
	movq	%rsi, %r14
	leal	0(%ebx,%r14d,1), %esi
	movq	%rsi, %rbx
	xorl	%ebx, %r9d
	rorl	$16, %r9d
	movq	%r9, %rsi
	leal	0(%r13d,%esi,1), %r9d
	xorl	%r9d, %r14d
	rorl	$20, %r14d
	movq	%rbx, %r13
	movq	%r14, %rbx
	leal	0(%r13d,%ebx,1), %r13d
	movl	%r13d, 56(%rsp)
	movl	56(%rsp), %r13d
	xorl	%r13d, %esi
	rorl	$24, %esi
	leal	0(%r9d,%esi,1), %r9d
	xorl	%r9d, %ebx
	rorl	$25, %ebx
	leal	0(%r8d,%edi,1), %r8d
	xorl	%r8d, %ebp
	rorl	$16, %ebp
	movl	64(%rsp), %r13d
	leal	0(%r13d,%ebp,1), %r13d
	xorl	%r13d, %edi
	rorl	$20, %edi
	leal	0(%r8d,%edi,1), %r8d
	xorl	%r8d, %ebp
	rorl	$24, %ebp
	leal	0(%r13d,%ebp,1), %r14d
	xorl	%r14d, %edi
	rorl	$25, %edi
	leal	0(%r10d,%ecx,1), %r10d
	xorl	%r10d, %r12d
	rorl	$16, %r12d
	movl	68(%rsp), %r13d
	leal	0(%r13d,%r12d,1), %r13d
	xorl	%r13d, %ecx
	rorl	$20, %ecx
	leal	0(%r10d,%ecx,1), %r10d
	xorl	%r10d, %r12d
	rorl	$24, %r12d
	leal	0(%r13d,%r12d,1), %r15d
	xorl	%r15d, %ecx
	rorl	$25, %ecx
	movl	128(%rsp), %r13d
	leal	0(%edx,%r13d,1), %edx
	movl	%edx, 320(%rsp)
	movl	56(%rsp), %edx
	movl	132(%rsp), %r13d
	leal	0(%edx,%r13d,1), %edx
	movl	%edx, 324(%rsp)
	movl	136(%rsp), %edx
	leal	0(%r8d,%edx,1), %edx
	movl	%edx, 328(%rsp)
	movl	140(%rsp), %edx
	leal	0(%r10d,%edx,1), %edx
	movl	%edx, 332(%rsp)
	movl	144(%rsp), %edx
	leal	0(%ecx,%edx,1), %r8d
	movl	%r8d, 336(%rsp)
	movl	148(%rsp), %ecx
	leal	0(%eax,%ecx,1), %eax
	movl	%eax, 340(%rsp)
	movl	152(%rsp), %eax
	leal	0(%ebx,%eax,1), %r8d
	movl	%r8d, 344(%rsp)
	movl	156(%rsp), %eax
	leal	0(%edi,%eax,1), %ecx
	movl	%ecx, 348(%rsp)
	movl	160(%rsp), %edx
	leal	0(%r14d,%edx,1), %ecx
	movl	%ecx, 352(%rsp)
	movl	164(%rsp), %edx
	leal	0(%r15d,%edx,1), %r10d
	movl	%r10d, 356(%rsp)
	movl	168(%rsp), %ecx
	leal	0(%r11d,%ecx,1), %r10d
	movl	%r10d, 360(%rsp)
	movl	172(%rsp), %eax
	leal	0(%r9d,%eax,1), %r10d
	movl	%r10d, 364(%rsp)
	movl	176(%rsp), %edi
	leal	0(%esi,%edi,1), %esi
	movl	180(%rsp), %r8d
	leal	0(%ebp,%r8d,1), %edi
	movl	%edi, 372(%rsp)
	movl	184(%rsp), %edi
	leal	0(%r12d,%edi,1), %ecx
	movl	%ecx, 376(%rsp)
	movl	60(%rsp), %edx
	movl	188(%rsp), %r8d
	leal	0(%edx,%r8d,1), %r8d
	movl	%r8d, 380(%rsp)
	movl	88(%rsp), %r9d
	leal	0(%esi,%r9d,1), %ecx
	movl	%ecx, 368(%rsp)
	leaq	256(%rsp), %rsi
	leaq	384(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	384(%rsp), %r8d
	movl	320(%rsp), %edx
	xorl	%edx, %r8d
	leaq	256(%rsp), %rdi
	movl	%r8d, 384(%rsp)
	leaq	384(%rsp), %rsi
	movq	$4, %rdx
	call	memcpy
	leaq	260(%rsp), %rsi
	leaq	384(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	384(%rsp), %eax
	movl	324(%rsp), %r11d
	xorl	%r11d, %eax
	leaq	260(%rsp), %rdi
	movl	%eax, 384(%rsp)
	leaq	384(%rsp), %rsi
	movq	$4, %rdx
	call	memcpy
	leaq	264(%rsp), %rsi
	leaq	384(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	384(%rsp), %edx
	movl	328(%rsp), %eax
	xorl	%eax, %edx
	leaq	264(%rsp), %rdi
	movl	%edx, 384(%rsp)
	leaq	384(%rsp), %rsi
	movq	$4, %rdx
	call	memcpy
	leaq	268(%rsp), %rsi
	leaq	384(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	384(%rsp), %esi
	movl	332(%rsp), %r11d
	xorl	%r11d, %esi
	leaq	268(%rsp), %rdi
	movl	%esi, 384(%rsp)
	leaq	384(%rsp), %rsi
	movq	$4, %rdx
	call	memcpy
	leaq	272(%rsp), %rsi
	leaq	384(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	384(%rsp), %eax
	movl	336(%rsp), %edi
	xorl	%edi, %eax
	leaq	272(%rsp), %rdi
	movl	%eax, 384(%rsp)
	leaq	384(%rsp), %rsi
	movq	$4, %rdx
	call	memcpy
	leaq	276(%rsp), %rsi
	leaq	384(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	384(%rsp), %r8d
	movl	340(%rsp), %ecx
	xorl	%ecx, %r8d
	leaq	276(%rsp), %rdi
	movl	%r8d, 384(%rsp)
	leaq	384(%rsp), %rsi
	movq	$4, %rdx
	call	memcpy
	leaq	280(%rsp), %rsi
	leaq	384(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	384(%rsp), %r8d
	movl	344(%rsp), %r11d
	xorl	%r11d, %r8d
	leaq	280(%rsp), %rdi
	movl	%r8d, 384(%rsp)
	leaq	384(%rsp), %rsi
	movq	$4, %rdx
	call	memcpy
	leaq	284(%rsp), %rsi
	leaq	384(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	384(%rsp), %r9d
	movl	348(%rsp), %eax
	xorl	%eax, %r9d
	leaq	284(%rsp), %rdi
	movl	%r9d, 384(%rsp)
	leaq	384(%rsp), %rsi
	movq	$4, %rdx
	call	memcpy
	leaq	288(%rsp), %rsi
	leaq	384(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	384(%rsp), %edx
	movl	352(%rsp), %edi
	xorl	%edi, %edx
	leaq	288(%rsp), %rdi
	movl	%edx, 384(%rsp)
	leaq	384(%rsp), %rsi
	movq	$4, %rdx
	call	memcpy
	leaq	292(%rsp), %rsi
	leaq	384(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	384(%rsp), %r8d
	movl	356(%rsp), %r11d
	xorl	%r11d, %r8d
	leaq	292(%rsp), %rdi
	movl	%r8d, 384(%rsp)
	leaq	384(%rsp), %rsi
	movq	$4, %rdx
	call	memcpy
	leaq	296(%rsp), %rsi
	leaq	384(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	384(%rsp), %r11d
	movl	360(%rsp), %eax
	xorl	%eax, %r11d
	leaq	296(%rsp), %rdi
	movl	%r11d, 384(%rsp)
	leaq	384(%rsp), %rsi
	movq	$4, %rdx
	call	memcpy
	leaq	300(%rsp), %rsi
	leaq	384(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	384(%rsp), %ecx
	movl	364(%rsp), %r8d
	xorl	%r8d, %ecx
	leaq	300(%rsp), %rdi
	movl	%ecx, 384(%rsp)
	leaq	384(%rsp), %rsi
	movq	$4, %rdx
	call	memcpy
	leaq	304(%rsp), %rsi
	leaq	384(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	384(%rsp), %r8d
	movl	368(%rsp), %edi
	xorl	%edi, %r8d
	leaq	304(%rsp), %rdi
	movl	%r8d, 384(%rsp)
	leaq	384(%rsp), %rsi
	movq	$4, %rdx
	call	memcpy
	leaq	308(%rsp), %rsi
	leaq	384(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	384(%rsp), %esi
	movl	372(%rsp), %r10d
	xorl	%r10d, %esi
	leaq	308(%rsp), %rdi
	movl	%esi, 384(%rsp)
	leaq	384(%rsp), %rsi
	movq	$4, %rdx
	call	memcpy
	leaq	312(%rsp), %rsi
	leaq	384(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	384(%rsp), %r9d
	movl	376(%rsp), %esi
	xorl	%esi, %r9d
	leaq	312(%rsp), %rdi
	movl	%r9d, 384(%rsp)
	leaq	384(%rsp), %rsi
	movq	$4, %rdx
	call	memcpy
	leaq	316(%rsp), %rsi
	leaq	384(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	384(%rsp), %ecx
	movl	380(%rsp), %esi
	xorl	%esi, %ecx
	leaq	316(%rsp), %rdi
	movl	%ecx, 384(%rsp)
	leaq	384(%rsp), %rsi
	movq	$4, %rdx
	call	memcpy
	leaq	256(%rsp), %rsi
	movl	120(%rsp), %edx
	movl	%edx, %edx
	movq	72(%rsp), %rdi
	call	memcpy
.L102:
	movq	8(%rsp), %rbx
	movq	16(%rsp), %rbp
	movq	24(%rsp), %r12
	movq	32(%rsp), %r13
	movq	40(%rsp), %r14
	movq	48(%rsp), %r15
	addq	$456, %rsp
	ret
	.cfi_endproc
	.type	Hacl_Chacha20_Vec32_chacha20_encrypt_32, @function
	.size	Hacl_Chacha20_Vec32_chacha20_encrypt_32, . - Hacl_Chacha20_Vec32_chacha20_encrypt_32
	.text
	.align	16
	.globl Hacl_Chacha20_Vec32_chacha20_decrypt_32
Hacl_Chacha20_Vec32_chacha20_decrypt_32:
	.cfi_startproc
	subq	$456, %rsp
	.cfi_adjust_cfa_offset	456
	leaq	464(%rsp), %rax
	movq	%rax, 0(%rsp)
	movq	%rbx, 8(%rsp)
	movq	%rbp, 16(%rsp)
	movq	%r12, 24(%rsp)
	movq	%r13, 32(%rsp)
	movq	%r14, 40(%rsp)
	movq	%r15, 48(%rsp)
	movq	%r9, %rbp
	movq	%r8, %r12
	movq	%rdx, 96(%rsp)
	movq	%rsi, 104(%rsp)
	movq	%rdi, %rbx
	xorl	%edx, %edx
	movl	%edx, 128(%rsp)
	xorl	%r9d, %r9d
	movl	%r9d, 132(%rsp)
	xorl	%r11d, %r11d
	movl	%r11d, 136(%rsp)
	xorl	%r9d, %r9d
	movl	%r9d, 140(%rsp)
	xorl	%r11d, %r11d
	movl	%r11d, 144(%rsp)
	xorl	%r11d, %r11d
	movl	%r11d, 148(%rsp)
	xorl	%eax, %eax
	movl	%eax, 152(%rsp)
	xorl	%r9d, %r9d
	movl	%r9d, 156(%rsp)
	xorl	%esi, %esi
	movl	%esi, 160(%rsp)
	xorl	%edx, %edx
	movl	%edx, 164(%rsp)
	xorl	%eax, %eax
	movl	%eax, 168(%rsp)
	xorl	%r8d, %r8d
	movl	%r8d, 172(%rsp)
	xorl	%edx, %edx
	movl	%edx, 176(%rsp)
	xorl	%edx, %edx
	movl	%edx, 180(%rsp)
	xorl	%edx, %edx
	movl	%edx, 184(%rsp)
	xorl	%r9d, %r9d
	movl	%r9d, 188(%rsp)
	movq	%rcx, %r13
	xorl	%r8d, %r8d
	movl	%r8d, 400(%rsp)
	xorl	%r10d, %r10d
	movl	%r10d, 404(%rsp)
	xorl	%r10d, %r10d
	movl	%r10d, 408(%rsp)
	xorl	%edi, %edi
	movl	%edi, 412(%rsp)
	xorl	%edi, %edi
	movl	%edi, 416(%rsp)
	xorl	%r9d, %r9d
	movl	%r9d, 420(%rsp)
	xorl	%edi, %edi
	movl	%edi, 424(%rsp)
	xorl	%eax, %eax
	movl	%eax, 428(%rsp)
	xorl	%ecx, %ecx
	movl	%ecx, 432(%rsp)
	xorl	%r8d, %r8d
	movl	%r8d, 436(%rsp)
	xorl	%r8d, %r8d
	movl	%r8d, 440(%rsp)
	xorl	%esi, %esi
	movl	%esi, 444(%rsp)
	movl	$1634760805, %edx
	movl	%edx, 384(%rsp)
	movl	$857760878, %eax
	movl	%eax, 388(%rsp)
	movl	$2036477234, %edi
	movl	%edi, 392(%rsp)
	movl	$1797285236, %edi
	movl	%edi, 396(%rsp)
	movq	%r13, %rsi
	leaq	448(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	448(%rsp), %eax
	movl	%eax, 400(%rsp)
	leaq	4(%r13), %rsi
	leaq	448(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	448(%rsp), %edx
	movl	%edx, 404(%rsp)
	leaq	8(%r13), %rsi
	leaq	448(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	448(%rsp), %esi
	movl	%esi, 408(%rsp)
	leaq	12(%r13), %rsi
	leaq	448(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	448(%rsp), %r11d
	movl	%r11d, 412(%rsp)
	leaq	16(%r13), %rsi
	leaq	448(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	448(%rsp), %esi
	movl	%esi, 416(%rsp)
	leaq	20(%r13), %rsi
	leaq	448(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	448(%rsp), %r8d
	movl	%r8d, 420(%rsp)
	leaq	24(%r13), %rsi
	leaq	448(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	448(%rsp), %r10d
	movl	%r10d, 424(%rsp)
	leaq	28(%r13), %rsi
	leaq	448(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	448(%rsp), %ecx
	movl	%ecx, 428(%rsp)
	movl	%ebp, 432(%rsp)
	movq	%r12, %rsi
	leaq	448(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	448(%rsp), %r9d
	movl	%r9d, 436(%rsp)
	leaq	4(%r12), %rsi
	leaq	448(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	448(%rsp), %r8d
	movl	%r8d, 440(%rsp)
	leaq	8(%r12), %rsi
	leaq	448(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	448(%rsp), %eax
	movl	%eax, 444(%rsp)
	movl	384(%rsp), %ecx
	movl	%ecx, 128(%rsp)
	movl	388(%rsp), %ecx
	movl	%ecx, 132(%rsp)
	movl	392(%rsp), %r11d
	movl	%r11d, 136(%rsp)
	movl	396(%rsp), %r11d
	movl	%r11d, 140(%rsp)
	movl	400(%rsp), %r10d
	movl	%r10d, 144(%rsp)
	movl	404(%rsp), %ecx
	movl	%ecx, 148(%rsp)
	movl	408(%rsp), %r11d
	movl	%r11d, 152(%rsp)
	movl	412(%rsp), %r9d
	movl	%r9d, 156(%rsp)
	movl	416(%rsp), %r8d
	movl	%r8d, 160(%rsp)
	movl	420(%rsp), %ecx
	movl	%ecx, 164(%rsp)
	movl	424(%rsp), %r10d
	movl	%r10d, 168(%rsp)
	movl	428(%rsp), %r11d
	movl	%r11d, 172(%rsp)
	movl	432(%rsp), %r10d
	movl	436(%rsp), %esi
	movl	%esi, 180(%rsp)
	movl	440(%rsp), %r11d
	movl	%r11d, 184(%rsp)
	movl	%eax, 188(%rsp)
	leal	0(%r10d), %ecx
	movl	%ecx, 176(%rsp)
	movq	%rbx, %rcx
	andl	$63, %ecx
	movl	%ecx, 120(%rsp)
	shrl	$6, %ebx
	movl	%ebx, 88(%rsp)
	movl	%ecx, 116(%rsp)
	xorl	%edi, %edi
	movl	%edi, 92(%rsp)
.L103:
	movl	92(%rsp), %r11d
	movl	88(%rsp), %ecx
	cmpl	%ecx, %r11d
	jae	.L104
	movl	92(%rsp), %r9d
	sall	$6, %r9d
	movl	%r9d, %r9d
	movq	104(%rsp), %r8
	leaq	0(%r8,%r9,1), %rdi
	movq	%rdi, 80(%rsp)
	movq	96(%rsp), %rcx
	leaq	0(%rcx,%r9,1), %r10
	movq	%r10, 72(%rsp)
	xorl	%r8d, %r8d
	movl	%r8d, 192(%rsp)
	xorl	%eax, %eax
	movl	%eax, 196(%rsp)
	xorl	%esi, %esi
	movl	%esi, 200(%rsp)
	xorl	%eax, %eax
	movl	%eax, 204(%rsp)
	xorl	%r10d, %r10d
	movl	%r10d, 208(%rsp)
	xorl	%ecx, %ecx
	movl	%ecx, 212(%rsp)
	xorl	%ecx, %ecx
	movl	%ecx, 216(%rsp)
	xorl	%edi, %edi
	movl	%edi, 220(%rsp)
	xorl	%r10d, %r10d
	movl	%r10d, 224(%rsp)
	xorl	%esi, %esi
	movl	%esi, 228(%rsp)
	xorl	%r9d, %r9d
	movl	%r9d, 232(%rsp)
	xorl	%r11d, %r11d
	movl	%r11d, 236(%rsp)
	xorl	%esi, %esi
	movl	%esi, 240(%rsp)
	xorl	%eax, %eax
	movl	%eax, 244(%rsp)
	xorl	%edi, %edi
	movl	%edi, 248(%rsp)
	xorl	%edi, %edi
	movl	%edi, 252(%rsp)
	movl	92(%rsp), %edi
	movq	%rdi, %rbx
	leaq	128(%rsp), %rsi
	leaq	192(%rsp), %rdi
	movq	$64, %rdx
	call	memcpy
	movq	%rbx, %r9
	movl	%r9d, 112(%rsp)
	movl	240(%rsp), %edx
	leal	0(%edx,%r9d,1), %r14d
	movl	192(%rsp), %ecx
	movl	208(%rsp), %r9d
	leal	0(%ecx,%r9d,1), %r11d
	xorl	%r11d, %r14d
	rorl	$16, %r14d
	movl	224(%rsp), %esi
	leal	0(%esi,%r14d,1), %r10d
	xorl	%r10d, %r9d
	rorl	$20, %r9d
	leal	0(%r11d,%r9d,1), %r13d
	xorl	%r13d, %r14d
	rorl	$24, %r14d
	leal	0(%r10d,%r14d,1), %r12d
	xorl	%r12d, %r9d
	rorl	$25, %r9d
	movl	196(%rsp), %r11d
	movl	212(%rsp), %ebp
	leal	0(%r11d,%ebp,1), %r8d
	movl	244(%rsp), %ebx
	xorl	%r8d, %ebx
	rorl	$16, %ebx
	movl	228(%rsp), %r10d
	leal	0(%r10d,%ebx,1), %edi
	xorl	%edi, %ebp
	rorl	$20, %ebp
	leal	0(%r8d,%ebp,1), %r11d
	xorl	%r11d, %ebx
	rorl	$24, %ebx
	leal	0(%edi,%ebx,1), %ecx
	xorl	%ecx, %ebp
	rorl	$25, %ebp
	movl	200(%rsp), %r10d
	movl	216(%rsp), %esi
	leal	0(%r10d,%esi,1), %r10d
	movl	248(%rsp), %edi
	xorl	%r10d, %edi
	rorl	$16, %edi
	movl	232(%rsp), %eax
	leal	0(%eax,%edi,1), %r8d
	xorl	%r8d, %esi
	rorl	$20, %esi
	leal	0(%r10d,%esi,1), %r10d
	movl	%r10d, 56(%rsp)
	xorl	%r10d, %edi
	rorl	$24, %edi
	leal	0(%r8d,%edi,1), %eax
	xorl	%eax, %esi
	rorl	$25, %esi
	movl	204(%rsp), %edx
	movl	220(%rsp), %r8d
	leal	0(%edx,%r8d,1), %edx
	movl	252(%rsp), %r15d
	xorl	%edx, %r15d
	rorl	$16, %r15d
	movl	236(%rsp), %r10d
	leal	0(%r10d,%r15d,1), %r10d
	xorl	%r10d, %r8d
	rorl	$20, %r8d
	leal	0(%edx,%r8d,1), %edx
	xorl	%edx, %r15d
	rorl	$24, %r15d
	leal	0(%r10d,%r15d,1), %r10d
	xorl	%r10d, %r8d
	rorl	$25, %r8d
	leal	0(%r13d,%ebp,1), %r13d
	xorl	%r13d, %r15d
	rorl	$16, %r15d
	leal	0(%eax,%r15d,1), %eax
	xorl	%eax, %ebp
	rorl	$20, %ebp
	leal	0(%r13d,%ebp,1), %r13d
	xorl	%r13d, %r15d
	rorl	$24, %r15d
	movl	%r15d, 68(%rsp)
	movl	68(%rsp), %r15d
	leal	0(%eax,%r15d,1), %eax
	movl	%eax, 64(%rsp)
	movl	64(%rsp), %eax
	xorl	%eax, %ebp
	rorl	$25, %ebp
	leal	0(%r11d,%esi,1), %r11d
	movq	%r14, %rax
	xorl	%r11d, %eax
	rorl	$16, %eax
	leal	0(%r10d,%eax,1), %r10d
	xorl	%r10d, %esi
	rorl	$20, %esi
	leal	0(%r11d,%esi,1), %r11d
	movq	%rax, %r14
	movq	%r11, %rax
	xorl	%eax, %r14d
	rorl	$24, %r14d
	movq	%r10, %r11
	movq	%r14, %r10
	leal	0(%r11d,%r10d,1), %r11d
	xorl	%r11d, %esi
	rorl	$25, %esi
	movl	56(%rsp), %r14d
	leal	0(%r14d,%r8d,1), %r15d
	movq	%rbx, %r14
	movq	%r15, %rbx
	xorl	%ebx, %r14d
	rorl	$16, %r14d
	leal	0(%r12d,%r14d,1), %r12d
	xorl	%r12d, %r8d
	rorl	$20, %r8d
	leal	0(%ebx,%r8d,1), %ebx
	movq	%r14, %r15
	xorl	%ebx, %r15d
	rorl	$24, %r15d
	movq	%r12, %r14
	movq	%r15, %r12
	leal	0(%r14d,%r12d,1), %r14d
	xorl	%r14d, %r8d
	rorl	$25, %r8d
	movq	%rdx, %r15
	movq	%r9, %rdx
	leal	0(%r15d,%edx,1), %r9d
	xorl	%r9d, %edi
	rorl	$16, %edi
	leal	0(%ecx,%edi,1), %ecx
	xorl	%ecx, %edx
	rorl	$20, %edx
	leal	0(%r9d,%edx,1), %r9d
	movq	%rdi, %r15
	xorl	%r9d, %r15d
	rorl	$24, %r15d
	movq	%rcx, %rdi
	movq	%r15, %rcx
	leal	0(%edi,%ecx,1), %edi
	xorl	%edi, %edx
	rorl	$25, %edx
	leal	0(%r13d,%edx,1), %r13d
	movq	%r10, %r15
	movq	%r13, %r10
	xorl	%r10d, %r15d
	rorl	$16, %r15d
	movq	%r15, %r13
	leal	0(%r14d,%r13d,1), %r14d
	xorl	%r14d, %edx
	rorl	$20, %edx
	movq	%r10, %r15
	movq	%rdx, %r10
	leal	0(%r15d,%r10d,1), %edx
	xorl	%edx, %r13d
	rorl	$24, %r13d
	movq	%r14, %r15
	movq	%r13, %r14
	leal	0(%r15d,%r14d,1), %r13d
	xorl	%r13d, %r10d
	rorl	$25, %r10d
	movq	%rbp, %r15
	leal	0(%eax,%r15d,1), %eax
	movq	%r12, %rbp
	xorl	%eax, %ebp
	rorl	$16, %ebp
	leal	0(%edi,%ebp,1), %edi
	xorl	%edi, %r15d
	rorl	$20, %r15d
	movq	%rax, %r12
	movq	%r15, %rax
	leal	0(%r12d,%eax,1), %r12d
	xorl	%r12d, %ebp
	rorl	$24, %ebp
	leal	0(%edi,%ebp,1), %edi
	movl	%edi, 60(%rsp)
	movl	60(%rsp), %edi
	xorl	%edi, %eax
	rorl	$25, %eax
	movq	%rbx, %rdi
	leal	0(%edi,%esi,1), %edi
	movq	%rdi, %rbx
	xorl	%ebx, %ecx
	rorl	$16, %ecx
	movl	64(%rsp), %edi
	leal	0(%edi,%ecx,1), %edi
	xorl	%edi, %esi
	rorl	$20, %esi
	leal	0(%ebx,%esi,1), %ebx
	xorl	%ebx, %ecx
	rorl	$24, %ecx
	leal	0(%edi,%ecx,1), %edi
	xorl	%edi, %esi
	rorl	$25, %esi
	leal	0(%r9d,%r8d,1), %r9d
	movl	68(%rsp), %r15d
	xorl	%r9d, %r15d
	rorl	$16, %r15d
	leal	0(%r11d,%r15d,1), %r11d
	xorl	%r11d, %r8d
	rorl	$20, %r8d
	leal	0(%r9d,%r8d,1), %r9d
	xorl	%r9d, %r15d
	rorl	$24, %r15d
	leal	0(%r11d,%r15d,1), %r11d
	xorl	%r11d, %r8d
	rorl	$25, %r8d
	leal	0(%edx,%eax,1), %edx
	xorl	%edx, %r15d
	rorl	$16, %r15d
	leal	0(%edi,%r15d,1), %edi
	xorl	%edi, %eax
	rorl	$20, %eax
	leal	0(%edx,%eax,1), %edx
	xorl	%edx, %r15d
	rorl	$24, %r15d
	movl	%r15d, 64(%rsp)
	movl	64(%rsp), %r15d
	leal	0(%edi,%r15d,1), %edi
	xorl	%edi, %eax
	rorl	$25, %eax
	leal	0(%r12d,%esi,1), %r12d
	movq	%r12, %r15
	xorl	%r15d, %r14d
	rorl	$16, %r14d
	leal	0(%r11d,%r14d,1), %r11d
	movq	%r11, %r12
	xorl	%r12d, %esi
	rorl	$20, %esi
	movq	%r15, %r11
	leal	0(%r11d,%esi,1), %r11d
	xorl	%r11d, %r14d
	rorl	$24, %r14d
	movq	%r12, %r15
	movq	%r14, %r12
	leal	0(%r15d,%r12d,1), %r14d
	movl	%r14d, 56(%rsp)
	movl	56(%rsp), %r14d
	xorl	%r14d, %esi
	rorl	$25, %esi
	leal	0(%ebx,%r8d,1), %ebx
	movq	%rbp, %r14
	movq	%rbx, %rbp
	xorl	%ebp, %r14d
	rorl	$16, %r14d
	movq	%r13, %rbx
	movq	%r14, %r13
	leal	0(%ebx,%r13d,1), %ebx
	xorl	%ebx, %r8d
	rorl	$20, %r8d
	leal	0(%ebp,%r8d,1), %ebp
	xorl	%ebp, %r13d
	rorl	$24, %r13d
	movq	%rbx, %r14
	movq	%r13, %rbx
	leal	0(%r14d,%ebx,1), %r13d
	xorl	%r13d, %r8d
	rorl	$25, %r8d
	movq	%r10, %r14
	leal	0(%r9d,%r14d,1), %r9d
	xorl	%r9d, %ecx
	rorl	$16, %ecx
	movl	60(%rsp), %r10d
	leal	0(%r10d,%ecx,1), %r10d
	xorl	%r10d, %r14d
	rorl	$20, %r14d
	leal	0(%r9d,%r14d,1), %r9d
	xorl	%r9d, %ecx
	rorl	$24, %ecx
	leal	0(%r10d,%ecx,1), %r10d
	xorl	%r10d, %r14d
	rorl	$25, %r14d
	leal	0(%edx,%r14d,1), %r15d
	movq	%r12, %rdx
	movq	%r15, %r12
	xorl	%r12d, %edx
	rorl	$16, %edx
	leal	0(%r13d,%edx,1), %r13d
	movq	%r13, %r15
	xorl	%r15d, %r14d
	rorl	$20, %r14d
	leal	0(%r12d,%r14d,1), %r12d
	movq	%r12, %r13
	xorl	%r13d, %edx
	rorl	$24, %edx
	movq	%rdx, %r12
	leal	0(%r15d,%r12d,1), %edx
	xorl	%edx, %r14d
	rorl	$25, %r14d
	movl	%r14d, 60(%rsp)
	leal	0(%r11d,%eax,1), %r11d
	xorl	%r11d, %ebx
	rorl	$16, %ebx
	leal	0(%r10d,%ebx,1), %r10d
	movq	%rax, %r14
	movq	%r10, %rax
	xorl	%eax, %r14d
	rorl	$20, %r14d
	movq	%r11, %r10
	movq	%r14, %r11
	leal	0(%r10d,%r11d,1), %r10d
	movq	%r10, %r14
	xorl	%r14d, %ebx
	rorl	$24, %ebx
	movq	%rbx, %r10
	leal	0(%eax,%r10d,1), %r10d
	xorl	%r10d, %r11d
	rorl	$25, %r11d
	movq	%rbp, %rax
	leal	0(%eax,%esi,1), %ebp
	movq	%rcx, %rax
	movq	%rbp, %rcx
	xorl	%ecx, %eax
	rorl	$16, %eax
	leal	0(%edi,%eax,1), %edi
	xorl	%edi, %esi
	rorl	$20, %esi
	leal	0(%ecx,%esi,1), %ecx
	movq	%rcx, %rbp
	xorl	%ebp, %eax
	rorl	$24, %eax
	movq	%rax, %rcx
	leal	0(%edi,%ecx,1), %edi
	xorl	%edi, %esi
	rorl	$25, %esi
	leal	0(%r9d,%r8d,1), %r9d
	movl	64(%rsp), %r15d
	xorl	%r9d, %r15d
	rorl	$16, %r15d
	movl	56(%rsp), %eax
	leal	0(%eax,%r15d,1), %eax
	xorl	%eax, %r8d
	rorl	$20, %r8d
	leal	0(%r9d,%r8d,1), %r9d
	xorl	%r9d, %r15d
	rorl	$24, %r15d
	leal	0(%eax,%r15d,1), %eax
	xorl	%eax, %r8d
	rorl	$25, %r8d
	leal	0(%r13d,%r11d,1), %r13d
	xorl	%r13d, %r15d
	rorl	$16, %r15d
	leal	0(%edi,%r15d,1), %edi
	xorl	%edi, %r11d
	rorl	$20, %r11d
	leal	0(%r13d,%r11d,1), %r13d
	xorl	%r13d, %r15d
	rorl	$24, %r15d
	movl	%r15d, 64(%rsp)
	movl	64(%rsp), %r15d
	leal	0(%edi,%r15d,1), %edi
	movl	%edi, 56(%rsp)
	movl	56(%rsp), %edi
	xorl	%edi, %r11d
	rorl	$25, %r11d
	movq	%r14, %rdi
	leal	0(%edi,%esi,1), %r14d
	movq	%r12, %rdi
	movq	%r14, %r12
	xorl	%r12d, %edi
	rorl	$16, %edi
	leal	0(%eax,%edi,1), %eax
	movq	%rax, %r14
	xorl	%r14d, %esi
	rorl	$20, %esi
	movq	%r12, %rax
	leal	0(%eax,%esi,1), %eax
	xorl	%eax, %edi
	rorl	$24, %edi
	movq	%r14, %r12
	movq	%rdi, %r14
	leal	0(%r12d,%r14d,1), %edi
	xorl	%edi, %esi
	rorl	$25, %esi
	leal	0(%ebp,%r8d,1), %ebp
	xorl	%ebp, %ebx
	rorl	$16, %ebx
	leal	0(%edx,%ebx,1), %edx
	movq	%r8, %r12
	movq	%rdx, %r8
	xorl	%r8d, %r12d
	rorl	$20, %r12d
	movq	%r12, %rdx
	leal	0(%ebp,%edx,1), %ebp
	xorl	%ebp, %ebx
	rorl	$24, %ebx
	leal	0(%r8d,%ebx,1), %r15d
	xorl	%r15d, %edx
	rorl	$25, %edx
	movl	60(%rsp), %r8d
	leal	0(%r9d,%r8d,1), %r12d
	movq	%rcx, %r9
	movq	%r12, %rcx
	xorl	%ecx, %r9d
	rorl	$16, %r9d
	movq	%r9, %r12
	leal	0(%r10d,%r12d,1), %r9d
	movq	%r9, %r10
	xorl	%r10d, %r8d
	rorl	$20, %r8d
	movq	%r8, %r9
	leal	0(%ecx,%r9d,1), %r8d
	xorl	%r8d, %r12d
	rorl	$24, %r12d
	leal	0(%r10d,%r12d,1), %ecx
	xorl	%ecx, %r9d
	rorl	$25, %r9d
	leal	0(%r13d,%r9d,1), %r10d
	movq	%r14, %r13
	xorl	%r10d, %r13d
	rorl	$16, %r13d
	movq	%r15, %r14
	leal	0(%r14d,%r13d,1), %r14d
	xorl	%r14d, %r9d
	rorl	$20, %r9d
	leal	0(%r10d,%r9d,1), %r10d
	xorl	%r10d, %r13d
	rorl	$24, %r13d
	movq	%r14, %r15
	movq	%r13, %r14
	leal	0(%r15d,%r14d,1), %r13d
	movl	%r13d, 60(%rsp)
	movl	60(%rsp), %r13d
	xorl	%r13d, %r9d
	rorl	$25, %r9d
	leal	0(%eax,%r11d,1), %eax
	xorl	%eax, %ebx
	rorl	$16, %ebx
	leal	0(%ecx,%ebx,1), %ecx
	movq	%r11, %r13
	movq	%rcx, %r11
	xorl	%r11d, %r13d
	rorl	$20, %r13d
	movq	%rax, %rcx
	movq	%r13, %rax
	leal	0(%ecx,%eax,1), %ecx
	xorl	%ecx, %ebx
	rorl	$24, %ebx
	leal	0(%r11d,%ebx,1), %r11d
	movq	%r11, %r13
	movq	%r13, %r11
	xorl	%r11d, %eax
	rorl	$25, %eax
	leal	0(%ebp,%esi,1), %r11d
	movq	%r12, %rbp
	xorl	%r11d, %ebp
	rorl	$16, %ebp
	movl	56(%rsp), %r12d
	leal	0(%r12d,%ebp,1), %r15d
	movq	%rsi, %r12
	movq	%r15, %rsi
	xorl	%esi, %r12d
	rorl	$20, %r12d
	leal	0(%r11d,%r12d,1), %r11d
	xorl	%r11d, %ebp
	rorl	$24, %ebp
	movq	%rsi, %r15
	movq	%rbp, %rsi
	leal	0(%r15d,%esi,1), %ebp
	xorl	%ebp, %r12d
	rorl	$25, %r12d
	leal	0(%r8d,%edx,1), %r8d
	movl	64(%rsp), %r15d
	xorl	%r8d, %r15d
	rorl	$16, %r15d
	leal	0(%edi,%r15d,1), %edi
	xorl	%edi, %edx
	rorl	$20, %edx
	leal	0(%r8d,%edx,1), %r8d
	xorl	%r8d, %r15d
	rorl	$24, %r15d
	leal	0(%edi,%r15d,1), %edi
	xorl	%edi, %edx
	rorl	$25, %edx
	leal	0(%r10d,%eax,1), %r10d
	xorl	%r10d, %r15d
	rorl	$16, %r15d
	leal	0(%ebp,%r15d,1), %ebp
	xorl	%ebp, %eax
	rorl	$20, %eax
	leal	0(%r10d,%eax,1), %r10d
	xorl	%r10d, %r15d
	rorl	$24, %r15d
	movl	%r15d, 64(%rsp)
	movl	64(%rsp), %r15d
	leal	0(%ebp,%r15d,1), %ebp
	xorl	%ebp, %eax
	rorl	$25, %eax
	leal	0(%ecx,%r12d,1), %ecx
	xorl	%ecx, %r14d
	rorl	$16, %r14d
	leal	0(%edi,%r14d,1), %edi
	movq	%r12, %r15
	xorl	%edi, %r15d
	rorl	$20, %r15d
	movq	%rcx, %r12
	movq	%r15, %rcx
	leal	0(%r12d,%ecx,1), %r12d
	xorl	%r12d, %r14d
	rorl	$24, %r14d
	movq	%rdi, %r15
	movq	%r14, %rdi
	leal	0(%r15d,%edi,1), %r14d
	movl	%r14d, 56(%rsp)
	movl	56(%rsp), %r14d
	xorl	%r14d, %ecx
	rorl	$25, %ecx
	movq	%r11, %r14
	movq	%rdx, %r11
	leal	0(%r14d,%r11d,1), %edx
	xorl	%edx, %ebx
	rorl	$16, %ebx
	movl	60(%rsp), %r14d
	leal	0(%r14d,%ebx,1), %r14d
	xorl	%r14d, %r11d
	rorl	$20, %r11d
	leal	0(%edx,%r11d,1), %edx
	xorl	%edx, %ebx
	rorl	$24, %ebx
	leal	0(%r14d,%ebx,1), %r14d
	xorl	%r14d, %r11d
	rorl	$25, %r11d
	leal	0(%r8d,%r9d,1), %r15d
	movq	%rsi, %r8
	movq	%r15, %rsi
	xorl	%esi, %r8d
	rorl	$16, %r8d
	leal	0(%r13d,%r8d,1), %r13d
	xorl	%r13d, %r9d
	rorl	$20, %r9d
	leal	0(%esi,%r9d,1), %esi
	xorl	%esi, %r8d
	rorl	$24, %r8d
	leal	0(%r13d,%r8d,1), %r13d
	xorl	%r13d, %r9d
	rorl	$25, %r9d
	leal	0(%r10d,%r9d,1), %r10d
	xorl	%r10d, %edi
	rorl	$16, %edi
	movq	%rdi, %r15
	leal	0(%r14d,%r15d,1), %r14d
	movq	%r9, %rdi
	movq	%r14, %r9
	xorl	%r9d, %edi
	rorl	$20, %edi
	leal	0(%r10d,%edi,1), %r10d
	movq	%r15, %r14
	xorl	%r10d, %r14d
	rorl	$24, %r14d
	leal	0(%r9d,%r14d,1), %r9d
	xorl	%r9d, %edi
	rorl	$25, %edi
	movl	%edi, 60(%rsp)
	movq	%r12, %rdi
	leal	0(%edi,%eax,1), %edi
	xorl	%edi, %ebx
	rorl	$16, %ebx
	leal	0(%r13d,%ebx,1), %r12d
	xorl	%r12d, %eax
	rorl	$20, %eax
	leal	0(%edi,%eax,1), %edi
	movq	%rdi, %r13
	xorl	%r13d, %ebx
	rorl	$24, %ebx
	movq	%r12, %rdi
	leal	0(%edi,%ebx,1), %edi
	xorl	%edi, %eax
	rorl	$25, %eax
	leal	0(%edx,%ecx,1), %edx
	xorl	%edx, %r8d
	rorl	$16, %r8d
	leal	0(%ebp,%r8d,1), %ebp
	movq	%rbp, %r12
	xorl	%r12d, %ecx
	rorl	$20, %ecx
	leal	0(%edx,%ecx,1), %ebp
	movq	%r8, %rdx
	xorl	%ebp, %edx
	rorl	$24, %edx
	movq	%r12, %r8
	leal	0(%r8d,%edx,1), %r8d
	xorl	%r8d, %ecx
	rorl	$25, %ecx
	leal	0(%esi,%r11d,1), %esi
	movl	64(%rsp), %r15d
	xorl	%esi, %r15d
	rorl	$16, %r15d
	movl	56(%rsp), %r12d
	leal	0(%r12d,%r15d,1), %r12d
	xorl	%r12d, %r11d
	rorl	$20, %r11d
	leal	0(%esi,%r11d,1), %esi
	xorl	%esi, %r15d
	rorl	$24, %r15d
	leal	0(%r12d,%r15d,1), %r12d
	xorl	%r12d, %r11d
	rorl	$25, %r11d
	leal	0(%r10d,%eax,1), %r10d
	xorl	%r10d, %r15d
	rorl	$16, %r15d
	leal	0(%r8d,%r15d,1), %r8d
	xorl	%r8d, %eax
	rorl	$20, %eax
	leal	0(%r10d,%eax,1), %r10d
	xorl	%r10d, %r15d
	rorl	$24, %r15d
	movl	%r15d, 64(%rsp)
	movl	64(%rsp), %r15d
	leal	0(%r8d,%r15d,1), %r8d
	movl	%r8d, 56(%rsp)
	movl	56(%rsp), %r8d
	xorl	%r8d, %eax
	rorl	$25, %eax
	movq	%r13, %r8
	leal	0(%r8d,%ecx,1), %r13d
	movq	%r14, %r8
	xorl	%r13d, %r8d
	rorl	$16, %r8d
	leal	0(%r12d,%r8d,1), %r12d
	xorl	%r12d, %ecx
	rorl	$20, %ecx
	leal	0(%r13d,%ecx,1), %r13d
	movq	%r8, %r14
	xorl	%r13d, %r14d
	rorl	$24, %r14d
	movq	%r12, %r8
	leal	0(%r8d,%r14d,1), %r8d
	xorl	%r8d, %ecx
	rorl	$25, %ecx
	leal	0(%ebp,%r11d,1), %ebp
	xorl	%ebp, %ebx
	rorl	$16, %ebx
	leal	0(%r9d,%ebx,1), %r9d
	xorl	%r9d, %r11d
	rorl	$20, %r11d
	leal	0(%ebp,%r11d,1), %r12d
	movq	%rbx, %rbp
	movq	%r12, %rbx
	xorl	%ebx, %ebp
	rorl	$24, %ebp
	movq	%rbp, %r12
	leal	0(%r9d,%r12d,1), %r9d
	xorl	%r9d, %r11d
	rorl	$25, %r11d
	movq	%rsi, %rbp
	movl	60(%rsp), %esi
	leal	0(%ebp,%esi,1), %ebp
	xorl	%ebp, %edx
	rorl	$16, %edx
	leal	0(%edi,%edx,1), %edi
	xorl	%edi, %esi
	rorl	$20, %esi
	leal	0(%ebp,%esi,1), %r15d
	movq	%rdx, %rbp
	movq	%r15, %rdx
	xorl	%edx, %ebp
	rorl	$24, %ebp
	leal	0(%edi,%ebp,1), %edi
	xorl	%edi, %esi
	rorl	$25, %esi
	leal	0(%r10d,%esi,1), %r10d
	xorl	%r10d, %r14d
	rorl	$16, %r14d
	movq	%r9, %r15
	movq	%r14, %r9
	leal	0(%r15d,%r9d,1), %r14d
	xorl	%r14d, %esi
	rorl	$20, %esi
	leal	0(%r10d,%esi,1), %r10d
	xorl	%r10d, %r9d
	rorl	$24, %r9d
	leal	0(%r14d,%r9d,1), %r14d
	movl	%r14d, 60(%rsp)
	movl	60(%rsp), %r14d
	xorl	%r14d, %esi
	rorl	$25, %esi
	leal	0(%r13d,%eax,1), %r13d
	xorl	%r13d, %r12d
	rorl	$16, %r12d
	leal	0(%edi,%r12d,1), %edi
	xorl	%edi, %eax
	rorl	$20, %eax
	leal	0(%r13d,%eax,1), %r13d
	xorl	%r13d, %r12d
	rorl	$24, %r12d
	movq	%rdi, %r14
	movq	%r12, %rdi
	leal	0(%r14d,%edi,1), %r12d
	xorl	%r12d, %eax
	rorl	$25, %eax
	leal	0(%ebx,%ecx,1), %ebx
	movq	%rbp, %r14
	movq	%rbx, %rbp
	xorl	%ebp, %r14d
	rorl	$16, %r14d
	movl	56(%rsp), %ebx
	leal	0(%ebx,%r14d,1), %ebx
	xorl	%ebx, %ecx
	rorl	$20, %ecx
	leal	0(%ebp,%ecx,1), %ebp
	xorl	%ebp, %r14d
	rorl	$24, %r14d
	leal	0(%ebx,%r14d,1), %ebx
	xorl	%ebx, %ecx
	rorl	$25, %ecx
	leal	0(%edx,%r11d,1), %edx
	movl	64(%rsp), %r15d
	xorl	%edx, %r15d
	rorl	$16, %r15d
	leal	0(%r8d,%r15d,1), %r8d
	xorl	%r8d, %r11d
	rorl	$20, %r11d
	leal	0(%edx,%r11d,1), %edx
	xorl	%edx, %r15d
	rorl	$24, %r15d
	leal	0(%r8d,%r15d,1), %r8d
	xorl	%r8d, %r11d
	rorl	$25, %r11d
	leal	0(%r10d,%eax,1), %r10d
	xorl	%r10d, %r15d
	rorl	$16, %r15d
	leal	0(%ebx,%r15d,1), %ebx
	xorl	%ebx, %eax
	rorl	$20, %eax
	leal	0(%r10d,%eax,1), %r10d
	xorl	%r10d, %r15d
	rorl	$24, %r15d
	movl	%r15d, 64(%rsp)
	movl	64(%rsp), %r15d
	leal	0(%ebx,%r15d,1), %ebx
	xorl	%ebx, %eax
	rorl	$25, %eax
	movq	%r13, %r15
	movq	%rcx, %r13
	leal	0(%r15d,%r13d,1), %ecx
	xorl	%ecx, %r9d
	rorl	$16, %r9d
	movq	%r8, %r15
	movq	%r9, %r8
	leal	0(%r15d,%r8d,1), %r9d
	xorl	%r9d, %r13d
	rorl	$20, %r13d
	leal	0(%ecx,%r13d,1), %ecx
	xorl	%ecx, %r8d
	rorl	$24, %r8d
	leal	0(%r9d,%r8d,1), %r15d
	movq	%r13, %r9
	movl	%r15d, 56(%rsp)
	movl	56(%rsp), %r13d
	xorl	%r13d, %r9d
	rorl	$25, %r9d
	movq	%r11, %r13
	leal	0(%ebp,%r13d,1), %r11d
	movq	%r11, %rbp
	xorl	%ebp, %edi
	rorl	$16, %edi
	movl	60(%rsp), %r11d
	leal	0(%r11d,%edi,1), %r11d
	xorl	%r11d, %r13d
	rorl	$20, %r13d
	leal	0(%ebp,%r13d,1), %ebp
	xorl	%ebp, %edi
	rorl	$24, %edi
	leal	0(%r11d,%edi,1), %r11d
	xorl	%r11d, %r13d
	rorl	$25, %r13d
	leal	0(%edx,%esi,1), %edx
	movq	%rdx, %r15
	xorl	%r15d, %r14d
	rorl	$16, %r14d
	movq	%r14, %rdx
	leal	0(%r12d,%edx,1), %r12d
	movq	%r12, %r14
	xorl	%r14d, %esi
	rorl	$20, %esi
	movq	%r15, %r12
	leal	0(%r12d,%esi,1), %r12d
	xorl	%r12d, %edx
	rorl	$24, %edx
	leal	0(%r14d,%edx,1), %r15d
	movq	%rsi, %r14
	movq	%r15, %rsi
	xorl	%esi, %r14d
	rorl	$25, %r14d
	leal	0(%r10d,%r14d,1), %r10d
	movq	%r10, %r15
	xorl	%r15d, %r8d
	rorl	$16, %r8d
	movq	%r11, %r10
	leal	0(%r10d,%r8d,1), %r10d
	movq	%r10, %r11
	xorl	%r11d, %r14d
	rorl	$20, %r14d
	movq	%r15, %r10
	leal	0(%r10d,%r14d,1), %r10d
	xorl	%r10d, %r8d
	rorl	$24, %r8d
	leal	0(%r11d,%r8d,1), %r11d
	xorl	%r11d, %r14d
	rorl	$25, %r14d
	movl	%r14d, 60(%rsp)
	leal	0(%ecx,%eax,1), %ecx
	xorl	%ecx, %edi
	rorl	$16, %edi
	leal	0(%esi,%edi,1), %esi
	movq	%rax, %r14
	movq	%rsi, %rax
	xorl	%eax, %r14d
	rorl	$20, %r14d
	movq	%rcx, %rsi
	movq	%r14, %rcx
	leal	0(%esi,%ecx,1), %esi
	movq	%rsi, %r14
	xorl	%r14d, %edi
	rorl	$24, %edi
	movq	%rdi, %rsi
	leal	0(%eax,%esi,1), %edi
	xorl	%edi, %ecx
	rorl	$25, %ecx
	movq	%rbp, %rax
	leal	0(%eax,%r9d,1), %ebp
	movq	%rdx, %rax
	movq	%rbp, %rdx
	xorl	%edx, %eax
	rorl	$16, %eax
	leal	0(%ebx,%eax,1), %ebx
	movq	%r9, %rbp
	movq	%rbx, %r9
	xorl	%r9d, %ebp
	rorl	$20, %ebp
	movq	%rbp, %rbx
	leal	0(%edx,%ebx,1), %edx
	movq	%rdx, %rbp
	xorl	%ebp, %eax
	rorl	$24, %eax
	movq	%r9, %rdx
	movq	%rax, %r9
	leal	0(%edx,%r9d,1), %edx
	xorl	%edx, %ebx
	rorl	$25, %ebx
	movq	%r12, %rax
	leal	0(%eax,%r13d,1), %r12d
	movl	64(%rsp), %r15d
	xorl	%r12d, %r15d
	rorl	$16, %r15d
	movl	56(%rsp), %eax
	leal	0(%eax,%r15d,1), %eax
	xorl	%eax, %r13d
	rorl	$20, %r13d
	leal	0(%r12d,%r13d,1), %r12d
	xorl	%r12d, %r15d
	rorl	$24, %r15d
	leal	0(%eax,%r15d,1), %eax
	xorl	%eax, %r13d
	rorl	$25, %r13d
	leal	0(%r10d,%ecx,1), %r10d
	xorl	%r10d, %r15d
	rorl	$16, %r15d
	leal	0(%edx,%r15d,1), %edx
	xorl	%edx, %ecx
	rorl	$20, %ecx
	leal	0(%r10d,%ecx,1), %r10d
	xorl	%r10d, %r15d
	rorl	$24, %r15d
	movl	%r15d, 64(%rsp)
	movl	64(%rsp), %r15d
	leal	0(%edx,%r15d,1), %edx
	movl	%edx, 56(%rsp)
	movl	56(%rsp), %edx
	xorl	%edx, %ecx
	rorl	$25, %ecx
	movq	%r14, %rdx
	leal	0(%edx,%ebx,1), %r14d
	movq	%r8, %rdx
	movq	%r14, %r8
	xorl	%r8d, %edx
	rorl	$16, %edx
	leal	0(%eax,%edx,1), %eax
	movq	%rax, %r14
	xorl	%r14d, %ebx
	rorl	$20, %ebx
	movq	%r8, %rax
	leal	0(%eax,%ebx,1), %r8d
	movq	%r8, %rax
	xorl	%eax, %edx
	rorl	$24, %edx
	movq	%r14, %r8
	leal	0(%r8d,%edx,1), %r8d
	xorl	%r8d, %ebx
	rorl	$25, %ebx
	leal	0(%ebp,%r13d,1), %r14d
	movq	%rsi, %rbp
	movq	%r14, %rsi
	xorl	%esi, %ebp
	rorl	$16, %ebp
	leal	0(%r11d,%ebp,1), %r11d
	xorl	%r11d, %r13d
	rorl	$20, %r13d
	movq	%rsi, %r14
	movq	%r13, %rsi
	leal	0(%r14d,%esi,1), %r13d
	xorl	%r13d, %ebp
	rorl	$24, %ebp
	movq	%r11, %r14
	movq	%rbp, %r11
	leal	0(%r14d,%r11d,1), %ebp
	xorl	%ebp, %esi
	rorl	$25, %esi
	movq	%r12, %r14
	movl	60(%rsp), %r12d
	leal	0(%r14d,%r12d,1), %r14d
	xorl	%r14d, %r9d
	rorl	$16, %r9d
	leal	0(%edi,%r9d,1), %edi
	movq	%r12, %r15
	movq	%rdi, %r12
	xorl	%r12d, %r15d
	rorl	$20, %r15d
	movq	%r14, %rdi
	movq	%r15, %r14
	leal	0(%edi,%r14d,1), %edi
	xorl	%edi, %r9d
	rorl	$24, %r9d
	leal	0(%r12d,%r9d,1), %r12d
	xorl	%r12d, %r14d
	rorl	$25, %r14d
	movq	%r14, %r15
	leal	0(%r10d,%r15d,1), %r10d
	movq	%rdx, %r14
	xorl	%r10d, %r14d
	rorl	$16, %r14d
	movq	%rbp, %rdx
	leal	0(%edx,%r14d,1), %ebp
	movq	%r15, %rdx
	xorl	%ebp, %edx
	rorl	$20, %edx
	leal	0(%r10d,%edx,1), %r10d
	xorl	%r10d, %r14d
	rorl	$24, %r14d
	leal	0(%ebp,%r14d,1), %ebp
	xorl	%ebp, %edx
	rorl	$25, %edx
	leal	0(%eax,%ecx,1), %eax
	movq	%r11, %r15
	movq	%rax, %r11
	xorl	%r11d, %r15d
	rorl	$16, %r15d
	movq	%r12, %rax
	leal	0(%eax,%r15d,1), %eax
	movq	%rcx, %r12
	movq	%rax, %rcx
	xorl	%ecx, %r12d
	rorl	$20, %r12d
	movq	%r11, %rax
	movq	%r12, %r11
	leal	0(%eax,%r11d,1), %eax
	movq	%r15, %r12
	xorl	%eax, %r12d
	rorl	$24, %r12d
	leal	0(%ecx,%r12d,1), %ecx
	movl	%ecx, 60(%rsp)
	movl	60(%rsp), %ecx
	xorl	%ecx, %r11d
	rorl	$25, %r11d
	leal	0(%r13d,%ebx,1), %r13d
	movq	%r9, %rcx
	movq	%r13, %r9
	xorl	%r9d, %ecx
	rorl	$16, %ecx
	movl	56(%rsp), %r13d
	leal	0(%r13d,%ecx,1), %r13d
	xorl	%r13d, %ebx
	rorl	$20, %ebx
	leal	0(%r9d,%ebx,1), %r9d
	xorl	%r9d, %ecx
	rorl	$24, %ecx
	leal	0(%r13d,%ecx,1), %r13d
	xorl	%r13d, %ebx
	rorl	$25, %ebx
	leal	0(%edi,%esi,1), %edi
	movl	64(%rsp), %r15d
	xorl	%edi, %r15d
	rorl	$16, %r15d
	leal	0(%r8d,%r15d,1), %r8d
	xorl	%r8d, %esi
	rorl	$20, %esi
	leal	0(%edi,%esi,1), %edi
	xorl	%edi, %r15d
	rorl	$24, %r15d
	leal	0(%r8d,%r15d,1), %r8d
	xorl	%r8d, %esi
	rorl	$25, %esi
	leal	0(%r10d,%r11d,1), %r10d
	xorl	%r10d, %r15d
	rorl	$16, %r15d
	leal	0(%r13d,%r15d,1), %r13d
	xorl	%r13d, %r11d
	rorl	$20, %r11d
	leal	0(%r10d,%r11d,1), %r10d
	xorl	%r10d, %r15d
	rorl	$24, %r15d
	movl	%r15d, 64(%rsp)
	movl	64(%rsp), %r15d
	leal	0(%r13d,%r15d,1), %r13d
	xorl	%r13d, %r11d
	rorl	$25, %r11d
	leal	0(%eax,%ebx,1), %r15d
	movq	%r14, %rax
	movq	%r15, %r14
	xorl	%r14d, %eax
	rorl	$16, %eax
	leal	0(%r8d,%eax,1), %r15d
	movq	%rbx, %r8
	movq	%r15, %rbx
	xorl	%ebx, %r8d
	rorl	$20, %r8d
	leal	0(%r14d,%r8d,1), %r14d
	xorl	%r14d, %eax
	rorl	$24, %eax
	movq	%rbx, %r15
	movq	%rax, %rbx
	leal	0(%r15d,%ebx,1), %eax
	xorl	%eax, %r8d
	rorl	$25, %r8d
	leal	0(%r9d,%esi,1), %r15d
	movq	%r12, %r9
	movq	%r15, %r12
	xorl	%r12d, %r9d
	rorl	$16, %r9d
	leal	0(%ebp,%r9d,1), %ebp
	xorl	%ebp, %esi
	rorl	$20, %esi
	leal	0(%r12d,%esi,1), %r12d
	xorl	%r12d, %r9d
	rorl	$24, %r9d
	movq	%r9, %r15
	leal	0(%ebp,%r15d,1), %r9d
	xorl	%r9d, %esi
	rorl	$25, %esi
	leal	0(%edi,%edx,1), %edi
	xorl	%edi, %ecx
	rorl	$16, %ecx
	movl	60(%rsp), %ebp
	leal	0(%ebp,%ecx,1), %ebp
	xorl	%ebp, %edx
	rorl	$20, %edx
	leal	0(%edi,%edx,1), %edi
	xorl	%edi, %ecx
	rorl	$24, %ecx
	leal	0(%ebp,%ecx,1), %ebp
	xorl	%ebp, %edx
	rorl	$25, %edx
	leal	0(%r10d,%edx,1), %r10d
	xorl	%r10d, %ebx
	rorl	$16, %ebx
	leal	0(%r9d,%ebx,1), %r9d
	xorl	%r9d, %edx
	rorl	$20, %edx
	leal	0(%r10d,%edx,1), %r10d
	xorl	%r10d, %ebx
	rorl	$24, %ebx
	leal	0(%r9d,%ebx,1), %r9d
	xorl	%r9d, %edx
	rorl	$25, %edx
	movl	%edx, 68(%rsp)
	movq	%r11, %rdx
	leal	0(%r14d,%edx,1), %r11d
	xorl	%r11d, %r15d
	rorl	$16, %r15d
	leal	0(%ebp,%r15d,1), %ebp
	movq	%rbp, %r14
	xorl	%r14d, %edx
	rorl	$20, %edx
	leal	0(%r11d,%edx,1), %ebp
	movq	%r15, %r11
	xorl	%ebp, %r11d
	rorl	$24, %r11d
	leal	0(%r14d,%r11d,1), %r15d
	movq	%rdx, %r14
	movl	%r15d, 56(%rsp)
	movl	56(%rsp), %edx
	xorl	%edx, %r14d
	rorl	$25, %r14d
	movq	%r12, %rdx
	leal	0(%edx,%r8d,1), %edx
	xorl	%edx, %ecx
	rorl	$16, %ecx
	leal	0(%r13d,%ecx,1), %r12d
	xorl	%r12d, %r8d
	rorl	$20, %r8d
	leal	0(%edx,%r8d,1), %edx
	movq	%rcx, %r13
	movq	%rdx, %rcx
	xorl	%ecx, %r13d
	rorl	$24, %r13d
	movq	%r12, %rdx
	leal	0(%edx,%r13d,1), %r15d
	xorl	%r15d, %r8d
	rorl	$25, %r8d
	movq	%rsi, %rdx
	leal	0(%edi,%edx,1), %esi
	movl	64(%rsp), %edi
	xorl	%esi, %edi
	rorl	$16, %edi
	leal	0(%eax,%edi,1), %eax
	xorl	%eax, %edx
	rorl	$20, %edx
	leal	0(%esi,%edx,1), %r12d
	movq	%rdi, %rsi
	movq	%r12, %rdi
	xorl	%edi, %esi
	rorl	$24, %esi
	leal	0(%eax,%esi,1), %eax
	movq	%rax, %r12
	xorl	%r12d, %edx
	rorl	$25, %edx
	movq	%r10, %rax
	movq	%r14, %r10
	leal	0(%eax,%r10d,1), %r14d
	movq	%rsi, %rax
	movq	%r14, %rsi
	xorl	%esi, %eax
	rorl	$16, %eax
	movq	%rax, %r14
	leal	0(%r15d,%r14d,1), %eax
	xorl	%eax, %r10d
	rorl	$20, %r10d
	leal	0(%esi,%r10d,1), %esi
	xorl	%esi, %r14d
	rorl	$24, %r14d
	movl	%r14d, 60(%rsp)
	movl	60(%rsp), %r14d
	leal	0(%eax,%r14d,1), %eax
	xorl	%eax, %r10d
	rorl	$25, %r10d
	leal	0(%ebp,%r8d,1), %r14d
	movq	%rbx, %rbp
	movq	%r14, %rbx
	xorl	%ebx, %ebp
	rorl	$16, %ebp
	leal	0(%r12d,%ebp,1), %r12d
	movq	%r12, %r14
	xorl	%r14d, %r8d
	rorl	$20, %r8d
	leal	0(%ebx,%r8d,1), %ebx
	movq	%rbx, %r12
	xorl	%r12d, %ebp
	rorl	$24, %ebp
	movq	%r14, %rbx
	leal	0(%ebx,%ebp,1), %ebx
	xorl	%ebx, %r8d
	rorl	$25, %r8d
	leal	0(%ecx,%edx,1), %r14d
	movq	%r11, %rcx
	movq	%r14, %r11
	xorl	%r11d, %ecx
	rorl	$16, %ecx
	leal	0(%r9d,%ecx,1), %r9d
	xorl	%r9d, %edx
	rorl	$20, %edx
	leal	0(%r11d,%edx,1), %r11d
	xorl	%r11d, %ecx
	rorl	$24, %ecx
	movq	%rcx, %r14
	leal	0(%r9d,%r14d,1), %r15d
	xorl	%r15d, %edx
	rorl	$25, %edx
	movl	68(%rsp), %ecx
	leal	0(%edi,%ecx,1), %edi
	xorl	%edi, %r13d
	rorl	$16, %r13d
	movl	56(%rsp), %r9d
	leal	0(%r9d,%r13d,1), %r9d
	xorl	%r9d, %ecx
	rorl	$20, %ecx
	leal	0(%edi,%ecx,1), %edi
	xorl	%edi, %r13d
	rorl	$24, %r13d
	leal	0(%r9d,%r13d,1), %r9d
	xorl	%r9d, %ecx
	rorl	$25, %ecx
	leal	0(%esi,%ecx,1), %esi
	xorl	%esi, %ebp
	rorl	$16, %ebp
	leal	0(%r15d,%ebp,1), %r15d
	xorl	%r15d, %ecx
	rorl	$20, %ecx
	leal	0(%esi,%ecx,1), %esi
	xorl	%esi, %ebp
	rorl	$24, %ebp
	leal	0(%r15d,%ebp,1), %r15d
	movl	%r15d, 68(%rsp)
	movl	68(%rsp), %r15d
	xorl	%r15d, %ecx
	rorl	$25, %ecx
	leal	0(%r12d,%r10d,1), %r12d
	movq	%r14, %r15
	movq	%r12, %r14
	xorl	%r14d, %r15d
	rorl	$16, %r15d
	movq	%r15, %r12
	leal	0(%r9d,%r12d,1), %r9d
	xorl	%r9d, %r10d
	rorl	$20, %r10d
	leal	0(%r14d,%r10d,1), %r14d
	xorl	%r14d, %r12d
	rorl	$24, %r12d
	leal	0(%r9d,%r12d,1), %r9d
	movl	%r9d, 64(%rsp)
	movl	64(%rsp), %r9d
	xorl	%r9d, %r10d
	rorl	$25, %r10d
	movq	%r11, %r9
	leal	0(%r9d,%r8d,1), %r11d
	movq	%r13, %r9
	xorl	%r11d, %r9d
	rorl	$16, %r9d
	leal	0(%eax,%r9d,1), %eax
	xorl	%eax, %r8d
	rorl	$20, %r8d
	leal	0(%r11d,%r8d,1), %r11d
	xorl	%r11d, %r9d
	rorl	$24, %r9d
	leal	0(%eax,%r9d,1), %r13d
	movq	%r8, %rax
	movq	%r13, %r8
	xorl	%r8d, %eax
	rorl	$25, %eax
	movq	%rdi, %r13
	movq	%rdx, %rdi
	leal	0(%r13d,%edi,1), %edx
	movl	60(%rsp), %r13d
	xorl	%edx, %r13d
	rorl	$16, %r13d
	movq	%rbx, %r15
	movq	%r13, %rbx
	leal	0(%r15d,%ebx,1), %r13d
	xorl	%r13d, %edi
	rorl	$20, %edi
	leal	0(%edx,%edi,1), %edx
	movq	%rbx, %r15
	movq	%rdx, %rbx
	xorl	%ebx, %r15d
	rorl	$24, %r15d
	movq	%r15, %rdx
	leal	0(%r13d,%edx,1), %r13d
	movq	%r13, %r15
	xorl	%r15d, %edi
	rorl	$25, %edi
	leal	0(%esi,%r10d,1), %esi
	xorl	%esi, %edx
	rorl	$16, %edx
	leal	0(%r8d,%edx,1), %r8d
	xorl	%r8d, %r10d
	rorl	$20, %r10d
	leal	0(%esi,%r10d,1), %esi
	xorl	%esi, %edx
	rorl	$24, %edx
	leal	0(%r8d,%edx,1), %r13d
	movq	%r10, %r8
	xorl	%r13d, %r8d
	rorl	$25, %r8d
	leal	0(%r14d,%eax,1), %r10d
	movq	%r10, %r14
	xorl	%r14d, %ebp
	rorl	$16, %ebp
	movq	%rbp, %r10
	leal	0(%r15d,%r10d,1), %ebp
	xorl	%ebp, %eax
	rorl	$20, %eax
	leal	0(%r14d,%eax,1), %r14d
	movl	%r14d, 56(%rsp)
	movl	56(%rsp), %r14d
	xorl	%r14d, %r10d
	rorl	$24, %r10d
	movl	%r10d, 60(%rsp)
	movl	60(%rsp), %r10d
	leal	0(%ebp,%r10d,1), %r10d
	xorl	%r10d, %eax
	rorl	$25, %eax
	leal	0(%r11d,%edi,1), %r11d
	xorl	%r11d, %r12d
	rorl	$16, %r12d
	movl	68(%rsp), %ebp
	leal	0(%ebp,%r12d,1), %ebp
	xorl	%ebp, %edi
	rorl	$20, %edi
	leal	0(%r11d,%edi,1), %r11d
	xorl	%r11d, %r12d
	rorl	$24, %r12d
	leal	0(%ebp,%r12d,1), %r14d
	xorl	%r14d, %edi
	rorl	$25, %edi
	leal	0(%ebx,%ecx,1), %ebx
	xorl	%ebx, %r9d
	rorl	$16, %r9d
	movl	64(%rsp), %ebp
	leal	0(%ebp,%r9d,1), %ebp
	xorl	%ebp, %ecx
	rorl	$20, %ecx
	leal	0(%ebx,%ecx,1), %ebx
	xorl	%ebx, %r9d
	rorl	$24, %r9d
	leal	0(%ebp,%r9d,1), %r15d
	xorl	%r15d, %ecx
	rorl	$25, %ecx
	movl	128(%rsp), %ebp
	leal	0(%esi,%ebp,1), %esi
	movl	%esi, 192(%rsp)
	movl	56(%rsp), %esi
	movl	132(%rsp), %ebp
	leal	0(%esi,%ebp,1), %esi
	movl	%esi, 196(%rsp)
	movl	136(%rsp), %esi
	leal	0(%r11d,%esi,1), %r11d
	movl	%r11d, 200(%rsp)
	movl	140(%rsp), %r11d
	leal	0(%ebx,%r11d,1), %esi
	movl	%esi, 204(%rsp)
	movl	144(%rsp), %esi
	leal	0(%ecx,%esi,1), %r11d
	movl	%r11d, 208(%rsp)
	movl	148(%rsp), %r11d
	leal	0(%r8d,%r11d,1), %esi
	movl	%esi, 212(%rsp)
	movl	152(%rsp), %r8d
	leal	0(%eax,%r8d,1), %r8d
	movl	%r8d, 216(%rsp)
	movl	156(%rsp), %r11d
	leal	0(%edi,%r11d,1), %esi
	movl	%esi, 220(%rsp)
	movl	160(%rsp), %esi
	leal	0(%r14d,%esi,1), %r11d
	movl	%r11d, 224(%rsp)
	movl	164(%rsp), %r11d
	leal	0(%r15d,%r11d,1), %esi
	movl	%esi, 228(%rsp)
	movl	168(%rsp), %esi
	leal	0(%r13d,%esi,1), %r8d
	movl	%r8d, 232(%rsp)
	movl	172(%rsp), %r11d
	leal	0(%r10d,%r11d,1), %r10d
	movl	%r10d, 236(%rsp)
	movl	60(%rsp), %esi
	movl	176(%rsp), %r10d
	leal	0(%esi,%r10d,1), %edi
	movl	180(%rsp), %r8d
	leal	0(%r12d,%r8d,1), %r8d
	movl	%r8d, 244(%rsp)
	movl	184(%rsp), %r8d
	leal	0(%r9d,%r8d,1), %r8d
	movl	%r8d, 248(%rsp)
	movl	188(%rsp), %r8d
	leal	0(%edx,%r8d,1), %r9d
	movl	%r9d, 252(%rsp)
	movl	112(%rsp), %esi
	leal	0(%edi,%esi,1), %r8d
	movl	%r8d, 240(%rsp)
	leaq	384(%rsp), %rdi
	movq	$4, %rdx
	movq	72(%rsp), %rsi
	call	memcpy
	movl	384(%rsp), %eax
	movl	192(%rsp), %r8d
	xorl	%r8d, %eax
	movq	80(%rsp), %rdi
	movl	%eax, 384(%rsp)
	leaq	384(%rsp), %rsi
	movq	$4, %rdx
	call	memcpy
	movq	72(%rsp), %rax
	leaq	4(%rax), %rsi
	leaq	384(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	384(%rsp), %r11d
	movl	196(%rsp), %esi
	xorl	%esi, %r11d
	movq	80(%rsp), %r10
	leaq	4(%r10), %rdi
	movl	%r11d, 384(%rsp)
	leaq	384(%rsp), %rsi
	movq	$4, %rdx
	call	memcpy
	movq	72(%rsp), %rdi
	leaq	8(%rdi), %rsi
	leaq	384(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	384(%rsp), %r11d
	movl	200(%rsp), %esi
	xorl	%esi, %r11d
	movq	80(%rsp), %rsi
	leaq	8(%rsi), %rdi
	movl	%r11d, 384(%rsp)
	leaq	384(%rsp), %rsi
	movq	$4, %rdx
	call	memcpy
	movq	72(%rsp), %rdx
	leaq	12(%rdx), %rsi
	leaq	384(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	384(%rsp), %r8d
	movl	204(%rsp), %ecx
	xorl	%ecx, %r8d
	movq	80(%rsp), %rsi
	leaq	12(%rsi), %rdi
	movl	%r8d, 384(%rsp)
	leaq	384(%rsp), %rsi
	movq	$4, %rdx
	call	memcpy
	movq	72(%rsp), %r8
	leaq	16(%r8), %rsi
	leaq	384(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	384(%rsp), %edx
	movl	208(%rsp), %r11d
	xorl	%r11d, %edx
	movq	80(%rsp), %rcx
	leaq	16(%rcx), %rdi
	movl	%edx, 384(%rsp)
	leaq	384(%rsp), %rsi
	movq	$4, %rdx
	call	memcpy
	movq	72(%rsp), %rax
	leaq	20(%rax), %rsi
	leaq	384(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	384(%rsp), %r11d
	movl	212(%rsp), %r10d
	xorl	%r10d, %r11d
	movq	80(%rsp), %rdi
	leaq	20(%rdi), %rdi
	movl	%r11d, 384(%rsp)
	leaq	384(%rsp), %rsi
	movq	$4, %rdx
	call	memcpy
	movq	72(%rsp), %rcx
	leaq	24(%rcx), %rsi
	leaq	384(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	384(%rsp), %edx
	movl	216(%rsp), %r11d
	xorl	%r11d, %edx
	movq	80(%rsp), %rax
	leaq	24(%rax), %rdi
	movl	%edx, 384(%rsp)
	leaq	384(%rsp), %rsi
	movq	$4, %rdx
	call	memcpy
	movq	72(%rsp), %r9
	leaq	28(%r9), %rsi
	leaq	384(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	384(%rsp), %r8d
	movl	220(%rsp), %r11d
	xorl	%r11d, %r8d
	movq	80(%rsp), %rcx
	leaq	28(%rcx), %rdi
	movl	%r8d, 384(%rsp)
	leaq	384(%rsp), %rsi
	movq	$4, %rdx
	call	memcpy
	movq	72(%rsp), %r10
	leaq	32(%r10), %rsi
	leaq	384(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	384(%rsp), %r8d
	movl	224(%rsp), %edx
	xorl	%edx, %r8d
	movq	80(%rsp), %rdx
	leaq	32(%rdx), %rdi
	movl	%r8d, 384(%rsp)
	leaq	384(%rsp), %rsi
	movq	$4, %rdx
	call	memcpy
	movq	72(%rsp), %r8
	leaq	36(%r8), %rsi
	leaq	384(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	384(%rsp), %ecx
	movl	228(%rsp), %edi
	xorl	%edi, %ecx
	movq	80(%rsp), %r11
	leaq	36(%r11), %rdi
	movl	%ecx, 384(%rsp)
	leaq	384(%rsp), %rsi
	movq	$4, %rdx
	call	memcpy
	movq	72(%rsp), %rdx
	leaq	40(%rdx), %rsi
	leaq	384(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	384(%rsp), %ecx
	movl	232(%rsp), %r10d
	xorl	%r10d, %ecx
	movq	80(%rsp), %r9
	leaq	40(%r9), %rdi
	movl	%ecx, 384(%rsp)
	leaq	384(%rsp), %rsi
	movq	$4, %rdx
	call	memcpy
	movq	72(%rsp), %r9
	leaq	44(%r9), %rsi
	leaq	384(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	384(%rsp), %eax
	movl	236(%rsp), %ecx
	xorl	%ecx, %eax
	movq	80(%rsp), %r8
	leaq	44(%r8), %rdi
	movl	%eax, 384(%rsp)
	leaq	384(%rsp), %rsi
	movq	$4, %rdx
	call	memcpy
	movq	72(%rsp), %rsi
	leaq	48(%rsi), %rsi
	leaq	384(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	384(%rsp), %r8d
	movl	240(%rsp), %r9d
	xorl	%r9d, %r8d
	movq	80(%rsp), %r11
	leaq	48(%r11), %rdi
	movl	%r8d, 384(%rsp)
	leaq	384(%rsp), %rsi
	movq	$4, %rdx
	call	memcpy
	movq	72(%rsp), %r10
	leaq	52(%r10), %rsi
	leaq	384(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	384(%rsp), %r8d
	movl	244(%rsp), %edi
	xorl	%edi, %r8d
	movq	80(%rsp), %rax
	leaq	52(%rax), %rdi
	movl	%r8d, 384(%rsp)
	leaq	384(%rsp), %rsi
	movq	$4, %rdx
	call	memcpy
	movq	72(%rsp), %rdi
	leaq	56(%rdi), %rsi
	leaq	384(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	384(%rsp), %esi
	movl	248(%rsp), %r11d
	xorl	%r11d, %esi
	movq	80(%rsp), %rax
	leaq	56(%rax), %rdi
	movl	%esi, 384(%rsp)
	leaq	384(%rsp), %rsi
	movq	$4, %rdx
	call	memcpy
	movq	72(%rsp), %r8
	leaq	60(%r8), %rsi
	leaq	384(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	384(%rsp), %r10d
	movl	252(%rsp), %edx
	xorl	%edx, %r10d
	movq	80(%rsp), %rdi
	leaq	60(%rdi), %rdi
	movl	%r10d, 384(%rsp)
	leaq	384(%rsp), %rsi
	movq	$4, %rdx
	call	memcpy
	movl	92(%rsp), %r11d
	leal	1(%r11d), %esi
	movl	%esi, 92(%rsp)
	jmp	.L103
.L104:
	movl	116(%rsp), %r11d
	cmpl	$0, %r11d
	jbe	.L105
	movl	88(%rsp), %r11d
	sall	$6, %r11d
	movl	%r11d, %eax
	movq	104(%rsp), %rdx
	leaq	0(%rdx,%rax,1), %r8
	movq	%r8, 72(%rsp)
	movq	96(%rsp), %r10
	leaq	0(%r10,%rax,1), %rsi
	xorl	%edi, %edi
	movb	%dil, 256(%rsp)
	xorl	%r10d, %r10d
	movb	%r10b, 257(%rsp)
	xorl	%r11d, %r11d
	movb	%r11b, 258(%rsp)
	xorl	%r9d, %r9d
	movb	%r9b, 259(%rsp)
	xorl	%r9d, %r9d
	movb	%r9b, 260(%rsp)
	xorl	%edx, %edx
	movb	%dl, 261(%rsp)
	xorl	%eax, %eax
	movb	%al, 262(%rsp)
	xorl	%ecx, %ecx
	movb	%cl, 263(%rsp)
	xorl	%ecx, %ecx
	movb	%cl, 264(%rsp)
	xorl	%edi, %edi
	movb	%dil, 265(%rsp)
	xorl	%r8d, %r8d
	movb	%r8b, 266(%rsp)
	xorl	%edx, %edx
	movb	%dl, 267(%rsp)
	xorl	%eax, %eax
	movb	%al, 268(%rsp)
	xorl	%r10d, %r10d
	movb	%r10b, 269(%rsp)
	xorl	%eax, %eax
	movb	%al, 270(%rsp)
	xorl	%r10d, %r10d
	movb	%r10b, 271(%rsp)
	xorl	%eax, %eax
	movb	%al, 272(%rsp)
	xorl	%r10d, %r10d
	movb	%r10b, 273(%rsp)
	xorl	%r11d, %r11d
	movb	%r11b, 274(%rsp)
	xorl	%r9d, %r9d
	movb	%r9b, 275(%rsp)
	xorl	%r8d, %r8d
	movb	%r8b, 276(%rsp)
	xorl	%edi, %edi
	movb	%dil, 277(%rsp)
	xorl	%edx, %edx
	movb	%dl, 278(%rsp)
	xorl	%ecx, %ecx
	movb	%cl, 279(%rsp)
	xorl	%ecx, %ecx
	movb	%cl, 280(%rsp)
	xorl	%r9d, %r9d
	movb	%r9b, 281(%rsp)
	xorl	%edx, %edx
	movb	%dl, 282(%rsp)
	xorl	%r8d, %r8d
	movb	%r8b, 283(%rsp)
	xorl	%edi, %edi
	movb	%dil, 284(%rsp)
	xorl	%r9d, %r9d
	movb	%r9b, 285(%rsp)
	xorl	%ecx, %ecx
	movb	%cl, 286(%rsp)
	xorl	%r9d, %r9d
	movb	%r9b, 287(%rsp)
	xorl	%ecx, %ecx
	movb	%cl, 288(%rsp)
	xorl	%ecx, %ecx
	movb	%cl, 289(%rsp)
	xorl	%edx, %edx
	movb	%dl, 290(%rsp)
	xorl	%eax, %eax
	movb	%al, 291(%rsp)
	xorl	%r10d, %r10d
	movb	%r10b, 292(%rsp)
	xorl	%eax, %eax
	movb	%al, 293(%rsp)
	xorl	%r9d, %r9d
	movb	%r9b, 294(%rsp)
	xorl	%r11d, %r11d
	movb	%r11b, 295(%rsp)
	xorl	%r9d, %r9d
	movb	%r9b, 296(%rsp)
	xorl	%edx, %edx
	movb	%dl, 297(%rsp)
	xorl	%r11d, %r11d
	movb	%r11b, 298(%rsp)
	xorl	%eax, %eax
	movb	%al, 299(%rsp)
	xorl	%edx, %edx
	movb	%dl, 300(%rsp)
	xorl	%ecx, %ecx
	movb	%cl, 301(%rsp)
	xorl	%r8d, %r8d
	movb	%r8b, 302(%rsp)
	xorl	%edi, %edi
	movb	%dil, 303(%rsp)
	xorl	%edi, %edi
	movb	%dil, 304(%rsp)
	xorl	%r10d, %r10d
	movb	%r10b, 305(%rsp)
	xorl	%edx, %edx
	movb	%dl, 306(%rsp)
	xorl	%edx, %edx
	movb	%dl, 307(%rsp)
	xorl	%edx, %edx
	movb	%dl, 308(%rsp)
	xorl	%ecx, %ecx
	movb	%cl, 309(%rsp)
	xorl	%edi, %edi
	movb	%dil, 310(%rsp)
	xorl	%r8d, %r8d
	movb	%r8b, 311(%rsp)
	xorl	%r11d, %r11d
	movb	%r11b, 312(%rsp)
	xorl	%r10d, %r10d
	movb	%r10b, 313(%rsp)
	xorl	%r9d, %r9d
	movb	%r9b, 314(%rsp)
	xorl	%r9d, %r9d
	movb	%r9b, 315(%rsp)
	xorl	%edi, %edi
	movb	%dil, 316(%rsp)
	xorl	%ecx, %ecx
	movb	%cl, 317(%rsp)
	xorl	%edi, %edi
	movb	%dil, 318(%rsp)
	xorl	%r9d, %r9d
	movb	%r9b, 319(%rsp)
	leaq	256(%rsp), %rdi
	movl	120(%rsp), %r10d
	movl	%r10d, %edx
	call	memcpy
	xorl	%edi, %edi
	movl	%edi, 320(%rsp)
	xorl	%esi, %esi
	movl	%esi, 324(%rsp)
	xorl	%r9d, %r9d
	movl	%r9d, 328(%rsp)
	xorl	%r8d, %r8d
	movl	%r8d, 332(%rsp)
	xorl	%ecx, %ecx
	movl	%ecx, 336(%rsp)
	xorl	%r11d, %r11d
	movl	%r11d, 340(%rsp)
	xorl	%r10d, %r10d
	movl	%r10d, 344(%rsp)
	xorl	%r8d, %r8d
	movl	%r8d, 348(%rsp)
	xorl	%eax, %eax
	movl	%eax, 352(%rsp)
	xorl	%edx, %edx
	movl	%edx, 356(%rsp)
	xorl	%ecx, %ecx
	movl	%ecx, 360(%rsp)
	xorl	%edx, %edx
	movl	%edx, 364(%rsp)
	xorl	%r10d, %r10d
	movl	%r10d, 368(%rsp)
	xorl	%ecx, %ecx
	movl	%ecx, 372(%rsp)
	xorl	%ecx, %ecx
	movl	%ecx, 376(%rsp)
	xorl	%edx, %edx
	movl	%edx, 380(%rsp)
	leaq	128(%rsp), %rsi
	leaq	320(%rsp), %rdi
	movq	$64, %rdx
	call	memcpy
	movl	368(%rsp), %r10d
	movl	88(%rsp), %r9d
	leal	0(%r10d,%r9d,1), %edi
	movl	320(%rsp), %eax
	movl	336(%rsp), %r14d
	leal	0(%eax,%r14d,1), %r10d
	xorl	%r10d, %edi
	rorl	$16, %edi
	movl	352(%rsp), %r8d
	leal	0(%r8d,%edi,1), %edx
	xorl	%edx, %r14d
	rorl	$20, %r14d
	leal	0(%r10d,%r14d,1), %r11d
	xorl	%r11d, %edi
	rorl	$24, %edi
	leal	0(%edx,%edi,1), %ebp
	xorl	%ebp, %r14d
	rorl	$25, %r14d
	movl	324(%rsp), %r10d
	movl	340(%rsp), %edx
	leal	0(%r10d,%edx,1), %r9d
	movl	372(%rsp), %ebx
	xorl	%r9d, %ebx
	rorl	$16, %ebx
	movl	356(%rsp), %esi
	leal	0(%esi,%ebx,1), %ecx
	xorl	%ecx, %edx
	rorl	$20, %edx
	leal	0(%r9d,%edx,1), %eax
	xorl	%eax, %ebx
	rorl	$24, %ebx
	leal	0(%ecx,%ebx,1), %r12d
	xorl	%r12d, %edx
	rorl	$25, %edx
	movl	328(%rsp), %r8d
	movl	344(%rsp), %r10d
	leal	0(%r8d,%r10d,1), %r8d
	movl	376(%rsp), %r9d
	xorl	%r8d, %r9d
	rorl	$16, %r9d
	movl	360(%rsp), %ecx
	leal	0(%ecx,%r9d,1), %ecx
	xorl	%ecx, %r10d
	rorl	$20, %r10d
	leal	0(%r8d,%r10d,1), %esi
	movl	%esi, 56(%rsp)
	xorl	%esi, %r9d
	rorl	$24, %r9d
	leal	0(%ecx,%r9d,1), %ecx
	xorl	%ecx, %r10d
	rorl	$25, %r10d
	movl	332(%rsp), %r8d
	movl	348(%rsp), %esi
	leal	0(%r8d,%esi,1), %r8d
	movl	380(%rsp), %r15d
	xorl	%r8d, %r15d
	rorl	$16, %r15d
	movl	364(%rsp), %r13d
	leal	0(%r13d,%r15d,1), %r13d
	xorl	%r13d, %esi
	rorl	$20, %esi
	leal	0(%r8d,%esi,1), %r8d
	xorl	%r8d, %r15d
	rorl	$24, %r15d
	leal	0(%r13d,%r15d,1), %r13d
	xorl	%r13d, %esi
	rorl	$25, %esi
	leal	0(%r11d,%edx,1), %r11d
	xorl	%r11d, %r15d
	rorl	$16, %r15d
	leal	0(%ecx,%r15d,1), %ecx
	xorl	%ecx, %edx
	rorl	$20, %edx
	leal	0(%r11d,%edx,1), %r11d
	xorl	%r11d, %r15d
	rorl	$24, %r15d
	movl	%r15d, 64(%rsp)
	movl	64(%rsp), %r15d
	leal	0(%ecx,%r15d,1), %ecx
	xorl	%ecx, %edx
	rorl	$25, %edx
	leal	0(%eax,%r10d,1), %eax
	xorl	%eax, %edi
	rorl	$16, %edi
	leal	0(%r13d,%edi,1), %r13d
	xorl	%r13d, %r10d
	rorl	$20, %r10d
	leal	0(%eax,%r10d,1), %eax
	xorl	%eax, %edi
	rorl	$24, %edi
	leal	0(%r13d,%edi,1), %r13d
	movl	%r13d, 60(%rsp)
	movl	60(%rsp), %r13d
	xorl	%r13d, %r10d
	rorl	$25, %r10d
	movl	56(%rsp), %r13d
	leal	0(%r13d,%esi,1), %r15d
	movq	%rbx, %r13
	movq	%r15, %rbx
	xorl	%ebx, %r13d
	rorl	$16, %r13d
	leal	0(%ebp,%r13d,1), %ebp
	movq	%rbp, %r15
	xorl	%r15d, %esi
	rorl	$20, %esi
	leal	0(%ebx,%esi,1), %ebx
	movq	%rbx, %rbp
	xorl	%ebp, %r13d
	rorl	$24, %r13d
	movq	%r15, %rbx
	leal	0(%ebx,%r13d,1), %ebx
	xorl	%ebx, %esi
	rorl	$25, %esi
	leal	0(%r8d,%r14d,1), %r8d
	xorl	%r8d, %r9d
	rorl	$16, %r9d
	movq	%r9, %r15
	leal	0(%r12d,%r15d,1), %r12d
	movq	%r14, %r9
	xorl	%r12d, %r9d
	rorl	$20, %r9d
	leal	0(%r8d,%r9d,1), %r8d
	movq	%r15, %r14
	xorl	%r8d, %r14d
	rorl	$24, %r14d
	leal	0(%r12d,%r14d,1), %r15d
	xorl	%r15d, %r9d
	rorl	$25, %r9d
	leal	0(%r11d,%r9d,1), %r11d
	xorl	%r11d, %edi
	rorl	$16, %edi
	movq	%rbx, %r12
	movq	%rdi, %rbx
	leal	0(%r12d,%ebx,1), %edi
	xorl	%edi, %r9d
	rorl	$20, %r9d
	leal	0(%r11d,%r9d,1), %r11d
	xorl	%r11d, %ebx
	rorl	$24, %ebx
	leal	0(%edi,%ebx,1), %edi
	movq	%rdi, %r12
	movq	%r12, %rdi
	xorl	%edi, %r9d
	rorl	$25, %r9d
	leal	0(%eax,%edx,1), %eax
	movq	%r13, %rdi
	xorl	%eax, %edi
	rorl	$16, %edi
	movq	%rdi, %r13
	leal	0(%r15d,%r13d,1), %edi
	xorl	%edi, %edx
	rorl	$20, %edx
	leal	0(%eax,%edx,1), %eax
	xorl	%eax, %r13d
	rorl	$24, %r13d
	leal	0(%edi,%r13d,1), %edi
	movl	%edi, 56(%rsp)
	movl	56(%rsp), %edi
	xorl	%edi, %edx
	rorl	$25, %edx
	movq	%rbp, %rdi
	movq	%r10, %rbp
	leal	0(%edi,%ebp,1), %r10d
	movq	%r14, %rdi
	xorl	%r10d, %edi
	rorl	$16, %edi
	leal	0(%ecx,%edi,1), %ecx
	xorl	%ecx, %ebp
	rorl	$20, %ebp
	leal	0(%r10d,%ebp,1), %r10d
	xorl	%r10d, %edi
	rorl	$24, %edi
	leal	0(%ecx,%edi,1), %ecx
	movq	%rcx, %r14
	xorl	%r14d, %ebp
	rorl	$25, %ebp
	leal	0(%r8d,%esi,1), %ecx
	movl	64(%rsp), %r15d
	xorl	%ecx, %r15d
	rorl	$16, %r15d
	movl	60(%rsp), %r8d
	leal	0(%r8d,%r15d,1), %r8d
	xorl	%r8d, %esi
	rorl	$20, %esi
	leal	0(%ecx,%esi,1), %ecx
	xorl	%ecx, %r15d
	rorl	$24, %r15d
	leal	0(%r8d,%r15d,1), %r8d
	xorl	%r8d, %esi
	rorl	$25, %esi
	leal	0(%r11d,%edx,1), %r11d
	xorl	%r11d, %r15d
	rorl	$16, %r15d
	leal	0(%r14d,%r15d,1), %r14d
	xorl	%r14d, %edx
	rorl	$20, %edx
	leal	0(%r11d,%edx,1), %r11d
	xorl	%r11d, %r15d
	rorl	$24, %r15d
	movl	%r15d, 60(%rsp)
	movl	60(%rsp), %r15d
	leal	0(%r14d,%r15d,1), %r14d
	xorl	%r14d, %edx
	rorl	$25, %edx
	leal	0(%eax,%ebp,1), %eax
	movq	%rbx, %r15
	movq	%rax, %rbx
	xorl	%ebx, %r15d
	rorl	$16, %r15d
	movq	%r15, %rax
	leal	0(%r8d,%eax,1), %r8d
	xorl	%r8d, %ebp
	rorl	$20, %ebp
	leal	0(%ebx,%ebp,1), %ebx
	xorl	%ebx, %eax
	rorl	$24, %eax
	movq	%r8, %r15
	movq	%rax, %r8
	leal	0(%r15d,%r8d,1), %eax
	xorl	%eax, %ebp
	rorl	$25, %ebp
	leal	0(%r10d,%esi,1), %r10d
	movq	%r13, %r15
	movq	%r10, %r13
	xorl	%r13d, %r15d
	rorl	$16, %r15d
	movq	%r12, %r10
	movq	%r15, %r12
	leal	0(%r10d,%r12d,1), %r10d
	xorl	%r10d, %esi
	rorl	$20, %esi
	leal	0(%r13d,%esi,1), %r13d
	xorl	%r13d, %r12d
	rorl	$24, %r12d
	leal	0(%r10d,%r12d,1), %r10d
	xorl	%r10d, %esi
	rorl	$25, %esi
	leal	0(%ecx,%r9d,1), %ecx
	movq	%rcx, %r15
	xorl	%r15d, %edi
	rorl	$16, %edi
	movl	56(%rsp), %ecx
	leal	0(%ecx,%edi,1), %ecx
	xorl	%ecx, %r9d
	rorl	$20, %r9d
	leal	0(%r15d,%r9d,1), %r15d
	xorl	%r15d, %edi
	rorl	$24, %edi
	leal	0(%ecx,%edi,1), %ecx
	xorl	%ecx, %r9d
	rorl	$25, %r9d
	leal	0(%r11d,%r9d,1), %r11d
	xorl	%r11d, %r8d
	rorl	$16, %r8d
	leal	0(%r10d,%r8d,1), %r10d
	xorl	%r10d, %r9d
	rorl	$20, %r9d
	leal	0(%r11d,%r9d,1), %r11d
	xorl	%r11d, %r8d
	rorl	$24, %r8d
	leal	0(%r10d,%r8d,1), %r10d
	xorl	%r10d, %r9d
	rorl	$25, %r9d
	movl	%r9d, 56(%rsp)
	movq	%rbx, %r9
	leal	0(%r9d,%edx,1), %ebx
	movq	%r12, %r9
	xorl	%ebx, %r9d
	rorl	$16, %r9d
	leal	0(%ecx,%r9d,1), %ecx
	xorl	%ecx, %edx
	rorl	$20, %edx
	movq	%rbx, %r12
	movq	%rdx, %rbx
	leal	0(%r12d,%ebx,1), %edx
	movq	%rdx, %r12
	xorl	%r12d, %r9d
	rorl	$24, %r9d
	movq	%rcx, %rdx
	movq	%r9, %rcx
	leal	0(%edx,%ecx,1), %edx
	xorl	%edx, %ebx
	rorl	$25, %ebx
	leal	0(%r13d,%ebp,1), %r13d
	movq	%rdi, %r9
	movq	%r13, %rdi
	xorl	%edi, %r9d
	rorl	$16, %r9d
	leal	0(%r14d,%r9d,1), %r13d
	movq	%r13, %r14
	xorl	%r14d, %ebp
	rorl	$20, %ebp
	leal	0(%edi,%ebp,1), %edi
	movq	%rdi, %r13
	xorl	%r13d, %r9d
	rorl	$24, %r9d
	movq	%r14, %rdi
	movq	%r9, %r14
	leal	0(%edi,%r14d,1), %edi
	xorl	%edi, %ebp
	rorl	$25, %ebp
	movq	%r15, %r9
	leal	0(%r9d,%esi,1), %r9d
	movl	60(%rsp), %r15d
	xorl	%r9d, %r15d
	rorl	$16, %r15d
	leal	0(%eax,%r15d,1), %eax
	xorl	%eax, %esi
	rorl	$20, %esi
	leal	0(%r9d,%esi,1), %r9d
	xorl	%r9d, %r15d
	rorl	$24, %r15d
	leal	0(%eax,%r15d,1), %eax
	xorl	%eax, %esi
	rorl	$25, %esi
	leal	0(%r11d,%ebx,1), %r11d
	xorl	%r11d, %r15d
	rorl	$16, %r15d
	leal	0(%edi,%r15d,1), %edi
	xorl	%edi, %ebx
	rorl	$20, %ebx
	leal	0(%r11d,%ebx,1), %r11d
	xorl	%r11d, %r15d
	rorl	$24, %r15d
	movl	%r15d, 60(%rsp)
	movl	60(%rsp), %r15d
	leal	0(%edi,%r15d,1), %edi
	xorl	%edi, %ebx
	rorl	$25, %ebx
	leal	0(%r12d,%ebp,1), %r12d
	xorl	%r12d, %r8d
	rorl	$16, %r8d
	leal	0(%eax,%r8d,1), %eax
	xorl	%eax, %ebp
	rorl	$20, %ebp
	leal	0(%r12d,%ebp,1), %r12d
	xorl	%r12d, %r8d
	rorl	$24, %r8d
	movq	%rax, %r15
	movq	%r8, %rax
	leal	0(%r15d,%eax,1), %r8d
	xorl	%r8d, %ebp
	rorl	$25, %ebp
	leal	0(%r13d,%esi,1), %r15d
	movq	%rcx, %r13
	movq	%r15, %rcx
	xorl	%ecx, %r13d
	rorl	$16, %r13d
	leal	0(%r10d,%r13d,1), %r10d
	xorl	%r10d, %esi
	rorl	$20, %esi
	movq	%rcx, %r15
	movq	%rsi, %rcx
	leal	0(%r15d,%ecx,1), %esi
	xorl	%esi, %r13d
	rorl	$24, %r13d
	leal	0(%r10d,%r13d,1), %r15d
	xorl	%r15d, %ecx
	rorl	$25, %ecx
	movl	56(%rsp), %r10d
	leal	0(%r9d,%r10d,1), %r9d
	xorl	%r9d, %r14d
	rorl	$16, %r14d
	leal	0(%edx,%r14d,1), %edx
	xorl	%edx, %r10d
	rorl	$20, %r10d
	leal	0(%r9d,%r10d,1), %r9d
	xorl	%r9d, %r14d
	rorl	$24, %r14d
	leal	0(%edx,%r14d,1), %edx
	xorl	%edx, %r10d
	rorl	$25, %r10d
	leal	0(%r11d,%r10d,1), %r11d
	xorl	%r11d, %eax
	rorl	$16, %eax
	leal	0(%r15d,%eax,1), %r15d
	xorl	%r15d, %r10d
	rorl	$20, %r10d
	leal	0(%r11d,%r10d,1), %r11d
	xorl	%r11d, %eax
	rorl	$24, %eax
	leal	0(%r15d,%eax,1), %r15d
	movl	%r15d, 56(%rsp)
	movl	56(%rsp), %r15d
	xorl	%r15d, %r10d
	rorl	$25, %r10d
	leal	0(%r12d,%ebx,1), %r15d
	movq	%r13, %r12
	movq	%r15, %r13
	xorl	%r13d, %r12d
	rorl	$16, %r12d
	leal	0(%edx,%r12d,1), %r15d
	movq	%rbx, %rdx
	movq	%r15, %rbx
	xorl	%ebx, %edx
	rorl	$20, %edx
	leal	0(%r13d,%edx,1), %r13d
	xorl	%r13d, %r12d
	rorl	$24, %r12d
	leal	0(%ebx,%r12d,1), %ebx
	xorl	%ebx, %edx
	rorl	$25, %edx
	leal	0(%esi,%ebp,1), %r15d
	movq	%r14, %rsi
	movq	%r15, %r14
	xorl	%r14d, %esi
	rorl	$16, %esi
	leal	0(%edi,%esi,1), %edi
	movq	%rbp, %r15
	movq	%rdi, %rbp
	xorl	%ebp, %r15d
	rorl	$20, %r15d
	movq	%r14, %rdi
	movq	%r15, %r14
	leal	0(%edi,%r14d,1), %edi
	xorl	%edi, %esi
	rorl	$24, %esi
	movq	%rbp, %r15
	movq	%rsi, %rbp
	leal	0(%r15d,%ebp,1), %esi
	xorl	%esi, %r14d
	rorl	$25, %r14d
	leal	0(%r9d,%ecx,1), %r9d
	movl	60(%rsp), %r15d
	xorl	%r9d, %r15d
	rorl	$16, %r15d
	leal	0(%r8d,%r15d,1), %r8d
	xorl	%r8d, %ecx
	rorl	$20, %ecx
	leal	0(%r9d,%ecx,1), %r9d
	xorl	%r9d, %r15d
	rorl	$24, %r15d
	leal	0(%r8d,%r15d,1), %r8d
	xorl	%r8d, %ecx
	rorl	$25, %ecx
	leal	0(%r11d,%edx,1), %r11d
	xorl	%r11d, %r15d
	rorl	$16, %r15d
	leal	0(%esi,%r15d,1), %esi
	xorl	%esi, %edx
	rorl	$20, %edx
	leal	0(%r11d,%edx,1), %r11d
	xorl	%r11d, %r15d
	rorl	$24, %r15d
	movl	%r15d, 60(%rsp)
	movl	60(%rsp), %r15d
	leal	0(%esi,%r15d,1), %esi
	xorl	%esi, %edx
	rorl	$25, %edx
	leal	0(%r13d,%r14d,1), %r13d
	xorl	%r13d, %eax
	rorl	$16, %eax
	leal	0(%r8d,%eax,1), %r8d
	movq	%r14, %r15
	xorl	%r8d, %r15d
	rorl	$20, %r15d
	movq	%r13, %r14
	movq	%r15, %r13
	leal	0(%r14d,%r13d,1), %r15d
	xorl	%r15d, %eax
	rorl	$24, %eax
	movq	%r8, %r14
	movq	%rax, %r8
	leal	0(%r14d,%r8d,1), %eax
	xorl	%eax, %r13d
	rorl	$25, %r13d
	leal	0(%edi,%ecx,1), %edi
	movq	%rdi, %r14
	xorl	%r14d, %r12d
	rorl	$16, %r12d
	movl	56(%rsp), %edi
	leal	0(%edi,%r12d,1), %edi
	xorl	%edi, %ecx
	rorl	$20, %ecx
	leal	0(%r14d,%ecx,1), %r14d
	xorl	%r14d, %r12d
	rorl	$24, %r12d
	leal	0(%edi,%r12d,1), %edi
	xorl	%edi, %ecx
	rorl	$25, %ecx
	leal	0(%r9d,%r10d,1), %r9d
	xorl	%r9d, %ebp
	rorl	$16, %ebp
	leal	0(%ebx,%ebp,1), %ebx
	xorl	%ebx, %r10d
	rorl	$20, %r10d
	leal	0(%r9d,%r10d,1), %r9d
	xorl	%r9d, %ebp
	rorl	$24, %ebp
	leal	0(%ebx,%ebp,1), %ebx
	xorl	%ebx, %r10d
	rorl	$25, %r10d
	leal	0(%r11d,%r10d,1), %r11d
	xorl	%r11d, %r8d
	rorl	$16, %r8d
	leal	0(%edi,%r8d,1), %edi
	xorl	%edi, %r10d
	rorl	$20, %r10d
	leal	0(%r11d,%r10d,1), %r11d
	xorl	%r11d, %r8d
	rorl	$24, %r8d
	leal	0(%edi,%r8d,1), %edi
	xorl	%edi, %r10d
	rorl	$25, %r10d
	movl	%r10d, 56(%rsp)
	movq	%r15, %r10
	leal	0(%r10d,%edx,1), %r15d
	movq	%r12, %r10
	movq	%r15, %r12
	xorl	%r12d, %r10d
	rorl	$16, %r10d
	leal	0(%ebx,%r10d,1), %ebx
	xorl	%ebx, %edx
	rorl	$20, %edx
	leal	0(%r12d,%edx,1), %r12d
	movq	%r10, %r15
	xorl	%r12d, %r15d
	rorl	$24, %r15d
	movq	%rbx, %r10
	movq	%r15, %rbx
	leal	0(%r10d,%ebx,1), %r10d
	xorl	%r10d, %edx
	rorl	$25, %edx
	leal	0(%r14d,%r13d,1), %r15d
	movq	%rbp, %r14
	movq	%r15, %rbp
	xorl	%ebp, %r14d
	rorl	$16, %r14d
	leal	0(%esi,%r14d,1), %esi
	xorl	%esi, %r13d
	rorl	$20, %r13d
	leal	0(%ebp,%r13d,1), %ebp
	xorl	%ebp, %r14d
	rorl	$24, %r14d
	leal	0(%esi,%r14d,1), %esi
	xorl	%esi, %r13d
	rorl	$25, %r13d
	leal	0(%r9d,%ecx,1), %r9d
	movl	60(%rsp), %r15d
	xorl	%r9d, %r15d
	rorl	$16, %r15d
	leal	0(%eax,%r15d,1), %eax
	xorl	%eax, %ecx
	rorl	$20, %ecx
	leal	0(%r9d,%ecx,1), %r9d
	xorl	%r9d, %r15d
	rorl	$24, %r15d
	leal	0(%eax,%r15d,1), %eax
	xorl	%eax, %ecx
	rorl	$25, %ecx
	leal	0(%r11d,%edx,1), %r11d
	xorl	%r11d, %r15d
	rorl	$16, %r15d
	leal	0(%esi,%r15d,1), %esi
	xorl	%esi, %edx
	rorl	$20, %edx
	leal	0(%r11d,%edx,1), %r11d
	xorl	%r11d, %r15d
	rorl	$24, %r15d
	movl	%r15d, 60(%rsp)
	movl	60(%rsp), %r15d
	leal	0(%esi,%r15d,1), %esi
	xorl	%esi, %edx
	rorl	$25, %edx
	leal	0(%r12d,%r13d,1), %r15d
	movq	%r8, %r12
	movq	%r15, %r8
	xorl	%r8d, %r12d
	rorl	$16, %r12d
	leal	0(%eax,%r12d,1), %eax
	xorl	%eax, %r13d
	rorl	$20, %r13d
	leal	0(%r8d,%r13d,1), %r15d
	movq	%r12, %r8
	movq	%r15, %r12
	xorl	%r12d, %r8d
	rorl	$24, %r8d
	leal	0(%eax,%r8d,1), %eax
	xorl	%eax, %r13d
	rorl	$25, %r13d
	leal	0(%ebp,%ecx,1), %ebp
	xorl	%ebp, %ebx
	rorl	$16, %ebx
	leal	0(%edi,%ebx,1), %edi
	xorl	%edi, %ecx
	rorl	$20, %ecx
	leal	0(%ebp,%ecx,1), %r15d
	movq	%rbx, %rbp
	movq	%r15, %rbx
	xorl	%ebx, %ebp
	rorl	$24, %ebp
	leal	0(%edi,%ebp,1), %r15d
	movq	%rcx, %rdi
	xorl	%r15d, %edi
	rorl	$25, %edi
	movq	%r9, %rcx
	movl	56(%rsp), %r9d
	leal	0(%ecx,%r9d,1), %ecx
	xorl	%ecx, %r14d
	rorl	$16, %r14d
	leal	0(%r10d,%r14d,1), %r10d
	xorl	%r10d, %r9d
	rorl	$20, %r9d
	leal	0(%ecx,%r9d,1), %ecx
	xorl	%ecx, %r14d
	rorl	$24, %r14d
	leal	0(%r10d,%r14d,1), %r10d
	xorl	%r10d, %r9d
	rorl	$25, %r9d
	leal	0(%r11d,%r9d,1), %r11d
	xorl	%r11d, %r8d
	rorl	$16, %r8d
	leal	0(%r15d,%r8d,1), %r15d
	xorl	%r15d, %r9d
	rorl	$20, %r9d
	leal	0(%r11d,%r9d,1), %r11d
	xorl	%r11d, %r8d
	rorl	$24, %r8d
	leal	0(%r15d,%r8d,1), %r15d
	movl	%r15d, 56(%rsp)
	movl	56(%rsp), %r15d
	xorl	%r15d, %r9d
	rorl	$25, %r9d
	leal	0(%r12d,%edx,1), %r12d
	xorl	%r12d, %ebp
	rorl	$16, %ebp
	movq	%r10, %r15
	movq	%rbp, %r10
	leal	0(%r15d,%r10d,1), %ebp
	xorl	%ebp, %edx
	rorl	$20, %edx
	leal	0(%r12d,%edx,1), %r12d
	xorl	%r12d, %r10d
	rorl	$24, %r10d
	leal	0(%ebp,%r10d,1), %r15d
	movq	%rdx, %rbp
	movq	%r15, %rdx
	xorl	%edx, %ebp
	rorl	$25, %ebp
	leal	0(%ebx,%r13d,1), %ebx
	xorl	%ebx, %r14d
	rorl	$16, %r14d
	movq	%rsi, %r15
	movq	%r14, %rsi
	leal	0(%r15d,%esi,1), %r14d
	xorl	%r14d, %r13d
	rorl	$20, %r13d
	leal	0(%ebx,%r13d,1), %ebx
	xorl	%ebx, %esi
	rorl	$24, %esi
	leal	0(%r14d,%esi,1), %r14d
	xorl	%r14d, %r13d
	rorl	$25, %r13d
	leal	0(%ecx,%edi,1), %ecx
	movl	60(%rsp), %r15d
	xorl	%ecx, %r15d
	rorl	$16, %r15d
	leal	0(%eax,%r15d,1), %eax
	xorl	%eax, %edi
	rorl	$20, %edi
	leal	0(%ecx,%edi,1), %ecx
	xorl	%ecx, %r15d
	rorl	$24, %r15d
	leal	0(%eax,%r15d,1), %eax
	xorl	%eax, %edi
	rorl	$25, %edi
	leal	0(%r11d,%ebp,1), %r11d
	xorl	%r11d, %r15d
	rorl	$16, %r15d
	leal	0(%r14d,%r15d,1), %r14d
	xorl	%r14d, %ebp
	rorl	$20, %ebp
	leal	0(%r11d,%ebp,1), %r11d
	xorl	%r11d, %r15d
	rorl	$24, %r15d
	movl	%r15d, 60(%rsp)
	movl	60(%rsp), %r15d
	leal	0(%r14d,%r15d,1), %r14d
	movq	%r14, %r15
	xorl	%r15d, %ebp
	rorl	$25, %ebp
	leal	0(%r12d,%r13d,1), %r12d
	xorl	%r12d, %r8d
	rorl	$16, %r8d
	leal	0(%eax,%r8d,1), %eax
	xorl	%eax, %r13d
	rorl	$20, %r13d
	leal	0(%r12d,%r13d,1), %r12d
	xorl	%r12d, %r8d
	rorl	$24, %r8d
	movq	%rax, %r14
	movq	%r8, %rax
	leal	0(%r14d,%eax,1), %r8d
	xorl	%r8d, %r13d
	rorl	$25, %r13d
	leal	0(%ebx,%edi,1), %ebx
	movq	%rbx, %r14
	xorl	%r14d, %r10d
	rorl	$16, %r10d
	movl	56(%rsp), %ebx
	leal	0(%ebx,%r10d,1), %ebx
	xorl	%ebx, %edi
	rorl	$20, %edi
	leal	0(%r14d,%edi,1), %r14d
	xorl	%r14d, %r10d
	rorl	$24, %r10d
	leal	0(%ebx,%r10d,1), %ebx
	xorl	%ebx, %edi
	rorl	$25, %edi
	leal	0(%ecx,%r9d,1), %ecx
	xorl	%ecx, %esi
	rorl	$16, %esi
	leal	0(%edx,%esi,1), %edx
	xorl	%edx, %r9d
	rorl	$20, %r9d
	leal	0(%ecx,%r9d,1), %ecx
	xorl	%ecx, %esi
	rorl	$24, %esi
	leal	0(%edx,%esi,1), %edx
	xorl	%edx, %r9d
	rorl	$25, %r9d
	leal	0(%r11d,%r9d,1), %r11d
	xorl	%r11d, %eax
	rorl	$16, %eax
	leal	0(%ebx,%eax,1), %ebx
	xorl	%ebx, %r9d
	rorl	$20, %r9d
	leal	0(%r11d,%r9d,1), %r11d
	xorl	%r11d, %eax
	rorl	$24, %eax
	leal	0(%ebx,%eax,1), %ebx
	xorl	%ebx, %r9d
	rorl	$25, %r9d
	movl	%r9d, 56(%rsp)
	movq	%r12, %r9
	leal	0(%r9d,%ebp,1), %r12d
	movq	%r10, %r9
	movq	%r12, %r10
	xorl	%r10d, %r9d
	rorl	$16, %r9d
	leal	0(%edx,%r9d,1), %edx
	xorl	%edx, %ebp
	rorl	$20, %ebp
	movq	%r10, %r12
	movq	%rbp, %r10
	leal	0(%r12d,%r10d,1), %ebp
	movq	%rbp, %r12
	xorl	%r12d, %r9d
	rorl	$24, %r9d
	movq	%r9, %rbp
	leal	0(%edx,%ebp,1), %edx
	xorl	%edx, %r10d
	rorl	$25, %r10d
	movq	%r14, %r9
	leal	0(%r9d,%r13d,1), %r14d
	movq	%rsi, %r9
	movq	%r14, %rsi
	xorl	%esi, %r9d
	rorl	$16, %r9d
	leal	0(%r15d,%r9d,1), %r14d
	xorl	%r14d, %r13d
	rorl	$20, %r13d
	movq	%rsi, %r15
	movq	%r13, %rsi
	leal	0(%r15d,%esi,1), %r13d
	xorl	%r13d, %r9d
	rorl	$24, %r9d
	movq	%r9, %r15
	movq	%r15, %r9
	leal	0(%r14d,%r9d,1), %r9d
	xorl	%r9d, %esi
	rorl	$25, %esi
	leal	0(%ecx,%edi,1), %ecx
	movl	60(%rsp), %r14d
	xorl	%ecx, %r14d
	rorl	$16, %r14d
	leal	0(%r8d,%r14d,1), %r8d
	xorl	%r8d, %edi
	rorl	$20, %edi
	leal	0(%ecx,%edi,1), %ecx
	xorl	%ecx, %r14d
	rorl	$24, %r14d
	leal	0(%r8d,%r14d,1), %r8d
	xorl	%r8d, %edi
	rorl	$25, %edi
	leal	0(%r11d,%r10d,1), %r11d
	xorl	%r11d, %r14d
	rorl	$16, %r14d
	leal	0(%r9d,%r14d,1), %r9d
	xorl	%r9d, %r10d
	rorl	$20, %r10d
	leal	0(%r11d,%r10d,1), %r11d
	xorl	%r11d, %r14d
	rorl	$24, %r14d
	movl	%r14d, 60(%rsp)
	movl	60(%rsp), %r14d
	leal	0(%r9d,%r14d,1), %r9d
	xorl	%r9d, %r10d
	rorl	$25, %r10d
	leal	0(%r12d,%esi,1), %r14d
	movq	%rax, %r12
	movq	%r14, %rax
	xorl	%eax, %r12d
	rorl	$16, %r12d
	movq	%r12, %r14
	leal	0(%r8d,%r14d,1), %r8d
	movq	%rsi, %r12
	movq	%r8, %rsi
	xorl	%esi, %r12d
	rorl	$20, %r12d
	movq	%rax, %r8
	leal	0(%r8d,%r12d,1), %r8d
	movq	%r8, %rax
	xorl	%eax, %r14d
	rorl	$24, %r14d
	movq	%r14, %r8
	leal	0(%esi,%r8d,1), %esi
	xorl	%esi, %r12d
	rorl	$25, %r12d
	leal	0(%r13d,%edi,1), %r13d
	xorl	%r13d, %ebp
	rorl	$16, %ebp
	leal	0(%ebx,%ebp,1), %ebx
	xorl	%ebx, %edi
	rorl	$20, %edi
	leal	0(%r13d,%edi,1), %r13d
	movq	%r13, %r14
	xorl	%r14d, %ebp
	rorl	$24, %ebp
	leal	0(%ebx,%ebp,1), %r13d
	xorl	%r13d, %edi
	rorl	$25, %edi
	movl	56(%rsp), %ebx
	leal	0(%ecx,%ebx,1), %ecx
	xorl	%ecx, %r15d
	rorl	$16, %r15d
	leal	0(%edx,%r15d,1), %edx
	xorl	%edx, %ebx
	rorl	$20, %ebx
	leal	0(%ecx,%ebx,1), %ecx
	xorl	%ecx, %r15d
	rorl	$24, %r15d
	leal	0(%edx,%r15d,1), %edx
	xorl	%edx, %ebx
	rorl	$25, %ebx
	leal	0(%r11d,%ebx,1), %r11d
	xorl	%r11d, %r8d
	rorl	$16, %r8d
	leal	0(%r13d,%r8d,1), %r13d
	xorl	%r13d, %ebx
	rorl	$20, %ebx
	leal	0(%r11d,%ebx,1), %r11d
	xorl	%r11d, %r8d
	rorl	$24, %r8d
	leal	0(%r13d,%r8d,1), %r13d
	movl	%r13d, 56(%rsp)
	movl	56(%rsp), %r13d
	xorl	%r13d, %ebx
	rorl	$25, %ebx
	leal	0(%eax,%r10d,1), %eax
	xorl	%eax, %ebp
	rorl	$16, %ebp
	leal	0(%edx,%ebp,1), %edx
	movq	%r10, %r13
	xorl	%edx, %r13d
	rorl	$20, %r13d
	movq	%rax, %r10
	movq	%r13, %rax
	leal	0(%r10d,%eax,1), %r10d
	xorl	%r10d, %ebp
	rorl	$24, %ebp
	movq	%rbp, %r13
	leal	0(%edx,%r13d,1), %ebp
	xorl	%ebp, %eax
	rorl	$25, %eax
	leal	0(%r14d,%r12d,1), %edx
	movq	%r15, %r14
	xorl	%edx, %r14d
	rorl	$16, %r14d
	leal	0(%r9d,%r14d,1), %r15d
	movq	%r12, %r9
	movq	%r15, %r12
	xorl	%r12d, %r9d
	rorl	$20, %r9d
	leal	0(%edx,%r9d,1), %edx
	xorl	%edx, %r14d
	rorl	$24, %r14d
	movq	%r12, %r15
	movq	%r14, %r12
	leal	0(%r15d,%r12d,1), %r14d
	xorl	%r14d, %r9d
	rorl	$25, %r9d
	leal	0(%ecx,%edi,1), %ecx
	movl	60(%rsp), %r15d
	xorl	%ecx, %r15d
	rorl	$16, %r15d
	leal	0(%esi,%r15d,1), %esi
	xorl	%esi, %edi
	rorl	$20, %edi
	leal	0(%ecx,%edi,1), %ecx
	xorl	%ecx, %r15d
	rorl	$24, %r15d
	leal	0(%esi,%r15d,1), %esi
	xorl	%esi, %edi
	rorl	$25, %edi
	leal	0(%r11d,%eax,1), %r11d
	xorl	%r11d, %r15d
	rorl	$16, %r15d
	leal	0(%r14d,%r15d,1), %r14d
	xorl	%r14d, %eax
	rorl	$20, %eax
	leal	0(%r11d,%eax,1), %r11d
	xorl	%r11d, %r15d
	rorl	$24, %r15d
	movl	%r15d, 60(%rsp)
	movl	60(%rsp), %r15d
	leal	0(%r14d,%r15d,1), %r14d
	xorl	%r14d, %eax
	rorl	$25, %eax
	leal	0(%r10d,%r9d,1), %r15d
	movq	%r8, %r10
	movq	%r15, %r8
	xorl	%r8d, %r10d
	rorl	$16, %r10d
	leal	0(%esi,%r10d,1), %esi
	xorl	%esi, %r9d
	rorl	$20, %r9d
	leal	0(%r8d,%r9d,1), %r15d
	movq	%r10, %r8
	xorl	%r15d, %r8d
	rorl	$24, %r8d
	movq	%r8, %r10
	leal	0(%esi,%r10d,1), %esi
	movq	%rsi, %r8
	xorl	%r8d, %r9d
	rorl	$25, %r9d
	leal	0(%edx,%edi,1), %edx
	movq	%rdx, %r8
	xorl	%r8d, %r13d
	rorl	$16, %r13d
	movl	56(%rsp), %edx
	leal	0(%edx,%r13d,1), %edx
	xorl	%edx, %edi
	rorl	$20, %edi
	leal	0(%r8d,%edi,1), %r8d
	xorl	%r8d, %r13d
	rorl	$24, %r13d
	leal	0(%edx,%r13d,1), %edx
	xorl	%edx, %edi
	rorl	$25, %edi
	leal	0(%ecx,%ebx,1), %ecx
	xorl	%ecx, %r12d
	rorl	$16, %r12d
	leal	0(%ebp,%r12d,1), %ebp
	xorl	%ebp, %ebx
	rorl	$20, %ebx
	leal	0(%ecx,%ebx,1), %ecx
	xorl	%ecx, %r12d
	rorl	$24, %r12d
	leal	0(%ebp,%r12d,1), %ebp
	xorl	%ebp, %ebx
	rorl	$25, %ebx
	leal	0(%r11d,%ebx,1), %r11d
	xorl	%r11d, %r10d
	rorl	$16, %r10d
	leal	0(%edx,%r10d,1), %edx
	xorl	%edx, %ebx
	rorl	$20, %ebx
	leal	0(%r11d,%ebx,1), %r11d
	xorl	%r11d, %r10d
	rorl	$24, %r10d
	leal	0(%edx,%r10d,1), %edx
	xorl	%edx, %ebx
	rorl	$25, %ebx
	movl	%ebx, 64(%rsp)
	movq	%r15, %rbx
	leal	0(%ebx,%eax,1), %ebx
	xorl	%ebx, %r13d
	rorl	$16, %r13d
	leal	0(%ebp,%r13d,1), %ebp
	movq	%rbp, %r15
	xorl	%r15d, %eax
	rorl	$20, %eax
	leal	0(%ebx,%eax,1), %ebx
	movq	%rbx, %rbp
	xorl	%ebp, %r13d
	rorl	$24, %r13d
	movq	%r15, %rbx
	leal	0(%ebx,%r13d,1), %ebx
	xorl	%ebx, %eax
	rorl	$25, %eax
	leal	0(%r8d,%r9d,1), %r8d
	xorl	%r8d, %r12d
	rorl	$16, %r12d
	leal	0(%r14d,%r12d,1), %r14d
	movq	%r14, %r15
	xorl	%r15d, %r9d
	rorl	$20, %r9d
	leal	0(%r8d,%r9d,1), %r14d
	movq	%r12, %r8
	xorl	%r14d, %r8d
	rorl	$24, %r8d
	movq	%r15, %r12
	leal	0(%r12d,%r8d,1), %r12d
	xorl	%r12d, %r9d
	rorl	$25, %r9d
	leal	0(%ecx,%edi,1), %ecx
	movl	60(%rsp), %r15d
	xorl	%ecx, %r15d
	rorl	$16, %r15d
	leal	0(%esi,%r15d,1), %esi
	xorl	%esi, %edi
	rorl	$20, %edi
	leal	0(%ecx,%edi,1), %ecx
	xorl	%ecx, %r15d
	rorl	$24, %r15d
	leal	0(%esi,%r15d,1), %esi
	xorl	%esi, %edi
	rorl	$25, %edi
	leal	0(%r11d,%eax,1), %r11d
	xorl	%r11d, %r15d
	rorl	$16, %r15d
	leal	0(%r12d,%r15d,1), %r12d
	xorl	%r12d, %eax
	rorl	$20, %eax
	leal	0(%r11d,%eax,1), %r11d
	xorl	%r11d, %r15d
	rorl	$24, %r15d
	movl	%r15d, 56(%rsp)
	movl	56(%rsp), %r15d
	leal	0(%r12d,%r15d,1), %r12d
	xorl	%r12d, %eax
	rorl	$25, %eax
	movq	%r9, %r15
	leal	0(%ebp,%r15d,1), %r9d
	xorl	%r9d, %r10d
	rorl	$16, %r10d
	leal	0(%esi,%r10d,1), %esi
	xorl	%esi, %r15d
	rorl	$20, %r15d
	leal	0(%r9d,%r15d,1), %ebp
	movq	%r10, %r9
	xorl	%ebp, %r9d
	rorl	$24, %r9d
	movq	%rsi, %r10
	leal	0(%r10d,%r9d,1), %r10d
	movq	%r15, %rsi
	xorl	%r10d, %esi
	rorl	$25, %esi
	leal	0(%r14d,%edi,1), %r14d
	xorl	%r14d, %r13d
	rorl	$16, %r13d
	leal	0(%edx,%r13d,1), %edx
	xorl	%edx, %edi
	rorl	$20, %edi
	leal	0(%r14d,%edi,1), %r15d
	movq	%r13, %r14
	movq	%r15, %r13
	xorl	%r13d, %r14d
	rorl	$24, %r14d
	leal	0(%edx,%r14d,1), %r15d
	xorl	%r15d, %edi
	rorl	$25, %edi
	movq	%rcx, %rdx
	movl	64(%rsp), %ecx
	leal	0(%edx,%ecx,1), %edx
	xorl	%edx, %r8d
	rorl	$16, %r8d
	leal	0(%ebx,%r8d,1), %ebx
	xorl	%ebx, %ecx
	rorl	$20, %ecx
	leal	0(%edx,%ecx,1), %edx
	xorl	%edx, %r8d
	rorl	$24, %r8d
	leal	0(%ebx,%r8d,1), %ebx
	xorl	%ebx, %ecx
	rorl	$25, %ecx
	leal	0(%r11d,%ecx,1), %r11d
	xorl	%r11d, %r9d
	rorl	$16, %r9d
	leal	0(%r15d,%r9d,1), %r15d
	xorl	%r15d, %ecx
	rorl	$20, %ecx
	leal	0(%r11d,%ecx,1), %r11d
	xorl	%r11d, %r9d
	rorl	$24, %r9d
	leal	0(%r15d,%r9d,1), %r15d
	movl	%r15d, 64(%rsp)
	movl	64(%rsp), %r15d
	xorl	%r15d, %ecx
	rorl	$25, %ecx
	leal	0(%ebp,%eax,1), %ebp
	movq	%rbp, %r15
	xorl	%r15d, %r14d
	rorl	$16, %r14d
	leal	0(%ebx,%r14d,1), %ebx
	movq	%rbx, %rbp
	xorl	%ebp, %eax
	rorl	$20, %eax
	movq	%r15, %rbx
	leal	0(%ebx,%eax,1), %ebx
	xorl	%ebx, %r14d
	rorl	$24, %r14d
	movq	%rbp, %r15
	movq	%r14, %rbp
	leal	0(%r15d,%ebp,1), %r14d
	movl	%r14d, 68(%rsp)
	movl	68(%rsp), %r14d
	xorl	%r14d, %eax
	rorl	$25, %eax
	leal	0(%r13d,%esi,1), %r13d
	movq	%r8, %r14
	movq	%r13, %r8
	xorl	%r8d, %r14d
	rorl	$16, %r14d
	movq	%r14, %r13
	leal	0(%r12d,%r13d,1), %r12d
	xorl	%r12d, %esi
	rorl	$20, %esi
	leal	0(%r8d,%esi,1), %r8d
	xorl	%r8d, %r13d
	rorl	$24, %r13d
	movq	%r12, %r14
	movq	%r13, %r12
	leal	0(%r14d,%r12d,1), %r13d
	movq	%r13, %r14
	xorl	%r14d, %esi
	rorl	$25, %esi
	movq	%rdx, %r13
	movq	%rdi, %rdx
	leal	0(%r13d,%edx,1), %edi
	movl	56(%rsp), %r13d
	xorl	%edi, %r13d
	rorl	$16, %r13d
	leal	0(%r10d,%r13d,1), %r15d
	xorl	%r15d, %edx
	rorl	$20, %edx
	leal	0(%edi,%edx,1), %edi
	movq	%rdi, %r10
	xorl	%r10d, %r13d
	rorl	$24, %r13d
	movq	%r15, %rdi
	movq	%r13, %r15
	leal	0(%edi,%r15d,1), %r13d
	movq	%rdx, %rdi
	xorl	%r13d, %edi
	rorl	$25, %edi
	leal	0(%r11d,%eax,1), %edx
	movq	%r15, %r11
	xorl	%edx, %r11d
	rorl	$16, %r11d
	leal	0(%r14d,%r11d,1), %r14d
	xorl	%r14d, %eax
	rorl	$20, %eax
	leal	0(%edx,%eax,1), %edx
	xorl	%edx, %r11d
	rorl	$24, %r11d
	movl	%r11d, 60(%rsp)
	movl	60(%rsp), %r11d
	leal	0(%r14d,%r11d,1), %r11d
	xorl	%r11d, %eax
	rorl	$25, %eax
	movq	%rsi, %r14
	leal	0(%ebx,%r14d,1), %esi
	movq	%rsi, %rbx
	xorl	%ebx, %r9d
	rorl	$16, %r9d
	movq	%r9, %rsi
	leal	0(%r13d,%esi,1), %r9d
	xorl	%r9d, %r14d
	rorl	$20, %r14d
	movq	%rbx, %r13
	movq	%r14, %rbx
	leal	0(%r13d,%ebx,1), %r13d
	movl	%r13d, 56(%rsp)
	movl	56(%rsp), %r13d
	xorl	%r13d, %esi
	rorl	$24, %esi
	leal	0(%r9d,%esi,1), %r9d
	xorl	%r9d, %ebx
	rorl	$25, %ebx
	leal	0(%r8d,%edi,1), %r8d
	xorl	%r8d, %ebp
	rorl	$16, %ebp
	movl	64(%rsp), %r13d
	leal	0(%r13d,%ebp,1), %r13d
	xorl	%r13d, %edi
	rorl	$20, %edi
	leal	0(%r8d,%edi,1), %r8d
	xorl	%r8d, %ebp
	rorl	$24, %ebp
	leal	0(%r13d,%ebp,1), %r14d
	xorl	%r14d, %edi
	rorl	$25, %edi
	leal	0(%r10d,%ecx,1), %r10d
	xorl	%r10d, %r12d
	rorl	$16, %r12d
	movl	68(%rsp), %r13d
	leal	0(%r13d,%r12d,1), %r13d
	xorl	%r13d, %ecx
	rorl	$20, %ecx
	leal	0(%r10d,%ecx,1), %r10d
	xorl	%r10d, %r12d
	rorl	$24, %r12d
	leal	0(%r13d,%r12d,1), %r15d
	xorl	%r15d, %ecx
	rorl	$25, %ecx
	movl	128(%rsp), %r13d
	leal	0(%edx,%r13d,1), %edx
	movl	%edx, 320(%rsp)
	movl	56(%rsp), %edx
	movl	132(%rsp), %r13d
	leal	0(%edx,%r13d,1), %edx
	movl	%edx, 324(%rsp)
	movl	136(%rsp), %edx
	leal	0(%r8d,%edx,1), %edx
	movl	%edx, 328(%rsp)
	movl	140(%rsp), %edx
	leal	0(%r10d,%edx,1), %edx
	movl	%edx, 332(%rsp)
	movl	144(%rsp), %edx
	leal	0(%ecx,%edx,1), %r8d
	movl	%r8d, 336(%rsp)
	movl	148(%rsp), %ecx
	leal	0(%eax,%ecx,1), %eax
	movl	%eax, 340(%rsp)
	movl	152(%rsp), %eax
	leal	0(%ebx,%eax,1), %r8d
	movl	%r8d, 344(%rsp)
	movl	156(%rsp), %eax
	leal	0(%edi,%eax,1), %ecx
	movl	%ecx, 348(%rsp)
	movl	160(%rsp), %edx
	leal	0(%r14d,%edx,1), %ecx
	movl	%ecx, 352(%rsp)
	movl	164(%rsp), %edx
	leal	0(%r15d,%edx,1), %r10d
	movl	%r10d, 356(%rsp)
	movl	168(%rsp), %ecx
	leal	0(%r11d,%ecx,1), %r10d
	movl	%r10d, 360(%rsp)
	movl	172(%rsp), %eax
	leal	0(%r9d,%eax,1), %r10d
	movl	%r10d, 364(%rsp)
	movl	176(%rsp), %edi
	leal	0(%esi,%edi,1), %esi
	movl	180(%rsp), %r8d
	leal	0(%ebp,%r8d,1), %edi
	movl	%edi, 372(%rsp)
	movl	184(%rsp), %edi
	leal	0(%r12d,%edi,1), %ecx
	movl	%ecx, 376(%rsp)
	movl	60(%rsp), %edx
	movl	188(%rsp), %r8d
	leal	0(%edx,%r8d,1), %r8d
	movl	%r8d, 380(%rsp)
	movl	88(%rsp), %r9d
	leal	0(%esi,%r9d,1), %ecx
	movl	%ecx, 368(%rsp)
	leaq	256(%rsp), %rsi
	leaq	384(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	384(%rsp), %r8d
	movl	320(%rsp), %edx
	xorl	%edx, %r8d
	leaq	256(%rsp), %rdi
	movl	%r8d, 384(%rsp)
	leaq	384(%rsp), %rsi
	movq	$4, %rdx
	call	memcpy
	leaq	260(%rsp), %rsi
	leaq	384(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	384(%rsp), %eax
	movl	324(%rsp), %r11d
	xorl	%r11d, %eax
	leaq	260(%rsp), %rdi
	movl	%eax, 384(%rsp)
	leaq	384(%rsp), %rsi
	movq	$4, %rdx
	call	memcpy
	leaq	264(%rsp), %rsi
	leaq	384(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	384(%rsp), %edx
	movl	328(%rsp), %eax
	xorl	%eax, %edx
	leaq	264(%rsp), %rdi
	movl	%edx, 384(%rsp)
	leaq	384(%rsp), %rsi
	movq	$4, %rdx
	call	memcpy
	leaq	268(%rsp), %rsi
	leaq	384(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	384(%rsp), %esi
	movl	332(%rsp), %r11d
	xorl	%r11d, %esi
	leaq	268(%rsp), %rdi
	movl	%esi, 384(%rsp)
	leaq	384(%rsp), %rsi
	movq	$4, %rdx
	call	memcpy
	leaq	272(%rsp), %rsi
	leaq	384(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	384(%rsp), %eax
	movl	336(%rsp), %edi
	xorl	%edi, %eax
	leaq	272(%rsp), %rdi
	movl	%eax, 384(%rsp)
	leaq	384(%rsp), %rsi
	movq	$4, %rdx
	call	memcpy
	leaq	276(%rsp), %rsi
	leaq	384(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	384(%rsp), %r8d
	movl	340(%rsp), %ecx
	xorl	%ecx, %r8d
	leaq	276(%rsp), %rdi
	movl	%r8d, 384(%rsp)
	leaq	384(%rsp), %rsi
	movq	$4, %rdx
	call	memcpy
	leaq	280(%rsp), %rsi
	leaq	384(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	384(%rsp), %r8d
	movl	344(%rsp), %r11d
	xorl	%r11d, %r8d
	leaq	280(%rsp), %rdi
	movl	%r8d, 384(%rsp)
	leaq	384(%rsp), %rsi
	movq	$4, %rdx
	call	memcpy
	leaq	284(%rsp), %rsi
	leaq	384(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	384(%rsp), %r9d
	movl	348(%rsp), %eax
	xorl	%eax, %r9d
	leaq	284(%rsp), %rdi
	movl	%r9d, 384(%rsp)
	leaq	384(%rsp), %rsi
	movq	$4, %rdx
	call	memcpy
	leaq	288(%rsp), %rsi
	leaq	384(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	384(%rsp), %edx
	movl	352(%rsp), %edi
	xorl	%edi, %edx
	leaq	288(%rsp), %rdi
	movl	%edx, 384(%rsp)
	leaq	384(%rsp), %rsi
	movq	$4, %rdx
	call	memcpy
	leaq	292(%rsp), %rsi
	leaq	384(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	384(%rsp), %r8d
	movl	356(%rsp), %r11d
	xorl	%r11d, %r8d
	leaq	292(%rsp), %rdi
	movl	%r8d, 384(%rsp)
	leaq	384(%rsp), %rsi
	movq	$4, %rdx
	call	memcpy
	leaq	296(%rsp), %rsi
	leaq	384(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	384(%rsp), %r11d
	movl	360(%rsp), %eax
	xorl	%eax, %r11d
	leaq	296(%rsp), %rdi
	movl	%r11d, 384(%rsp)
	leaq	384(%rsp), %rsi
	movq	$4, %rdx
	call	memcpy
	leaq	300(%rsp), %rsi
	leaq	384(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	384(%rsp), %ecx
	movl	364(%rsp), %r8d
	xorl	%r8d, %ecx
	leaq	300(%rsp), %rdi
	movl	%ecx, 384(%rsp)
	leaq	384(%rsp), %rsi
	movq	$4, %rdx
	call	memcpy
	leaq	304(%rsp), %rsi
	leaq	384(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	384(%rsp), %r8d
	movl	368(%rsp), %edi
	xorl	%edi, %r8d
	leaq	304(%rsp), %rdi
	movl	%r8d, 384(%rsp)
	leaq	384(%rsp), %rsi
	movq	$4, %rdx
	call	memcpy
	leaq	308(%rsp), %rsi
	leaq	384(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	384(%rsp), %esi
	movl	372(%rsp), %r10d
	xorl	%r10d, %esi
	leaq	308(%rsp), %rdi
	movl	%esi, 384(%rsp)
	leaq	384(%rsp), %rsi
	movq	$4, %rdx
	call	memcpy
	leaq	312(%rsp), %rsi
	leaq	384(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	384(%rsp), %r9d
	movl	376(%rsp), %esi
	xorl	%esi, %r9d
	leaq	312(%rsp), %rdi
	movl	%r9d, 384(%rsp)
	leaq	384(%rsp), %rsi
	movq	$4, %rdx
	call	memcpy
	leaq	316(%rsp), %rsi
	leaq	384(%rsp), %rdi
	movq	$4, %rdx
	call	memcpy
	movl	384(%rsp), %ecx
	movl	380(%rsp), %esi
	xorl	%esi, %ecx
	leaq	316(%rsp), %rdi
	movl	%ecx, 384(%rsp)
	leaq	384(%rsp), %rsi
	movq	$4, %rdx
	call	memcpy
	leaq	256(%rsp), %rsi
	movl	120(%rsp), %edx
	movl	%edx, %edx
	movq	72(%rsp), %rdi
	call	memcpy
.L105:
	movq	8(%rsp), %rbx
	movq	16(%rsp), %rbp
	movq	24(%rsp), %r12
	movq	32(%rsp), %r13
	movq	40(%rsp), %r14
	movq	48(%rsp), %r15
	addq	$456, %rsp
	ret
	.cfi_endproc
	.type	Hacl_Chacha20_Vec32_chacha20_decrypt_32, @function
	.size	Hacl_Chacha20_Vec32_chacha20_decrypt_32, . - Hacl_Chacha20_Vec32_chacha20_decrypt_32
