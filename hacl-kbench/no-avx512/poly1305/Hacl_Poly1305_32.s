# File generated by CompCert 3.7
# Command line: -O3 -S -I . -I ./kremlin/include/ -I ./kremlin/kremlib/dist/minimal/ -D_BSD_SOURCE -D_DEFAULT_SOURCE -DKRML_VERIFIED_UINT128 Hacl_Poly1305_32.c
	.data
	.align	4
	.globl	Hacl_Poly1305_32_blocklen_ccomp
Hacl_Poly1305_32_blocklen_ccomp:
	.long	16
	.type	Hacl_Poly1305_32_blocklen_ccomp, @object
	.size	Hacl_Poly1305_32_blocklen_ccomp, . - Hacl_Poly1305_32_blocklen_ccomp
	.text
	.align	16
	.globl Hacl_Poly1305_32_poly1305_init_ccomp
Hacl_Poly1305_32_poly1305_init_ccomp:
	.cfi_startproc
	subq	$40, %rsp
	.cfi_adjust_cfa_offset	40
	leaq	48(%rsp), %rax
	movq	%rax, 0(%rsp)
	movq	%rbx, 8(%rsp)
	movq	%rbp, 16(%rsp)
	movq	%r12, 24(%rsp)
	leaq	40(%rdi), %rbp
	movq	%rsi, %r12
	xorq	%r11, %r11
	movq	%r11, 0(%rdi)
	xorq	%rdx, %rdx
	movq	%rdx, 8(%rdi)
	xorq	%rcx, %rcx
	movq	%rcx, 16(%rdi)
	xorq	%rsi, %rsi
	movq	%rsi, 24(%rdi)
	xorq	%rax, %rax
	movq	%rax, 32(%rdi)
	movq	%r12, %rsi
	leaq	32(%rsp), %rdi
	movq	$8, %rdx
	call	memcpy
	movq	32(%rsp), %rbx
	leaq	8(%r12), %rsi
	leaq	32(%rsp), %rdi
	movq	$8, %rdx
	call	memcpy
	movq	32(%rsp), %rcx
	movq	%rbx, %rdi
	andq	.L100(%rip), %rdi
	movq	%rcx, %rdx
	andq	.L101(%rip), %rdx
	andq	$67108863, %rbx
	movq	%rdi, %r8
	shrq	$26, %r8
	andq	$67108863, %r8
	shrq	$52, %rdi
	andq	$16380, %rcx
	salq	$12, %rcx
	orq	%rcx, %rdi
	movq	%rdx, %rax
	shrq	$14, %rax
	andq	$67108863, %rax
	shrq	$40, %rdx
	movq	%rbx, 0(%rbp)
	movq	%r8, 8(%rbp)
	movq	%rdi, 16(%rbp)
	movq	%rax, 24(%rbp)
	movq	%rdx, 32(%rbp)
	movq	0(%rbp), %rsi
	movq	8(%rbp), %r10
	movq	16(%rbp), %r9
	movq	24(%rbp), %r11
	leaq	0(%rsi,%rsi,4), %rcx
	movq	%rcx, 40(%rbp)
	leaq	0(%r10,%r10,4), %r10
	movq	%r10, 48(%rbp)
	leaq	0(%r9,%r9,4), %rsi
	movq	%rsi, 56(%rbp)
	leaq	0(%r11,%r11,4), %rsi
	movq	%rsi, 64(%rbp)
	leaq	0(%rdx,%rdx,4), %rax
	movq	%rax, 72(%rbp)
	movq	0(%rbp), %r11
	movq	%r11, 80(%rbp)
	movq	8(%rbp), %r8
	movq	%r8, 88(%rbp)
	movq	16(%rbp), %rdi
	movq	%rdi, 96(%rbp)
	movq	24(%rbp), %r9
	movq	%r9, 104(%rbp)
	movq	32(%rbp), %rdi
	movq	%rdi, 112(%rbp)
	movq	40(%rbp), %rax
	movq	%rax, 120(%rbp)
	movq	48(%rbp), %r9
	movq	%r9, 128(%rbp)
	movq	56(%rbp), %rsi
	movq	%rsi, 136(%rbp)
	movq	64(%rbp), %r8
	movq	%r8, 144(%rbp)
	movq	72(%rbp), %r10
	movq	%r10, 152(%rbp)
	movq	8(%rsp), %rbx
	movq	16(%rsp), %rbp
	movq	24(%rsp), %r12
	addq	$40, %rsp
	ret
	.cfi_endproc
	.type	Hacl_Poly1305_32_poly1305_init_ccomp, @function
	.size	Hacl_Poly1305_32_poly1305_init_ccomp, . - Hacl_Poly1305_32_poly1305_init_ccomp
	.section	.rodata.cst8,"aM",@progbits,8
	.align	8
.L101:	.quad	0xffffffc0ffffffc
.L100:	.quad	0xffffffc0fffffff
	.text
	.align	16
	.globl Hacl_Poly1305_32_poly1305_update1_ccomp
Hacl_Poly1305_32_poly1305_update1_ccomp:
	.cfi_startproc
	subq	$152, %rsp
	.cfi_adjust_cfa_offset	152
	leaq	160(%rsp), %rax
	movq	%rax, 0(%rsp)
	movq	%rbx, 8(%rsp)
	movq	%rbp, 16(%rsp)
	movq	%r12, 24(%rsp)
	movq	%r13, 32(%rsp)
	movq	%r14, 40(%rsp)
	movq	%r15, 48(%rsp)
	movq	%rsi, %r13
	movq	%rdi, %rbx
	leaq	40(%rbx), %rbp
	xorq	%r11, %r11
	movq	%r11, 104(%rsp)
	xorq	%r11, %r11
	movq	%r11, 112(%rsp)
	xorq	%rdi, %rdi
	movq	%rdi, 120(%rsp)
	xorq	%rsi, %rsi
	movq	%rsi, 128(%rsp)
	xorq	%rdx, %rdx
	movq	%rdx, 136(%rsp)
	leaq	144(%rsp), %rdi
	movq	$8, %rdx
	movq	%r13, %rsi
	call	memcpy
	movq	144(%rsp), %r12
	leaq	8(%r13), %rsi
	leaq	144(%rsp), %rdi
	movq	$8, %rdx
	call	memcpy
	movq	144(%rsp), %rcx
	movq	%r12, %rdi
	andq	$67108863, %rdi
	movq	%r12, %rsi
	shrq	$26, %rsi
	andq	$67108863, %rsi
	shrq	$52, %r12
	movq	%rcx, %r9
	andq	$16383, %r9
	salq	$12, %r9
	movq	%r12, %rdx
	orq	%r9, %rdx
	movq	%rcx, %rax
	shrq	$14, %rax
	andq	$67108863, %rax
	shrq	$40, %rcx
	movq	%rdi, 104(%rsp)
	movq	%rsi, 112(%rsp)
	movq	%rdx, 120(%rsp)
	movq	%rax, 128(%rsp)
	orq	$16777216, %rcx
	movq	%rcx, 136(%rsp)
	movq	%rbp, %r11
	movq	0(%r11), %r14
	movq	8(%r11), %r12
	movq	16(%r11), %r15
	movq	24(%r11), %r8
	movq	%r8, 80(%rsp)
	movq	32(%r11), %r8
	movq	%r8, 72(%rsp)
	movq	48(%r11), %r8
	movq	%r8, 96(%rsp)
	movq	56(%r11), %r8
	movq	%r8, 88(%rsp)
	movq	64(%r11), %rbp
	movq	72(%r11), %r13
	movq	%rdi, 56(%rsp)
	movq	0(%rbx), %rdi
	movq	8(%rbx), %r10
	movq	16(%rbx), %r9
	movq	24(%rbx), %r11
	movq	32(%rbx), %r8
	movq	%r8, 64(%rsp)
	movq	56(%rsp), %r8
	leaq	0(%rdi,%r8,1), %rdi
	leaq	0(%r10,%rsi,1), %rsi
	leaq	0(%r9,%rdx,1), %r9
	leaq	0(%r11,%rax,1), %r11
	movq	64(%rsp), %r8
	leaq	0(%r8,%rcx,1), %rcx
	movq	%r14, %r10
	imulq	%rdi, %r10
	movq	%r12, %r8
	imulq	%rdi, %r8
	movq	%r15, %rdx
	imulq	%rdi, %rdx
	movq	80(%rsp), %rax
	imulq	%rdi, %rax
	movq	%rax, 56(%rsp)
	movq	72(%rsp), %rax
	imulq	%rdi, %rax
	movq	%rax, 64(%rsp)
	movq	%r13, %rdi
	imulq	%rsi, %rdi
	leaq	0(%r10,%rdi,1), %rdi
	movq	%r14, %r10
	imulq	%rsi, %r10
	leaq	0(%r8,%r10,1), %r8
	movq	%r12, %rax
	imulq	%rsi, %rax
	leaq	0(%rdx,%rax,1), %r10
	movq	%r15, %rdx
	imulq	%rsi, %rdx
	movq	56(%rsp), %rax
	leaq	0(%rax,%rdx,1), %rax
	movq	80(%rsp), %rdx
	imulq	%rsi, %rdx
	movq	64(%rsp), %rsi
	leaq	0(%rsi,%rdx,1), %rdx
	movq	%rbp, %rsi
	imulq	%r9, %rsi
	leaq	0(%rdi,%rsi,1), %rsi
	movq	%r13, %rdi
	imulq	%r9, %rdi
	leaq	0(%r8,%rdi,1), %rdi
	movq	%r14, %r8
	imulq	%r9, %r8
	leaq	0(%r10,%r8,1), %r8
	movq	%r12, %r10
	imulq	%r9, %r10
	leaq	0(%rax,%r10,1), %r10
	imulq	%r9, %r15
	leaq	0(%rdx,%r15,1), %r9
	movq	88(%rsp), %rax
	imulq	%r11, %rax
	leaq	0(%rsi,%rax,1), %rdx
	movq	%rbp, %rax
	imulq	%r11, %rax
	leaq	0(%rdi,%rax,1), %rdi
	movq	%r13, %rax
	imulq	%r11, %rax
	leaq	0(%r8,%rax,1), %r8
	movq	%r14, %rax
	imulq	%r11, %rax
	leaq	0(%r10,%rax,1), %rax
	imulq	%r11, %r12
	leaq	0(%r9,%r12,1), %rsi
	movq	96(%rsp), %r10
	imulq	%rcx, %r10
	leaq	0(%rdx,%r10,1), %rdx
	movq	88(%rsp), %r10
	imulq	%rcx, %r10
	leaq	0(%rdi,%r10,1), %rdi
	imulq	%rcx, %rbp
	leaq	0(%r8,%rbp,1), %r8
	imulq	%rcx, %r13
	leaq	0(%rax,%r13,1), %r9
	imulq	%rcx, %r14
	leaq	0(%rsi,%r14,1), %r10
	movq	%rdx, %r11
	shrq	$26, %r11
	movq	%r9, %rsi
	shrq	$26, %rsi
	andq	$67108863, %rdx
	andq	$67108863, %r9
	leaq	0(%rdi,%r11,1), %rdi
	leaq	0(%r10,%rsi,1), %rax
	movq	%rdi, %r10
	shrq	$26, %r10
	movq	%rax, %r11
	shrq	$26, %r11
	leaq	0(,%r11,4), %rcx
	leaq	0(%r11,%rcx,1), %rcx
	andq	$67108863, %rdi
	andq	$67108863, %rax
	leaq	0(%r8,%r10,1), %r11
	leaq	0(%rdx,%rcx,1), %r8
	movq	%r11, %rsi
	shrq	$26, %rsi
	movq	%r8, %r10
	shrq	$26, %r10
	andq	$67108863, %r11
	andq	$67108863, %r8
	leaq	0(%r9,%rsi,1), %rsi
	leaq	0(%rdi,%r10,1), %r10
	movq	%rsi, %rcx
	shrq	$26, %rcx
	andq	$67108863, %rsi
	leaq	0(%rax,%rcx,1), %rax
	movq	%r8, 0(%rbx)
	movq	%r10, 8(%rbx)
	movq	%r11, 16(%rbx)
	movq	%rsi, 24(%rbx)
	movq	%rax, 32(%rbx)
	movq	8(%rsp), %rbx
	movq	16(%rsp), %rbp
	movq	24(%rsp), %r12
	movq	32(%rsp), %r13
	movq	40(%rsp), %r14
	movq	48(%rsp), %r15
	addq	$152, %rsp
	ret
	.cfi_endproc
	.type	Hacl_Poly1305_32_poly1305_update1_ccomp, @function
	.size	Hacl_Poly1305_32_poly1305_update1_ccomp, . - Hacl_Poly1305_32_poly1305_update1_ccomp
	.text
	.align	16
	.globl Hacl_Poly1305_32_poly1305_update_ccomp
Hacl_Poly1305_32_poly1305_update_ccomp:
	.cfi_startproc
	subq	$232, %rsp
	.cfi_adjust_cfa_offset	232
	leaq	240(%rsp), %rax
	movq	%rax, 0(%rsp)
	movq	%rbx, 8(%rsp)
	movq	%rbp, 16(%rsp)
	movq	%r12, 24(%rsp)
	movq	%r13, 32(%rsp)
	movq	%r14, 40(%rsp)
	movq	%r15, 48(%rsp)
	movq	%rdx, 104(%rsp)
	leaq	40(%rdi), %rcx
	movq	%rcx, 96(%rsp)
	movq	%rdi, 88(%rsp)
	movq	%rsi, %rax
	shrl	$4, %eax
	movl	%eax, 112(%rsp)
	andl	$15, %esi
	movl	%esi, 84(%rsp)
	xorl	%r9d, %r9d
	movl	%r9d, 80(%rsp)
.L102:
	movl	80(%rsp), %esi
	movl	112(%rsp), %r8d
	cmpl	%r8d, %esi
	jae	.L103
	movl	80(%rsp), %ecx
	sall	$4, %ecx
	movl	%ecx, %eax
	movq	104(%rsp), %r9
	leaq	0(%r9,%rax,1), %rbp
	xorq	%rcx, %rcx
	movq	%rcx, 144(%rsp)
	xorq	%r9, %r9
	movq	%r9, 152(%rsp)
	xorq	%r10, %r10
	movq	%r10, 160(%rsp)
	xorq	%rcx, %rcx
	movq	%rcx, 168(%rsp)
	xorq	%r8, %r8
	movq	%r8, 176(%rsp)
	leaq	224(%rsp), %rdi
	movq	$8, %rdx
	movq	%rbp, %rsi
	call	memcpy
	movq	224(%rsp), %rbx
	leaq	8(%rbp), %rsi
	leaq	224(%rsp), %rdi
	movq	$8, %rdx
	call	memcpy
	movq	224(%rsp), %rdx
	movq	%rbx, %rcx
	movq	%rcx, %r11
	andq	$67108863, %r11
	movq	%rcx, %rax
	shrq	$26, %rax
	andq	$67108863, %rax
	shrq	$52, %rcx
	movq	%rdx, %rdi
	andq	$16383, %rdi
	salq	$12, %rdi
	orq	%rdi, %rcx
	movq	%rdx, %r12
	shrq	$14, %r12
	andq	$67108863, %r12
	shrq	$40, %rdx
	movq	%rcx, %rbx
	movq	%r11, 144(%rsp)
	movq	%rax, 152(%rsp)
	movq	%rbx, 160(%rsp)
	movq	%r12, 168(%rsp)
	movq	%rdx, %rbp
	orq	$16777216, %rbp
	movq	%rbp, 176(%rsp)
	movq	96(%rsp), %rdi
	movq	0(%rdi), %r15
	movq	8(%rdi), %r10
	movq	16(%rdi), %rdx
	movq	24(%rdi), %rsi
	movq	%rsi, 72(%rsp)
	movq	32(%rdi), %rcx
	movq	%rcx, 64(%rsp)
	movq	48(%rdi), %rcx
	movq	%rcx, 120(%rsp)
	movq	56(%rdi), %rcx
	movq	64(%rdi), %rsi
	movq	72(%rdi), %r14
	movq	88(%rsp), %rdi
	movq	0(%rdi), %r9
	movq	8(%rdi), %r13
	movq	16(%rdi), %rdi
	movq	%rdi, 56(%rsp)
	movq	88(%rsp), %r8
	movq	24(%r8), %rdi
	movq	32(%r8), %r8
	leaq	0(%r9,%r11,1), %r9
	leaq	0(%r13,%rax,1), %r13
	movq	56(%rsp), %rax
	leaq	0(%rax,%rbx,1), %rbx
	leaq	0(%rdi,%r12,1), %r11
	leaq	0(%r8,%rbp,1), %r12
	movq	%r15, %r8
	imulq	%r9, %r8
	movq	%r10, %rdi
	imulq	%r9, %rdi
	movq	%rdx, %rbp
	imulq	%r9, %rbp
	movq	72(%rsp), %rax
	imulq	%r9, %rax
	movq	%rax, 56(%rsp)
	movq	64(%rsp), %rax
	imulq	%r9, %rax
	movq	%rax, 64(%rsp)
	movq	%r14, %r9
	imulq	%r13, %r9
	leaq	0(%r8,%r9,1), %rax
	movq	%r15, %r8
	imulq	%r13, %r8
	leaq	0(%rdi,%r8,1), %rdi
	movq	%r10, %r8
	imulq	%r13, %r8
	leaq	0(%rbp,%r8,1), %r8
	movq	%rdx, %r9
	imulq	%r13, %r9
	movq	56(%rsp), %rbp
	leaq	0(%rbp,%r9,1), %r9
	movq	72(%rsp), %rbp
	imulq	%r13, %rbp
	movq	64(%rsp), %r13
	leaq	0(%r13,%rbp,1), %rbp
	movq	%rsi, %r13
	imulq	%rbx, %r13
	leaq	0(%rax,%r13,1), %rax
	movq	%r14, %r13
	imulq	%rbx, %r13
	leaq	0(%rdi,%r13,1), %r13
	movq	%r15, %rdi
	imulq	%rbx, %rdi
	leaq	0(%r8,%rdi,1), %rdi
	movq	%r10, %r8
	imulq	%rbx, %r8
	leaq	0(%r9,%r8,1), %r8
	imulq	%rbx, %rdx
	leaq	0(%rbp,%rdx,1), %rdx
	movq	%rcx, %r9
	imulq	%r11, %r9
	leaq	0(%rax,%r9,1), %r9
	movq	%rsi, %rax
	imulq	%r11, %rax
	leaq	0(%r13,%rax,1), %rbx
	movq	%r14, %rax
	imulq	%r11, %rax
	leaq	0(%rdi,%rax,1), %rax
	movq	%r15, %rdi
	imulq	%r11, %rdi
	leaq	0(%r8,%rdi,1), %rdi
	imulq	%r11, %r10
	leaq	0(%rdx,%r10,1), %rdx
	movq	120(%rsp), %r8
	imulq	%r12, %r8
	leaq	0(%r9,%r8,1), %r8
	imulq	%r12, %rcx
	leaq	0(%rbx,%rcx,1), %r9
	imulq	%r12, %rsi
	leaq	0(%rax,%rsi,1), %r10
	imulq	%r12, %r14
	leaq	0(%rdi,%r14,1), %rax
	imulq	%r12, %r15
	leaq	0(%rdx,%r15,1), %rdx
	movq	%r8, %rcx
	shrq	$26, %rcx
	movq	%rax, %rsi
	shrq	$26, %rsi
	andq	$67108863, %r8
	andq	$67108863, %rax
	leaq	0(%r9,%rcx,1), %rcx
	leaq	0(%rdx,%rsi,1), %r11
	movq	%rcx, %rdx
	shrq	$26, %rdx
	movq	%r11, %r9
	shrq	$26, %r9
	leaq	0(,%r9,4), %rdi
	leaq	0(%r9,%rdi,1), %r9
	andq	$67108863, %rcx
	andq	$67108863, %r11
	leaq	0(%r10,%rdx,1), %rsi
	leaq	0(%r8,%r9,1), %r8
	movq	%rsi, %rdi
	shrq	$26, %rdi
	movq	%r8, %rdx
	shrq	$26, %rdx
	andq	$67108863, %rsi
	andq	$67108863, %r8
	leaq	0(%rax,%rdi,1), %r9
	leaq	0(%rcx,%rdx,1), %r10
	movq	%r9, %rdx
	shrq	$26, %rdx
	andq	$67108863, %r9
	leaq	0(%r11,%rdx,1), %rdx
	movq	88(%rsp), %r11
	movq	%r8, 0(%r11)
	movq	%r10, 8(%r11)
	movq	%rsi, 16(%r11)
	movq	88(%rsp), %r8
	movq	%r9, 24(%r8)
	movq	%rdx, 32(%r8)
	movl	80(%rsp), %r11d
	leal	1(%r11d), %r11d
	movl	%r11d, 80(%rsp)
	jmp	.L102
.L103:
	movl	84(%rsp), %r10d
	cmpl	$0, %r10d
	jbe	.L104
	movl	112(%rsp), %edx
	sall	$4, %edx
	movl	%edx, %esi
	movq	104(%rsp), %rdx
	leaq	0(%rdx,%rsi,1), %rsi
	xorq	%rcx, %rcx
	movq	%rcx, 184(%rsp)
	xorq	%r9, %r9
	movq	%r9, 192(%rsp)
	xorq	%rdx, %rdx
	movq	%rdx, 200(%rsp)
	xorq	%rcx, %rcx
	movq	%rcx, 208(%rsp)
	xorq	%rax, %rax
	movq	%rax, 216(%rsp)
	xorl	%edx, %edx
	movb	%dl, 128(%rsp)
	xorl	%edi, %edi
	movb	%dil, 129(%rsp)
	xorl	%r10d, %r10d
	movb	%r10b, 130(%rsp)
	xorl	%edi, %edi
	movb	%dil, 131(%rsp)
	xorl	%edx, %edx
	movb	%dl, 132(%rsp)
	xorl	%edi, %edi
	movb	%dil, 133(%rsp)
	xorl	%r11d, %r11d
	movb	%r11b, 134(%rsp)
	xorl	%r8d, %r8d
	movb	%r8b, 135(%rsp)
	xorl	%r11d, %r11d
	movb	%r11b, 136(%rsp)
	xorl	%r9d, %r9d
	movb	%r9b, 137(%rsp)
	xorl	%r10d, %r10d
	movb	%r10b, 138(%rsp)
	xorl	%r11d, %r11d
	movb	%r11b, 139(%rsp)
	xorl	%r9d, %r9d
	movb	%r9b, 140(%rsp)
	xorl	%r8d, %r8d
	movb	%r8b, 141(%rsp)
	xorl	%r11d, %r11d
	movb	%r11b, 142(%rsp)
	xorl	%r8d, %r8d
	movb	%r8b, 143(%rsp)
	leaq	128(%rsp), %rdi
	movl	84(%rsp), %eax
	movl	%eax, %edx
	call	memcpy
	leaq	128(%rsp), %rsi
	leaq	224(%rsp), %rdi
	movq	$8, %rdx
	call	memcpy
	movq	224(%rsp), %rbx
	leaq	136(%rsp), %rsi
	leaq	224(%rsp), %rdi
	movq	$8, %rdx
	call	memcpy
	movq	224(%rsp), %rdx
	movq	%rbx, %r10
	andq	$67108863, %r10
	movq	%rbx, %rsi
	shrq	$26, %rsi
	andq	$67108863, %rsi
	shrq	$52, %rbx
	movq	%rdx, %r11
	andq	$16383, %r11
	salq	$12, %r11
	orq	%r11, %rbx
	movq	%rdx, %rdi
	shrq	$14, %rdi
	andq	$67108863, %rdi
	shrq	$40, %rdx
	movq	%r10, 184(%rsp)
	movq	%rsi, 192(%rsp)
	movq	%rbx, 200(%rsp)
	movq	%rdi, 208(%rsp)
	movq	%rdx, 216(%rsp)
	movq	$1, %rdi
	movl	84(%rsp), %r10d
	leal	0(,%r10d,8), %ecx
	movl	$1321528399, %edx
	movq	%rcx, %rax
	mull	%edx
	shrl	$3, %edx
	movq	%rdx, %rsi
	imull	$26, %esi
	subl	%esi, %ecx
	salq	%cl, %rdi
	leaq	184(%rsp), %rcx
	movl	%edx, %esi
	movq	0(%rcx,%rsi,8), %r11
	leaq	184(%rsp), %rax
	orq	%rdi, %r11
	movq	%r11, 0(%rax,%rsi,8)
	movq	96(%rsp), %rdi
	movq	0(%rdi), %rbx
	movq	8(%rdi), %rbp
	movq	16(%rdi), %rsi
	movq	24(%rdi), %rcx
	movq	%rcx, 72(%rsp)
	movq	32(%rdi), %r8
	movq	%r8, 64(%rsp)
	movq	48(%rdi), %r9
	movq	%r9, 80(%rsp)
	movq	56(%rdi), %rcx
	movq	64(%rdi), %r9
	movq	72(%rdi), %r15
	movq	184(%rsp), %r14
	movq	192(%rsp), %rdi
	movq	200(%rsp), %r8
	movq	%r8, 56(%rsp)
	movq	208(%rsp), %r10
	movq	216(%rsp), %r11
	movq	88(%rsp), %rax
	movq	0(%rax), %rdx
	movq	8(%rax), %r8
	movq	16(%rax), %r12
	movq	88(%rsp), %rax
	movq	24(%rax), %r13
	movq	32(%rax), %rax
	leaq	0(%rdx,%r14,1), %r14
	leaq	0(%r8,%rdi,1), %r8
	movq	56(%rsp), %rdx
	leaq	0(%r12,%rdx,1), %r12
	leaq	0(%r13,%r10,1), %rdi
	movq	%rax, %r10
	leaq	0(%r10,%r11,1), %r13
	movq	%rbx, %r10
	imulq	%r14, %r10
	movq	%rbp, %rdx
	imulq	%r14, %rdx
	movq	%rsi, %r11
	imulq	%r14, %r11
	movq	72(%rsp), %rax
	imulq	%r14, %rax
	movq	%rax, 56(%rsp)
	movq	64(%rsp), %rax
	imulq	%r14, %rax
	movq	%rax, 64(%rsp)
	movq	%r15, %rax
	imulq	%r8, %rax
	leaq	0(%r10,%rax,1), %rax
	movq	%rbx, %r10
	imulq	%r8, %r10
	leaq	0(%rdx,%r10,1), %rdx
	movq	%rbp, %r10
	imulq	%r8, %r10
	leaq	0(%r11,%r10,1), %r11
	movq	%rsi, %r10
	imulq	%r8, %r10
	movq	56(%rsp), %r14
	leaq	0(%r14,%r10,1), %r10
	movq	72(%rsp), %r14
	imulq	%r8, %r14
	movq	%r14, %r8
	movq	64(%rsp), %r8
	leaq	0(%r8,%r14,1), %r8
	movq	%r9, %r14
	imulq	%r12, %r14
	leaq	0(%rax,%r14,1), %rax
	movq	%r15, %r14
	imulq	%r12, %r14
	leaq	0(%rdx,%r14,1), %r14
	movq	%rbx, %rdx
	imulq	%r12, %rdx
	leaq	0(%r11,%rdx,1), %rdx
	movq	%rbp, %r11
	imulq	%r12, %r11
	leaq	0(%r10,%r11,1), %r10
	imulq	%r12, %rsi
	leaq	0(%r8,%rsi,1), %r8
	movq	%rcx, %r11
	imulq	%rdi, %r11
	leaq	0(%rax,%r11,1), %r11
	movq	%r9, %rax
	imulq	%rdi, %rax
	leaq	0(%r14,%rax,1), %rax
	movq	%r15, %rsi
	imulq	%rdi, %rsi
	leaq	0(%rdx,%rsi,1), %rdx
	movq	%rbx, %rsi
	imulq	%rdi, %rsi
	leaq	0(%r10,%rsi,1), %rsi
	imulq	%rdi, %rbp
	leaq	0(%r8,%rbp,1), %r10
	movq	80(%rsp), %rdi
	imulq	%r13, %rdi
	leaq	0(%r11,%rdi,1), %rdi
	imulq	%r13, %rcx
	leaq	0(%rax,%rcx,1), %rax
	imulq	%r13, %r9
	leaq	0(%rdx,%r9,1), %r11
	imulq	%r13, %r15
	leaq	0(%rsi,%r15,1), %rcx
	imulq	%r13, %rbx
	leaq	0(%r10,%rbx,1), %rsi
	movq	%rdi, %rdx
	shrq	$26, %rdx
	movq	%rcx, %r8
	shrq	$26, %r8
	andq	$67108863, %rdi
	andq	$67108863, %rcx
	leaq	0(%rax,%rdx,1), %r10
	leaq	0(%rsi,%r8,1), %rax
	movq	%r10, %r8
	shrq	$26, %r8
	movq	%rax, %r9
	shrq	$26, %r9
	leaq	0(,%r9,4), %rsi
	leaq	0(%r9,%rsi,1), %rdx
	andq	$67108863, %r10
	andq	$67108863, %rax
	leaq	0(%r11,%r8,1), %r8
	leaq	0(%rdi,%rdx,1), %r11
	movq	%r8, %rdx
	shrq	$26, %rdx
	movq	%r11, %rsi
	shrq	$26, %rsi
	andq	$67108863, %r8
	andq	$67108863, %r11
	leaq	0(%rcx,%rdx,1), %rdi
	leaq	0(%r10,%rsi,1), %r9
	movq	%rdi, %rcx
	shrq	$26, %rcx
	andq	$67108863, %rdi
	leaq	0(%rax,%rcx,1), %rax
	movq	88(%rsp), %rcx
	movq	%r11, 0(%rcx)
	movq	%r9, 8(%rcx)
	movq	%r8, 16(%rcx)
	movq	88(%rsp), %r10
	movq	%rdi, 24(%r10)
	movq	%rax, 32(%r10)
.L104:
	movq	8(%rsp), %rbx
	movq	16(%rsp), %rbp
	movq	24(%rsp), %r12
	movq	32(%rsp), %r13
	movq	40(%rsp), %r14
	movq	48(%rsp), %r15
	addq	$232, %rsp
	ret
	.cfi_endproc
	.type	Hacl_Poly1305_32_poly1305_update_ccomp, @function
	.size	Hacl_Poly1305_32_poly1305_update_ccomp, . - Hacl_Poly1305_32_poly1305_update_ccomp
	.text
	.align	16
	.globl Hacl_Poly1305_32_poly1305_finish_ccomp
Hacl_Poly1305_32_poly1305_finish_ccomp:
	.cfi_startproc
	subq	$56, %rsp
	.cfi_adjust_cfa_offset	56
	leaq	64(%rsp), %rax
	movq	%rax, 0(%rsp)
	movq	%rbx, 8(%rsp)
	movq	%rbp, 16(%rsp)
	movq	%r12, 24(%rsp)
	movq	%r13, 32(%rsp)
	movq	%r14, 40(%rsp)
	movq	%rdi, %rbx
	leaq	16(%rsi), %rbp
	movq	0(%rdx), %rsi
	movq	8(%rdx), %r9
	movq	16(%rdx), %rax
	movq	24(%rdx), %r11
	movq	32(%rdx), %r8
	movq	%rsi, %r10
	andq	$67108863, %r10
	shrq	$26, %rsi
	leaq	0(%r9,%rsi,1), %rcx
	movq	%rcx, %rsi
	andq	$67108863, %rsi
	shrq	$26, %rcx
	leaq	0(%rax,%rcx,1), %rcx
	movq	%rcx, %r9
	andq	$67108863, %r9
	shrq	$26, %rcx
	leaq	0(%r11,%rcx,1), %rdi
	movq	%rdi, %rax
	andq	$67108863, %rax
	shrq	$26, %rdi
	leaq	0(%r8,%rdi,1), %rdi
	movq	%rdi, %r8
	andq	$67108863, %r8
	shrq	$26, %rdi
	leaq	0(%rdi,%rdi,4), %r11
	leaq	0(%r10,%r11,1), %rcx
	movq	%rcx, %r12
	andq	$67108863, %r12
	shrq	$26, %rcx
	leaq	0(%rsi,%rcx,1), %r10
	movq	%r10, %rsi
	andq	$67108863, %rsi
	shrq	$26, %r10
	leaq	0(%r9,%r10,1), %r11
	movq	%r11, %r10
	andq	$67108863, %r10
	shrq	$26, %r11
	leaq	0(%rax,%r11,1), %r9
	movq	%r9, %rcx
	andq	$67108863, %rcx
	shrq	$26, %r9
	leaq	0(%r8,%r9,1), %rax
	movq	%rax, %rdi
	andq	$67108863, %rdi
	shrq	$26, %rax
	leaq	0(%rax,%rax,4), %r11
	leaq	0(%r12,%r11,1), %r9
	movq	%rdi, %rax
	xorq	$67108863, %rax
	movq	%rax, %r8
	notq	%r8
	leaq	1(%r8), %r8
	orq	%r8, %rax
	shrq	$63, %rax
	leaq	-1(%rax), %rax
	movq	%rcx, %r8
	xorq	$67108863, %r8
	movq	%r8, %r13
	notq	%r13
	leaq	1(%r13), %r13
	orq	%r13, %r8
	shrq	$63, %r8
	leaq	-1(%r8), %r8
	andq	%r8, %rax
	movq	%r10, %r8
	xorq	$67108863, %r8
	movq	%r8, %r13
	notq	%r13
	leaq	1(%r13), %r13
	orq	%r13, %r8
	shrq	$63, %r8
	leaq	-1(%r8), %r8
	andq	%r8, %rax
	movq	%rsi, %r8
	xorq	$67108863, %r8
	movq	%r8, %r13
	notq	%r13
	leaq	1(%r13), %r13
	orq	%r13, %r8
	shrq	$63, %r8
	leaq	-1(%r8), %r8
	andq	%r8, %rax
	movq	%r9, %r8
	movq	%r8, %r13
	xorq	$67108859, %r13
	leaq	-67108859(%r12,%r11,1), %r11
	xorq	$67108859, %r11
	orq	%r11, %r13
	xorq	%r13, %r8
	shrq	$63, %r8
	leaq	-1(%r8), %r8
	andq	%r8, %rax
	movq	%rax, %r11
	andq	$67108863, %r11
	andq	$67108859, %rax
	subq	%rax, %r9
	subq	%r11, %rsi
	subq	%r11, %r10
	subq	%r11, %rcx
	subq	%r11, %rdi
	movq	%r9, 0(%rdx)
	movq	%rsi, 8(%rdx)
	movq	%r10, 16(%rdx)
	movq	%rcx, 24(%rdx)
	movq	%rdi, 32(%rdx)
	movq	0(%rdx), %r13
	movq	8(%rdx), %r8
	movq	16(%rdx), %r12
	movq	24(%rdx), %r9
	salq	$26, %r8
	orq	%r8, %r13
	movq	%r12, %r10
	salq	$52, %r10
	orq	%r10, %r13
	shrq	$12, %r12
	salq	$14, %r9
	orq	%r9, %r12
	salq	$40, %rdi
	orq	%rdi, %r12
	leaq	48(%rsp), %rdi
	movq	$8, %rdx
	movq	%rbp, %rsi
	call	memcpy
	movq	48(%rsp), %r14
	leaq	8(%rbp), %rsi
	leaq	48(%rsp), %rdi
	movq	$8, %rdx
	call	memcpy
	movq	48(%rsp), %rdx
	leaq	0(%r13,%r14,1), %r9
	leaq	0(%r12,%rdx,1), %rax
	movq	%r9, %r10
	xorq	%r14, %r10
	movq	%r9, %r8
	subq	%r14, %r8
	xorq	%r14, %r8
	orq	%r8, %r10
	movq	%r9, %rcx
	xorq	%r10, %rcx
	shrq	$63, %rcx
	leaq	0(%rax,%rcx,1), %rbp
	movq	%r9, 48(%rsp)
	leaq	48(%rsp), %rsi
	movq	$8, %rdx
	movq	%rbx, %rdi
	call	memcpy
	leaq	8(%rbx), %rdi
	movq	%rbp, 48(%rsp)
	leaq	48(%rsp), %rsi
	movq	$8, %rdx
	call	memcpy
	movq	8(%rsp), %rbx
	movq	16(%rsp), %rbp
	movq	24(%rsp), %r12
	movq	32(%rsp), %r13
	movq	40(%rsp), %r14
	addq	$56, %rsp
	ret
	.cfi_endproc
	.type	Hacl_Poly1305_32_poly1305_finish_ccomp, @function
	.size	Hacl_Poly1305_32_poly1305_finish_ccomp, . - Hacl_Poly1305_32_poly1305_finish_ccomp
	.text
	.align	16
	.globl Hacl_Poly1305_32_poly1305_mac_ccomp
Hacl_Poly1305_32_poly1305_mac_ccomp:
	.cfi_startproc
	subq	$248, %rsp
	.cfi_adjust_cfa_offset	248
	leaq	256(%rsp), %rax
	movq	%rax, 0(%rsp)
	movq	%rbx, 8(%rsp)
	movq	%rbp, 16(%rsp)
	movq	%r12, 24(%rsp)
	movq	%r13, 32(%rsp)
	movq	%rcx, %r12
	movq	%rdx, %rbx
	movq	%rsi, %rbp
	movq	%rdi, %r13
	xorq	%rsi, %rsi
	movq	%rsi, 40(%rsp)
	xorq	%r8, %r8
	movq	%r8, 48(%rsp)
	xorq	%rax, %rax
	movq	%rax, 56(%rsp)
	xorq	%r8, %r8
	movq	%r8, 64(%rsp)
	xorq	%r9, %r9
	movq	%r9, 72(%rsp)
	xorq	%rdx, %rdx
	movq	%rdx, 80(%rsp)
	xorq	%r11, %r11
	movq	%r11, 88(%rsp)
	xorq	%r9, %r9
	movq	%r9, 96(%rsp)
	xorq	%rcx, %rcx
	movq	%rcx, 104(%rsp)
	xorq	%r8, %r8
	movq	%r8, 112(%rsp)
	xorq	%rdx, %rdx
	movq	%rdx, 120(%rsp)
	xorq	%rdi, %rdi
	movq	%rdi, 128(%rsp)
	xorq	%rdx, %rdx
	movq	%rdx, 136(%rsp)
	xorq	%r10, %r10
	movq	%r10, 144(%rsp)
	xorq	%rax, %rax
	movq	%rax, 152(%rsp)
	xorq	%rax, %rax
	movq	%rax, 160(%rsp)
	xorq	%r10, %r10
	movq	%r10, 168(%rsp)
	xorq	%rsi, %rsi
	movq	%rsi, 176(%rsp)
	xorq	%rsi, %rsi
	movq	%rsi, 184(%rsp)
	xorq	%rax, %rax
	movq	%rax, 192(%rsp)
	xorq	%rdi, %rdi
	movq	%rdi, 200(%rsp)
	xorq	%rdi, %rdi
	movq	%rdi, 208(%rsp)
	xorq	%rcx, %rcx
	movq	%rcx, 216(%rsp)
	xorq	%rcx, %rcx
	movq	%rcx, 224(%rsp)
	xorq	%r11, %r11
	movq	%r11, 232(%rsp)
	leaq	40(%rsp), %rdi
	movq	%r12, %rsi
	call	Hacl_Poly1305_32_poly1305_init_ccomp
	leaq	40(%rsp), %rdi
	movq	%rbx, %rdx
	movq	%rbp, %rsi
	call	Hacl_Poly1305_32_poly1305_update_ccomp
	leaq	40(%rsp), %rdx
	movq	%r12, %rsi
	movq	%r13, %rdi
	call	Hacl_Poly1305_32_poly1305_finish_ccomp
	movq	8(%rsp), %rbx
	movq	16(%rsp), %rbp
	movq	24(%rsp), %r12
	movq	32(%rsp), %r13
	addq	$248, %rsp
	ret
	.cfi_endproc
	.type	Hacl_Poly1305_32_poly1305_mac_ccomp, @function
	.size	Hacl_Poly1305_32_poly1305_mac_ccomp, . - Hacl_Poly1305_32_poly1305_mac_ccomp
